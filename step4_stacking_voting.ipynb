{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e2e6815f-30ef-4149-a4d4-6c29f19114b6",
   "metadata": {},
   "source": [
    "# Installing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e357a663-22b4-4546-bac2-6890ae6881e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Installing libraries without displaying output\n",
    "!pip install scikit-learn\n",
    "!pip install missingno\n",
    "!pip install xgboost\n",
    "!pip install imbalanced-learn\n",
    "!pip install fancyimpute\n",
    "!pip install tensorflow\n",
    "!pip install tabulate\n",
    "!pip install statsmodels\n",
    "!pip install lightgbm\n",
    "#!pip install yellowbrick\n",
    "!pip install scikit-optimize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07374855-0fdf-4c1b-97a6-4f4c435f6b8e",
   "metadata": {},
   "source": [
    "# Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90ef7806-8429-408a-93cd-6ab5f75ba7c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import datasets\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from collections import Counter\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from imblearn.combine import SMOTEENN\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.feature_selection import RFECV\n",
    "from imblearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from imblearn.combine import SMOTEENN\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from imblearn.over_sampling import SMOTENC\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "#from yellowbrick.features import RFECV\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "import numpy as np\n",
    "\n",
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f7adb09-a98a-4615-b9df-bb2fca86a5a4",
   "metadata": {},
   "source": [
    "# Dataset preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b13ca69-f1ad-4e43-9257-d2c01a3985ef",
   "metadata": {},
   "source": [
    "## Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "873190b6-3acc-4412-ba68-d9cc267a70ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Anxiety ------------------------------------\n",
    "# loading feature engineered variables\n",
    "anx_data=pd.read_csv('df_anx_t6_2.csv')\n",
    "\n",
    "seed = 42 \n",
    "\n",
    "# Loading in data\n",
    "X_anx = anx_data.iloc[:, :-1]\n",
    "y_anx = anx_data.iloc[:, -1]\n",
    "\n",
    "\n",
    "# Distressed ---------------------------------\n",
    "# loading feature engineered variables\n",
    "dis_data=pd.read_csv('data_2.csv')\n",
    "\n",
    "seed = 42 \n",
    "\n",
    "# Loading in data\n",
    "X_dis = dis_data.iloc[:, :-1]\n",
    "y_dis = dis_data.iloc[:, -1]\n",
    "\n",
    "# Removing index column\n",
    "X_dis = X_dis.drop(X_dis.columns[0], axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52de1622-4d64-4fe1-87fb-9117067d1a8f",
   "metadata": {},
   "source": [
    "# Defining models for ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25c6d7e0-00fd-4b91-8d28-4a6f64d86991",
   "metadata": {},
   "source": [
    "## Anxiety"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd9481c7-93c0-4011-a68f-29ff404ff5eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of top 5 models after HP-tuning for Anxiety present detection\n",
    "anx_models = [\n",
    "    ('Random Forest', RandomForestClassifier(class_weight = 'balanced', max_depth = None, max_features = 'log2', min_samples_leaf = 3, min_samples_split = 7, n_estimators = 90, random_state=42)),\n",
    "    ('SVM', SVC(C = 1.0, class_weight = None, coef0 = 0.05, degree = 12, gamma = 'auto', kernel = 'rbf', max_iter = 7000, probability = True, shrinking = True, random_state=42)),\n",
    "    ('KNN', KNeighborsClassifier(algorithm = 'auto', leaf_size = 10, n_neighbors = 5, p = 1, weights = 'uniform',  n_jobs=-1)),\n",
    "    ('XGB', xgb.XGBClassifier(colsample_bytree = 0.9, learning_rate = 0.4, max_depth = 3, min_child_weight = 1, n_estimators = 162, subsample = 0.9, random_state=42, n_jobs=-1)),\n",
    "    ('LGBM', lgb.LGBMClassifier(boosting_type = 'gbdt', class_weight = None, colsample_bytree = 0.75, learning_rate = 0.1, max_depth = 5, n_estimators = 20, num_leaves = 10, objective = 'binary', random_state=42, n_jobs =-1, verbose=-1))\n",
    "]\n",
    "\n",
    "# Best features found through feature-selection methods for Anxiety present detection\n",
    "selected_features = ['ucla_t6_sum', 'summary_score_kccq_base', 'eq5d5l_index_t6', 'MCS_t6', 'ImplWeight', 'personality_type_D']\n",
    "X_anx_selected = X_anx[selected_features]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8487b9c0-6115-42be-af50-15048ad434c5",
   "metadata": {},
   "source": [
    "## Distressed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "346ef193-8e9e-4785-aaa1-155990f92aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of top 5 models after HP-tuning for Distress present detection\n",
    "dis_models = [\n",
    "    ('Random Forest', RandomForestClassifier(class_weight=None, max_depth=6, max_features='sqrt', min_samples_leaf=1, min_samples_split=2, n_estimators=66, random_state=42)),\n",
    "    ('SVM', SVC(C=50, class_weight=None, coef0=0.3, degree=5, gamma='auto', kernel='sigmoid', max_iter=5000, probability=True, shrinking=True, random_state=42)),\n",
    "    ('KNN', KNeighborsClassifier(algorithm='auto', leaf_size=5, n_neighbors=6, p=2, weights='distance', n_jobs=-1)),\n",
    "    ('XGB', xgb.XGBClassifier(colsample_bytree=1, learning_rate=0.3, max_depth=8, min_child_weight=1, n_estimators=63, subsample=1, random_state=42, n_jobs=-1)),\n",
    "    ('LGBM', lgb.LGBMClassifier(boosting_type='dart', class_weight=None, colsample_bytree=0.7, learning_rate=0.2, max_depth=2, n_estimators=87, num_leaves=3, objective='binary', random_state=42, n_jobs=-1, verbose=-1))\n",
    "]\n",
    "\n",
    "# Best features found through feature-selection methods for Distress present detection\n",
    "selected_features = ['ucla_t6_sum', 'eq5d5l_index_t6', 'depression_base_score', 'icdc_t6_sum', 'MCS_t6', 'Return_to_function_t6', 'age', 'anxiety_base_score']\n",
    "X_dis_selected = X_dis[selected_features]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fe21879-159e-4529-8077-3ad2436127a8",
   "metadata": {},
   "source": [
    "# Stacking classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92bb752e-c42f-44bc-bbda-b341558d9b8a",
   "metadata": {},
   "source": [
    "## Anxiety"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb6aa58-0498-4ef4-bfc4-7fa47812d497",
   "metadata": {},
   "source": [
    "### Base-learner combinations for stacking classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3995e393-7577-429f-aecb-6343fc906635",
   "metadata": {},
   "source": [
    "This segment iterates in the number of base-learners and creates different unique combinations for three different meta-learners, RF, LGR and SVM meta-learners. \n",
    "\n",
    "The base-learners consist of the top 5 best models for Anxiety present detection after Hyperparameter tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6b44efd-3d80-4e71-8cb2-ffbaa956c987",
   "metadata": {},
   "source": [
    "#### RF-meta learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "45b77016-34ca-4c2f-ad6d-45cfda022412",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating combination 1/31\n",
      "Cross-Validation F1-macro scores: [0.76886035 0.92295345 0.89152542 0.75876079 0.86350575]\n",
      "Mean F1-macro score: 0.8411211534617902\n",
      "\n",
      "Evaluating combination 2/31\n",
      "Cross-Validation F1-macro scores: [0.89152542 0.8762355  0.89152542 0.75876079 0.68333333]\n",
      "Mean F1-macro score: 0.8202760938837068\n",
      "\n",
      "Evaluating combination 3/31\n",
      "Cross-Validation F1-macro scores: [0.92295345 0.95874517 0.95874517 0.73314607 0.96380952]\n",
      "Mean F1-macro score: 0.907479874633349\n",
      "\n",
      "Evaluating combination 4/31\n",
      "Cross-Validation F1-macro scores: [0.77674419 0.93181818 0.72727273 0.78888889 0.92288961]\n",
      "Mean F1-macro score: 0.829522718883184\n",
      "\n",
      "Evaluating combination 5/31\n",
      "Cross-Validation F1-macro scores: [0.83865546 0.77411765 0.91111111 0.75876079 0.76866883]\n",
      "Mean F1-macro score: 0.8102627687607971\n",
      "\n",
      "Evaluating combination 6/31\n",
      "Cross-Validation F1-macro scores: [0.8762355  0.8762355  0.95874517 0.78888889 0.9587136 ]\n",
      "Mean F1-macro score: 0.8917637299627641\n",
      "\n",
      "Evaluating combination 7/31\n",
      "Cross-Validation F1-macro scores: [0.8762355  0.95874517 0.92295345 0.75876079 0.96380952]\n",
      "Mean F1-macro score: 0.8961008857859024\n",
      "\n",
      "Evaluating combination 8/31\n",
      "Cross-Validation F1-macro scores: [0.79372583 0.92295345 0.92295345 0.78888889 0.90309419]\n",
      "Mean F1-macro score: 0.8663231607744171\n",
      "\n",
      "Evaluating combination 9/31\n",
      "Cross-Validation F1-macro scores: [0.82222222 0.8762355  0.92295345 0.78888889 0.89142857]\n",
      "Mean F1-macro score: 0.8603457259860499\n",
      "\n",
      "Evaluating combination 10/31\n",
      "Cross-Validation F1-macro scores: [0.86363636 0.92295345 0.92295345 0.78888889 0.89142857]\n",
      "Mean F1-macro score: 0.8779721452081002\n",
      "\n",
      "Evaluating combination 11/31\n",
      "Cross-Validation F1-macro scores: [0.92295345 0.95874517 0.95874517 0.78888889 0.9587136 ]\n",
      "Mean F1-macro score: 0.9176092547223558\n",
      "\n",
      "Evaluating combination 12/31\n",
      "Cross-Validation F1-macro scores: [0.89152542 0.8762355  0.95874517 0.78888889 0.91104869]\n",
      "Mean F1-macro score: 0.8852887327105167\n",
      "\n",
      "Evaluating combination 13/31\n",
      "Cross-Validation F1-macro scores: [0.89152542 0.89152542 0.81920904 0.73314607 0.96380952]\n",
      "Mean F1-macro score: 0.8598430956461808\n",
      "\n",
      "Evaluating combination 14/31\n",
      "Cross-Validation F1-macro scores: [0.89152542 0.77411765 0.91111111 0.75876079 0.87614081]\n",
      "Mean F1-macro score: 0.8423311565046585\n",
      "\n",
      "Evaluating combination 15/31\n",
      "Cross-Validation F1-macro scores: [0.86363636 0.8762355  0.91111111 0.75876079 0.87614081]\n",
      "Mean F1-macro score: 0.8571769143438495\n",
      "\n",
      "Evaluating combination 16/31\n",
      "Cross-Validation F1-macro scores: [0.82222222 0.91111111 0.95874517 0.75876079 1.        ]\n",
      "Mean F1-macro score: 0.890167858212551\n",
      "\n",
      "Evaluating combination 17/31\n",
      "Cross-Validation F1-macro scores: [0.8762355  0.8762355  0.95874517 0.78888889 0.96380952]\n",
      "Mean F1-macro score: 0.8927829141683891\n",
      "\n",
      "Evaluating combination 18/31\n",
      "Cross-Validation F1-macro scores: [0.92295345 0.8762355  0.95874517 0.78888889 0.9587136 ]\n",
      "Mean F1-macro score: 0.9011073209019862\n",
      "\n",
      "Evaluating combination 19/31\n",
      "Cross-Validation F1-macro scores: [0.92295345 0.91111111 0.95874517 0.78888889 0.96380952]\n",
      "Mean F1-macro score: 0.9091016280603877\n",
      "\n",
      "Evaluating combination 20/31\n",
      "Cross-Validation F1-macro scores: [0.92295345 0.91111111 0.95874517 0.78888889 0.92288961]\n",
      "Mean F1-macro score: 0.9009176453764051\n",
      "\n",
      "Evaluating combination 21/31\n",
      "Cross-Validation F1-macro scores: [0.79372583 0.91111111 0.92295345 0.78888889 0.89142857]\n",
      "Mean F1-macro score: 0.861621569943458\n",
      "\n",
      "Evaluating combination 22/31\n",
      "Cross-Validation F1-macro scores: [0.92295345 0.95874517 0.95874517 0.78888889 0.96380952]\n",
      "Mean F1-macro score: 0.9186284389279807\n",
      "\n",
      "Evaluating combination 23/31\n",
      "Cross-Validation F1-macro scores: [0.92295345 0.95874517 0.95874517 0.78888889 0.87614081]\n",
      "Mean F1-macro score: 0.9010946958349155\n",
      "\n",
      "Evaluating combination 24/31\n",
      "Cross-Validation F1-macro scores: [0.92295345 0.91111111 0.95874517 0.78888889 0.91104869]\n",
      "Mean F1-macro score: 0.8985494611261983\n",
      "\n",
      "Evaluating combination 25/31\n",
      "Cross-Validation F1-macro scores: [0.92295345 0.95874517 0.91111111 0.78888889 0.96380952]\n",
      "Mean F1-macro score: 0.9091016280603877\n",
      "\n",
      "Evaluating combination 26/31\n",
      "Cross-Validation F1-macro scores: [0.92295345 0.8762355  0.95874517 0.78888889 1.        ]\n",
      "Mean F1-macro score: 0.9093646003457063\n",
      "\n",
      "Evaluating combination 27/31\n",
      "Cross-Validation F1-macro scores: [0.92295345 0.8762355  0.95874517 0.78888889 0.9587136 ]\n",
      "Mean F1-macro score: 0.9011073209019862\n",
      "\n",
      "Evaluating combination 28/31\n",
      "Cross-Validation F1-macro scores: [0.92295345 0.8762355  0.95874517 0.78888889 0.9587136 ]\n",
      "Mean F1-macro score: 0.9011073209019862\n",
      "\n",
      "Evaluating combination 29/31\n",
      "Cross-Validation F1-macro scores: [0.92295345 0.8762355  0.95874517 0.78888889 0.89142857]\n",
      "Mean F1-macro score: 0.8876503146314206\n",
      "\n",
      "Evaluating combination 30/31\n",
      "Cross-Validation F1-macro scores: [0.92295345 0.95874517 0.95874517 0.78888889 0.96380952]\n",
      "Mean F1-macro score: 0.9186284389279807\n",
      "\n",
      "Evaluating combination 31/31\n",
      "Cross-Validation F1-macro scores: [0.92295345 0.91111111 0.95874517 0.78888889 1.        ]\n",
      "Mean F1-macro score: 0.9163397232984829\n",
      "\n",
      "Best combination: ['SVM', 'KNN', 'XGB']\n",
      "Best F1-macro score: 0.9186284389279807\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import numpy as np\n",
    "\n",
    "# Defining preprocessing steps\n",
    "scaler = RobustScaler()\n",
    "imputer = SimpleImputer()\n",
    "\n",
    "# Defining stratified k-fold\n",
    "stratified_kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Defining base learners\n",
    "rf_model = RandomForestClassifier(class_weight='balanced', max_depth=None, max_features='log2', min_samples_leaf=3, min_samples_split=7, n_estimators=90, random_state=42)\n",
    "svm_model = SVC(C=1.0, class_weight=None, coef0=0.05, degree=12, gamma='auto', kernel='rbf', max_iter=7000, probability=True, shrinking=True, random_state=42)\n",
    "knn_model = KNeighborsClassifier(algorithm='auto', leaf_size=10, n_neighbors=5, p=1, weights='uniform', n_jobs=-1)\n",
    "xgb_model = xgb.XGBClassifier(colsample_bytree=0.9, learning_rate=0.4, max_depth=3, min_child_weight=1, n_estimators=162, subsample=0.9, random_state=42, n_jobs=-1)\n",
    "lgb_model = lgb.LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=0.75, learning_rate=0.1, max_depth=5, n_estimators=20, num_leaves=10, objective='binary', random_state=42, n_jobs=-1, verbose=-1)\n",
    "\n",
    "# Defining different combinations of base learners\n",
    "model_combinations = [\n",
    "    [('Random Forest', rf_model)], # 1\n",
    "    [('SVM', svm_model)], # 2\n",
    "    [('KNN', knn_model)], # 3\n",
    "    [('XGB', xgb_model)], # 4\n",
    "    [('LGBM', lgb_model)], # 5\n",
    "    [('Random Forest', rf_model), ('SVM', svm_model)], # 6\n",
    "    [('Random Forest', rf_model), ('KNN', knn_model)], # 7\n",
    "    [('Random Forest', rf_model), ('XGB', xgb_model)], # 8\n",
    "    [('Random Forest', rf_model), ('LGBM', lgb_model)], # 9\n",
    "    [('SVM', svm_model), ('KNN', knn_model)], # 10\n",
    "    [('SVM', svm_model), ('XGB', xgb_model)], # 11\n",
    "    [('SVM', svm_model), ('LGBM', lgb_model)], # 12\n",
    "    [('KNN', knn_model), ('XGB', xgb_model)], # 13\n",
    "    [('KNN', knn_model), ('LGBM', lgb_model)], # 14\n",
    "    [('XGB', xgb_model), ('LGBM', lgb_model)], # 15\n",
    "    [('Random Forest', rf_model), ('SVM', svm_model), ('KNN', knn_model)], # 16\n",
    "    [('Random Forest', rf_model), ('SVM', svm_model), ('XGB', xgb_model)], # 17\n",
    "    [('Random Forest', rf_model), ('SVM', svm_model), ('LGBM', lgb_model)], # 18\n",
    "    [('Random Forest', rf_model), ('KNN', knn_model), ('XGB', xgb_model)], # 19\n",
    "    [('Random Forest', rf_model), ('KNN', knn_model), ('LGBM', lgb_model)], # 20\n",
    "    [('Random Forest', rf_model), ('XGB', xgb_model), ('LGBM', lgb_model)], # 21\n",
    "    [('SVM', svm_model), ('KNN', knn_model), ('XGB', xgb_model)], # 22\n",
    "    [('SVM', svm_model), ('KNN', knn_model), ('LGBM', lgb_model)], # 23\n",
    "    [('SVM', svm_model), ('XGB', xgb_model), ('LGBM', lgb_model)], # 24\n",
    "    [('KNN', knn_model), ('XGB', xgb_model), ('LGBM', lgb_model)], # 25\n",
    "    [('Random Forest', rf_model), ('SVM', svm_model), ('KNN', knn_model), ('XGB', xgb_model)], # 26\n",
    "    [('Random Forest', rf_model), ('SVM', svm_model), ('KNN', knn_model), ('LGBM', lgb_model)], # 27\n",
    "    [('Random Forest', rf_model), ('SVM', svm_model), ('XGB', xgb_model), ('LGBM', lgb_model)], # 28\n",
    "    [('Random Forest', rf_model), ('KNN', knn_model), ('XGB', xgb_model), ('LGBM', lgb_model)], # 29\n",
    "    [('SVM', svm_model), ('KNN', knn_model), ('XGB', xgb_model), ('LGBM', lgb_model)], # 30 \n",
    "    [('Random Forest', rf_model), ('SVM', svm_model), ('KNN', knn_model), ('XGB', xgb_model), ('LGBM', lgb_model)] # 31\n",
    "]\n",
    "\n",
    "best_score = 0\n",
    "best_combination = None\n",
    "\n",
    "# Iterating over each combination and evaluate its performance\n",
    "for idx, combination in enumerate(model_combinations):\n",
    "    print(f\"Evaluating combination {idx + 1}/{len(model_combinations)}\")\n",
    "    \n",
    "    # Creating the stacking classifier with the current combination\n",
    "    stacking_clf = StackingClassifier(estimators=combination, final_estimator=RandomForestClassifier(random_state=42), cv=stratified_kfold)\n",
    "    \n",
    "    # Defining pipeline\n",
    "    pipeline = Pipeline(steps=[\n",
    "        ('Robust', scaler),\n",
    "        ('Simple', imputer),\n",
    "        ('Classifier', stacking_clf)\n",
    "    ])\n",
    "    \n",
    "    # Performing cross-validation\n",
    "    cv_scores = cross_val_score(pipeline, X_anx_selected, y_anx, cv=stratified_kfold, scoring='f1_macro')\n",
    "    \n",
    "    # Printing the cross-validation scores\n",
    "    print(\"Cross-Validation F1-macro scores:\", cv_scores)\n",
    "    print(\"Mean F1-macro score:\", np.mean(cv_scores))\n",
    "    print()\n",
    "    \n",
    "    # Checking if this combination has the best score\n",
    "    mean_score = np.mean(cv_scores)\n",
    "    if mean_score > best_score:\n",
    "        best_score = mean_score\n",
    "        best_combination = [model[0] for model in combination]\n",
    "\n",
    "# Printing the best combination and score\n",
    "print(\"Best combination:\", best_combination)\n",
    "print(\"Best F1-macro score:\", best_score)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6ab1d6c-3b76-44f8-9fe6-12816da4489d",
   "metadata": {},
   "source": [
    "#### LGR-meta learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "ae257d0e-4c1d-4e51-be15-badde37aef67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating combination 1/31\n",
      "Cross-Validation F1-macro scores: [0.82222222 0.82222222 0.95874517 0.78888889 0.85525648]\n",
      "Mean F1-macro score: 0.8494669948301233\n",
      "\n",
      "Evaluating combination 2/31\n",
      "Cross-Validation F1-macro scores: [0.92295345 0.78901099 0.85534907 0.78888889 0.91104869]\n",
      "Mean F1-macro score: 0.8534502177800954\n",
      "\n",
      "Evaluating combination 3/31\n",
      "Cross-Validation F1-macro scores: [0.82222222 0.85534907 0.78901099 0.78888889 0.9587136 ]\n",
      "Mean F1-macro score: 0.8428369547444368\n",
      "\n",
      "Evaluating combination 4/31\n",
      "Cross-Validation F1-macro scores: [0.8762355  0.91111111 0.92295345 0.78888889 1.        ]\n",
      "Mean F1-macro score: 0.8998377894781134\n",
      "\n",
      "Evaluating combination 5/31\n",
      "Cross-Validation F1-macro scores: [0.8459069  0.85534907 0.78901099 0.78888889 0.91104869]\n",
      "Mean F1-macro score: 0.8380409079887633\n",
      "\n",
      "Evaluating combination 6/31\n",
      "Cross-Validation F1-macro scores: [0.92295345 0.85534907 0.95874517 0.75876079 0.91104869]\n",
      "Mean F1-macro score: 0.8813714337460041\n",
      "\n",
      "Evaluating combination 7/31\n",
      "Cross-Validation F1-macro scores: [0.82222222 0.91111111 0.95874517 0.78888889 1.        ]\n",
      "Mean F1-macro score: 0.8961934775342597\n",
      "\n",
      "Evaluating combination 8/31\n",
      "Cross-Validation F1-macro scores: [0.82222222 0.91111111 0.92295345 0.75876079 0.9587136 ]\n",
      "Mean F1-macro score: 0.8747522358876832\n",
      "\n",
      "Evaluating combination 9/31\n",
      "Cross-Validation F1-macro scores: [0.8762355  0.91111111 0.95874517 0.78888889 0.9587136 ]\n",
      "Mean F1-macro score: 0.8987388529155407\n",
      "\n",
      "Evaluating combination 10/31\n",
      "Cross-Validation F1-macro scores: [0.92295345 0.85534907 0.95874517 0.75876079 0.9587136 ]\n",
      "Mean F1-macro score: 0.8909044164745687\n",
      "\n",
      "Evaluating combination 11/31\n",
      "Cross-Validation F1-macro scores: [0.92295345 0.91111111 0.95874517 0.75876079 0.9587136 ]\n",
      "Mean F1-macro score: 0.902056824533054\n",
      "\n",
      "Evaluating combination 12/31\n",
      "Cross-Validation F1-macro scores: [0.92295345 0.85534907 0.95874517 0.78888889 0.91104869]\n",
      "Mean F1-macro score: 0.8873970530677129\n",
      "\n",
      "Evaluating combination 13/31\n",
      "Cross-Validation F1-macro scores: [0.8762355  0.91111111 0.95874517 0.78888889 1.        ]\n",
      "Mean F1-macro score: 0.9069961323592608\n",
      "\n",
      "Evaluating combination 14/31\n",
      "Cross-Validation F1-macro scores: [0.92295345 0.91111111 0.91111111 0.78888889 1.        ]\n",
      "Mean F1-macro score: 0.90681291243089\n",
      "\n",
      "Evaluating combination 15/31\n",
      "Cross-Validation F1-macro scores: [0.8762355  0.91111111 0.92295345 0.78888889 1.        ]\n",
      "Mean F1-macro score: 0.8998377894781134\n",
      "\n",
      "Evaluating combination 16/31\n",
      "Cross-Validation F1-macro scores: [0.92295345 0.85534907 0.95874517 0.75876079 1.        ]\n",
      "Mean F1-macro score: 0.8991616959182889\n",
      "\n",
      "Evaluating combination 17/31\n",
      "Cross-Validation F1-macro scores: [0.92295345 0.91111111 0.95874517 0.75876079 0.9587136 ]\n",
      "Mean F1-macro score: 0.902056824533054\n",
      "\n",
      "Evaluating combination 18/31\n",
      "Cross-Validation F1-macro scores: [0.92295345 0.91111111 0.95874517 0.75876079 0.9587136 ]\n",
      "Mean F1-macro score: 0.902056824533054\n",
      "\n",
      "Evaluating combination 19/31\n",
      "Cross-Validation F1-macro scores: [0.8762355  0.91111111 0.95874517 0.75876079 1.        ]\n",
      "Mean F1-macro score: 0.9009705130375523\n",
      "\n",
      "Evaluating combination 20/31\n",
      "Cross-Validation F1-macro scores: [0.92295345 0.91111111 0.95874517 0.78888889 1.        ]\n",
      "Mean F1-macro score: 0.9163397232984829\n",
      "\n",
      "Evaluating combination 21/31\n",
      "Cross-Validation F1-macro scores: [0.8762355  0.91111111 0.92295345 0.78888889 1.        ]\n",
      "Mean F1-macro score: 0.8998377894781134\n",
      "\n",
      "Evaluating combination 22/31\n",
      "Cross-Validation F1-macro scores: [0.92295345 0.95874517 0.95874517 0.75876079 1.        ]\n",
      "Mean F1-macro score: 0.9198409148443674\n",
      "\n",
      "Evaluating combination 23/31\n",
      "Cross-Validation F1-macro scores: [0.92295345 0.85534907 0.95874517 0.78888889 1.        ]\n",
      "Mean F1-macro score: 0.9051873152399974\n",
      "\n",
      "Evaluating combination 24/31\n",
      "Cross-Validation F1-macro scores: [0.92295345 0.91111111 0.95874517 0.75876079 1.        ]\n",
      "Mean F1-macro score: 0.9103141039767741\n",
      "\n",
      "Evaluating combination 25/31\n",
      "Cross-Validation F1-macro scores: [0.92295345 0.91111111 0.95874517 0.78888889 1.        ]\n",
      "Mean F1-macro score: 0.9163397232984829\n",
      "\n",
      "Evaluating combination 26/31\n",
      "Cross-Validation F1-macro scores: [0.92295345 0.91111111 0.95874517 0.75876079 1.        ]\n",
      "Mean F1-macro score: 0.9103141039767741\n",
      "\n",
      "Evaluating combination 27/31\n",
      "Cross-Validation F1-macro scores: [0.92295345 0.91111111 0.95874517 0.75876079 1.        ]\n",
      "Mean F1-macro score: 0.9103141039767741\n",
      "\n",
      "Evaluating combination 28/31\n",
      "Cross-Validation F1-macro scores: [0.92295345 0.91111111 0.95874517 0.75876079 1.        ]\n",
      "Mean F1-macro score: 0.9103141039767741\n",
      "\n",
      "Evaluating combination 29/31\n",
      "Cross-Validation F1-macro scores: [0.8762355  0.91111111 0.95874517 0.75876079 1.        ]\n",
      "Mean F1-macro score: 0.9009705130375523\n",
      "\n",
      "Evaluating combination 30/31\n",
      "Cross-Validation F1-macro scores: [0.92295345 0.91111111 0.95874517 0.75876079 1.        ]\n",
      "Mean F1-macro score: 0.9103141039767741\n",
      "\n",
      "Evaluating combination 31/31\n",
      "Cross-Validation F1-macro scores: [0.92295345 0.91111111 0.95874517 0.75876079 1.        ]\n",
      "Mean F1-macro score: 0.9103141039767741\n",
      "\n",
      "Best combination: ['SVM', 'KNN', 'XGB']\n",
      "Best F1-macro score: 0.9198409148443674\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import numpy as np\n",
    "\n",
    "# Defining preprocessing steps\n",
    "scaler = RobustScaler()\n",
    "imputer = SimpleImputer()\n",
    "\n",
    "# Defining stratified k-fold\n",
    "stratified_kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Defining base learners\n",
    "rf_model = RandomForestClassifier(class_weight='balanced', max_depth=None, max_features='log2', min_samples_leaf=3, min_samples_split=7, n_estimators=90, random_state=42)\n",
    "svm_model = SVC(C=1.0, class_weight=None, coef0=0.05, degree=12, gamma='auto', kernel='rbf', max_iter=7000, probability=True, shrinking=True, random_state=42)\n",
    "knn_model = KNeighborsClassifier(algorithm='auto', leaf_size=10, n_neighbors=5, p=1, weights='uniform', n_jobs=-1)\n",
    "xgb_model = xgb.XGBClassifier(colsample_bytree=0.9, learning_rate=0.4, max_depth=3, min_child_weight=1, n_estimators=162, subsample=0.9, random_state=42, n_jobs=-1)\n",
    "lgb_model = lgb.LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=0.75, learning_rate=0.1, max_depth=5, n_estimators=20, num_leaves=10, objective='binary', random_state=42, n_jobs=-1, verbose=-1)\n",
    "\n",
    "# Defining different combinations of base learners\n",
    "model_combinations = [\n",
    "    [('Random Forest', rf_model)], # 1\n",
    "    [('SVM', svm_model)], # 2\n",
    "    [('KNN', knn_model)], # 3\n",
    "    [('XGB', xgb_model)], # 4\n",
    "    [('LGBM', lgb_model)], # 5\n",
    "    [('Random Forest', rf_model), ('SVM', svm_model)], # 6\n",
    "    [('Random Forest', rf_model), ('KNN', knn_model)], # 7\n",
    "    [('Random Forest', rf_model), ('XGB', xgb_model)], # 8\n",
    "    [('Random Forest', rf_model), ('LGBM', lgb_model)], # 9\n",
    "    [('SVM', svm_model), ('KNN', knn_model)], # 10\n",
    "    [('SVM', svm_model), ('XGB', xgb_model)], # 11\n",
    "    [('SVM', svm_model), ('LGBM', lgb_model)], # 12\n",
    "    [('KNN', knn_model), ('XGB', xgb_model)], # 13\n",
    "    [('KNN', knn_model), ('LGBM', lgb_model)], # 14\n",
    "    [('XGB', xgb_model), ('LGBM', lgb_model)], # 15\n",
    "    [('Random Forest', rf_model), ('SVM', svm_model), ('KNN', knn_model)], # 16\n",
    "    [('Random Forest', rf_model), ('SVM', svm_model), ('XGB', xgb_model)], # 17\n",
    "    [('Random Forest', rf_model), ('SVM', svm_model), ('LGBM', lgb_model)], # 18\n",
    "    [('Random Forest', rf_model), ('KNN', knn_model), ('XGB', xgb_model)], # 19\n",
    "    [('Random Forest', rf_model), ('KNN', knn_model), ('LGBM', lgb_model)], # 20\n",
    "    [('Random Forest', rf_model), ('XGB', xgb_model), ('LGBM', lgb_model)], # 21\n",
    "    [('SVM', svm_model), ('KNN', knn_model), ('XGB', xgb_model)], # 22\n",
    "    [('SVM', svm_model), ('KNN', knn_model), ('LGBM', lgb_model)], # 23\n",
    "    [('SVM', svm_model), ('XGB', xgb_model), ('LGBM', lgb_model)], # 24\n",
    "    [('KNN', knn_model), ('XGB', xgb_model), ('LGBM', lgb_model)], # 25\n",
    "    [('Random Forest', rf_model), ('SVM', svm_model), ('KNN', knn_model), ('XGB', xgb_model)], # 26\n",
    "    [('Random Forest', rf_model), ('SVM', svm_model), ('KNN', knn_model), ('LGBM', lgb_model)], # 27\n",
    "    [('Random Forest', rf_model), ('SVM', svm_model), ('XGB', xgb_model), ('LGBM', lgb_model)], # 28\n",
    "    [('Random Forest', rf_model), ('KNN', knn_model), ('XGB', xgb_model), ('LGBM', lgb_model)], # 29\n",
    "    [('SVM', svm_model), ('KNN', knn_model), ('XGB', xgb_model), ('LGBM', lgb_model)], # 30 \n",
    "    [('Random Forest', rf_model), ('SVM', svm_model), ('KNN', knn_model), ('XGB', xgb_model), ('LGBM', lgb_model)] # 31\n",
    "]\n",
    "\n",
    "best_score = 0\n",
    "best_combination = None\n",
    "\n",
    "# Iterating over each combination and evaluate its performance\n",
    "for idx, combination in enumerate(model_combinations):\n",
    "    print(f\"Evaluating combination {idx + 1}/{len(model_combinations)}\")\n",
    "    \n",
    "    # Creating the stacking classifier with the current combination\n",
    "    stacking_clf = StackingClassifier(estimators=combination, final_estimator=LogisticRegression(random_state=42), cv=stratified_kfold)\n",
    "    \n",
    "    # Defining pipeline\n",
    "    pipeline = Pipeline(steps=[\n",
    "        ('Robust', scaler),\n",
    "        ('Simple', imputer),\n",
    "        ('Classifier', stacking_clf)\n",
    "    ])\n",
    "    \n",
    "    # Performing cross-validation\n",
    "    cv_scores = cross_val_score(pipeline, X_anx_selected, y_anx, cv=stratified_kfold, scoring='f1_macro')\n",
    "    \n",
    "    # Printing the cross-validation scores\n",
    "    print(\"Cross-Validation F1-macro scores:\", cv_scores)\n",
    "    print(\"Mean F1-macro score:\", np.mean(cv_scores))\n",
    "    print()\n",
    "    \n",
    "    # Checking if this combination has the best score\n",
    "    mean_score = np.mean(cv_scores)\n",
    "    if mean_score > best_score:\n",
    "        best_score = mean_score\n",
    "        best_combination = [model[0] for model in combination]\n",
    "\n",
    "# Printing the best combination and score\n",
    "print(\"Best combination:\", best_combination)\n",
    "print(\"Best F1-macro score:\", best_score)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bae6155-edfb-44db-a903-024de2286159",
   "metadata": {},
   "source": [
    "#### SVM-meta learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "b0e0e107-ccdb-4ad1-808d-254b5fd59056",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating combination 1/31\n",
      "Cross-Validation F1-macro scores: [0.82222222 0.8762355  0.95874517 0.78888889 0.9587136 ]\n",
      "Mean F1-macro score: 0.880961075137763\n",
      "\n",
      "Evaluating combination 2/31\n",
      "Cross-Validation F1-macro scores: [0.92295345 0.8762355  0.95874517 0.75876079 0.91104869]\n",
      "Mean F1-macro score: 0.885548718851713\n",
      "\n",
      "Evaluating combination 3/31\n",
      "Cross-Validation F1-macro scores: [0.86363636 0.95874517 0.95874517 0.73314607 0.96380952]\n",
      "Mean F1-macro score: 0.895616457151954\n",
      "\n",
      "Evaluating combination 4/31\n",
      "Cross-Validation F1-macro scores: [0.8762355  0.91111111 0.92295345 0.78888889 1.        ]\n",
      "Mean F1-macro score: 0.8998377894781134\n",
      "\n",
      "Evaluating combination 5/31\n",
      "Cross-Validation F1-macro scores: [0.89152542 0.91111111 0.91111111 0.78888889 0.91104869]\n",
      "Mean F1-macro score: 0.8827370447957004\n",
      "\n",
      "Evaluating combination 6/31\n",
      "Cross-Validation F1-macro scores: [0.92295345 0.95874517 0.95874517 0.75876079 0.9587136 ]\n",
      "Mean F1-macro score: 0.9115836354006472\n",
      "\n",
      "Evaluating combination 7/31\n",
      "Cross-Validation F1-macro scores: [0.92295345 0.92295345 0.95874517 0.75876079 0.96380952]\n",
      "Mean F1-macro score: 0.9054444767251246\n",
      "\n",
      "Evaluating combination 8/31\n",
      "Cross-Validation F1-macro scores: [0.82222222 0.91111111 0.92295345 0.78888889 0.9587136 ]\n",
      "Mean F1-macro score: 0.880777855209392\n",
      "\n",
      "Evaluating combination 9/31\n",
      "Cross-Validation F1-macro scores: [0.8762355  0.8762355  0.95874517 0.78888889 1.        ]\n",
      "Mean F1-macro score: 0.9000210094064842\n",
      "\n",
      "Evaluating combination 10/31\n",
      "Cross-Validation F1-macro scores: [0.92295345 0.95874517 0.95874517 0.75876079 1.        ]\n",
      "Mean F1-macro score: 0.9198409148443674\n",
      "\n",
      "Evaluating combination 11/31\n",
      "Cross-Validation F1-macro scores: [0.8762355  0.95874517 0.95874517 0.75876079 1.        ]\n",
      "Mean F1-macro score: 0.9104973239051452\n",
      "\n",
      "Evaluating combination 12/31\n",
      "Cross-Validation F1-macro scores: [0.92295345 0.91111111 0.95874517 0.78888889 1.        ]\n",
      "Mean F1-macro score: 0.9163397232984829\n",
      "\n",
      "Evaluating combination 13/31\n",
      "Cross-Validation F1-macro scores: [0.8762355  0.95874517 0.95874517 0.75876079 0.96380952]\n",
      "Mean F1-macro score: 0.90325922866705\n",
      "\n",
      "Evaluating combination 14/31\n",
      "Cross-Validation F1-macro scores: [0.92295345 0.95874517 0.95874517 0.78888889 0.96380952]\n",
      "Mean F1-macro score: 0.9186284389279807\n",
      "\n",
      "Evaluating combination 15/31\n",
      "Cross-Validation F1-macro scores: [0.8762355  0.91111111 0.95874517 0.78888889 0.9587136 ]\n",
      "Mean F1-macro score: 0.8987388529155407\n",
      "\n",
      "Evaluating combination 16/31\n",
      "Cross-Validation F1-macro scores: [0.92295345 0.95874517 0.95874517 0.75876079 1.        ]\n",
      "Mean F1-macro score: 0.9198409148443674\n",
      "\n",
      "Evaluating combination 17/31\n",
      "Cross-Validation F1-macro scores: [0.92295345 0.95874517 0.95874517 0.78888889 0.9587136 ]\n",
      "Mean F1-macro score: 0.9176092547223558\n",
      "\n",
      "Evaluating combination 18/31\n",
      "Cross-Validation F1-macro scores: [0.92295345 0.91111111 0.95874517 0.78888889 1.        ]\n",
      "Mean F1-macro score: 0.9163397232984829\n",
      "\n",
      "Evaluating combination 19/31\n",
      "Cross-Validation F1-macro scores: [0.92295345 0.95874517 0.95874517 0.78888889 0.96380952]\n",
      "Mean F1-macro score: 0.9186284389279807\n",
      "\n",
      "Evaluating combination 20/31\n",
      "Cross-Validation F1-macro scores: [0.92295345 0.91111111 0.95874517 0.78888889 0.96380952]\n",
      "Mean F1-macro score: 0.9091016280603877\n",
      "\n",
      "Evaluating combination 21/31\n",
      "Cross-Validation F1-macro scores: [0.8762355  0.91111111 0.92295345 0.78888889 0.9587136 ]\n",
      "Mean F1-macro score: 0.8915805100343933\n",
      "\n",
      "Evaluating combination 22/31\n",
      "Cross-Validation F1-macro scores: [0.92295345 0.95874517 0.95874517 0.75876079 1.        ]\n",
      "Mean F1-macro score: 0.9198409148443674\n",
      "\n",
      "Evaluating combination 23/31\n",
      "Cross-Validation F1-macro scores: [0.92295345 0.95874517 0.95874517 0.75876079 1.        ]\n",
      "Mean F1-macro score: 0.9198409148443674\n",
      "\n",
      "Evaluating combination 24/31\n",
      "Cross-Validation F1-macro scores: [0.8762355  0.95874517 0.95874517 0.78888889 1.        ]\n",
      "Mean F1-macro score: 0.9165229432268539\n",
      "\n",
      "Evaluating combination 25/31\n",
      "Cross-Validation F1-macro scores: [0.92295345 0.95874517 0.95874517 0.78888889 0.96380952]\n",
      "Mean F1-macro score: 0.9186284389279807\n",
      "\n",
      "Evaluating combination 26/31\n",
      "Cross-Validation F1-macro scores: [0.92295345 0.95874517 0.95874517 0.78888889 1.        ]\n",
      "Mean F1-macro score: 0.9258665341660759\n",
      "\n",
      "Evaluating combination 27/31\n",
      "Cross-Validation F1-macro scores: [0.92295345 0.91111111 0.95874517 0.78888889 1.        ]\n",
      "Mean F1-macro score: 0.9163397232984829\n",
      "\n",
      "Evaluating combination 28/31\n",
      "Cross-Validation F1-macro scores: [0.92295345 0.91111111 0.95874517 0.78888889 1.        ]\n",
      "Mean F1-macro score: 0.9163397232984829\n",
      "\n",
      "Evaluating combination 29/31\n",
      "Cross-Validation F1-macro scores: [0.92295345 0.91111111 0.95874517 0.78888889 0.96380952]\n",
      "Mean F1-macro score: 0.9091016280603877\n",
      "\n",
      "Evaluating combination 30/31\n",
      "Cross-Validation F1-macro scores: [0.92295345 0.95874517 0.95874517 0.78888889 1.        ]\n",
      "Mean F1-macro score: 0.9258665341660759\n",
      "\n",
      "Evaluating combination 31/31\n",
      "Cross-Validation F1-macro scores: [0.92295345 0.95874517 0.95874517 0.78888889 1.        ]\n",
      "Mean F1-macro score: 0.9258665341660759\n",
      "\n",
      "Best combination: ['Random Forest', 'SVM', 'KNN', 'XGB']\n",
      "Best F1-macro score: 0.9258665341660759\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import numpy as np\n",
    "\n",
    "# Defining preprocessing steps\n",
    "scaler = RobustScaler()\n",
    "imputer = SimpleImputer()\n",
    "\n",
    "# Defining stratified k-fold\n",
    "stratified_kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Defining base models\n",
    "rf_model = RandomForestClassifier(class_weight='balanced', max_depth=None, max_features='log2', min_samples_leaf=3, min_samples_split=7, n_estimators=90, random_state=42)\n",
    "svm_model = SVC(C=1.0, class_weight=None, coef0=0.05, degree=12, gamma='auto', kernel='rbf', max_iter=7000, probability=True, shrinking=True, random_state=42)\n",
    "knn_model = KNeighborsClassifier(algorithm='auto', leaf_size=10, n_neighbors=5, p=1, weights='uniform', n_jobs=-1)\n",
    "xgb_model = xgb.XGBClassifier(colsample_bytree=0.9, learning_rate=0.4, max_depth=3, min_child_weight=1, n_estimators=162, subsample=0.9, random_state=42, n_jobs=-1)\n",
    "lgb_model = lgb.LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=0.75, learning_rate=0.1, max_depth=5, n_estimators=20, num_leaves=10, objective='binary', random_state=42, n_jobs=-1, verbose=-1)\n",
    "\n",
    "# Defining different combinations of models\n",
    "model_combinations = [\n",
    "    [('Random Forest', rf_model)], # 1\n",
    "    [('SVM', svm_model)], # 2\n",
    "    [('KNN', knn_model)], # 3\n",
    "    [('XGB', xgb_model)], # 4\n",
    "    [('LGBM', lgb_model)], # 5\n",
    "    [('Random Forest', rf_model), ('SVM', svm_model)], # 6\n",
    "    [('Random Forest', rf_model), ('KNN', knn_model)], # 7\n",
    "    [('Random Forest', rf_model), ('XGB', xgb_model)], # 8\n",
    "    [('Random Forest', rf_model), ('LGBM', lgb_model)], # 9\n",
    "    [('SVM', svm_model), ('KNN', knn_model)], # 10\n",
    "    [('SVM', svm_model), ('XGB', xgb_model)], # 11\n",
    "    [('SVM', svm_model), ('LGBM', lgb_model)], # 12\n",
    "    [('KNN', knn_model), ('XGB', xgb_model)], # 13\n",
    "    [('KNN', knn_model), ('LGBM', lgb_model)], # 14\n",
    "    [('XGB', xgb_model), ('LGBM', lgb_model)], # 15\n",
    "    [('Random Forest', rf_model), ('SVM', svm_model), ('KNN', knn_model)], # 16\n",
    "    [('Random Forest', rf_model), ('SVM', svm_model), ('XGB', xgb_model)], # 17\n",
    "    [('Random Forest', rf_model), ('SVM', svm_model), ('LGBM', lgb_model)], # 18\n",
    "    [('Random Forest', rf_model), ('KNN', knn_model), ('XGB', xgb_model)], # 19\n",
    "    [('Random Forest', rf_model), ('KNN', knn_model), ('LGBM', lgb_model)], # 20\n",
    "    [('Random Forest', rf_model), ('XGB', xgb_model), ('LGBM', lgb_model)], # 21\n",
    "    [('SVM', svm_model), ('KNN', knn_model), ('XGB', xgb_model)], # 22\n",
    "    [('SVM', svm_model), ('KNN', knn_model), ('LGBM', lgb_model)], # 23\n",
    "    [('SVM', svm_model), ('XGB', xgb_model), ('LGBM', lgb_model)], # 24\n",
    "    [('KNN', knn_model), ('XGB', xgb_model), ('LGBM', lgb_model)], # 25\n",
    "    [('Random Forest', rf_model), ('SVM', svm_model), ('KNN', knn_model), ('XGB', xgb_model)], # 26\n",
    "    [('Random Forest', rf_model), ('SVM', svm_model), ('KNN', knn_model), ('LGBM', lgb_model)], # 27\n",
    "    [('Random Forest', rf_model), ('SVM', svm_model), ('XGB', xgb_model), ('LGBM', lgb_model)], # 28\n",
    "    [('Random Forest', rf_model), ('KNN', knn_model), ('XGB', xgb_model), ('LGBM', lgb_model)], # 29\n",
    "    [('SVM', svm_model), ('KNN', knn_model), ('XGB', xgb_model), ('LGBM', lgb_model)], # 30 \n",
    "    [('Random Forest', rf_model), ('SVM', svm_model), ('KNN', knn_model), ('XGB', xgb_model), ('LGBM', lgb_model)] # 31\n",
    "]\n",
    "\n",
    "best_score = 0\n",
    "best_combination = None\n",
    "\n",
    "# Iterating over each combination and evaluate its performance\n",
    "for idx, combination in enumerate(model_combinations):\n",
    "    print(f\"Evaluating combination {idx + 1}/{len(model_combinations)}\")\n",
    "    \n",
    "    # Creating the stacking classifier with the current combination\n",
    "    stacking_clf = StackingClassifier(estimators=combination, final_estimator=SVC(random_state=42), cv=stratified_kfold)\n",
    "    \n",
    "    # Defining pipeline\n",
    "    pipeline = Pipeline(steps=[\n",
    "        ('Robust', scaler),\n",
    "        ('Simple', imputer),\n",
    "        ('Classifier', stacking_clf)\n",
    "    ])\n",
    "    \n",
    "    # Performing cross-validation\n",
    "    cv_scores = cross_val_score(pipeline, X_anx_selected, y_anx, cv=stratified_kfold, scoring='f1_macro')\n",
    "    \n",
    "    # Printing the cross-validation scores\n",
    "    print(\"Cross-Validation F1-macro scores:\", cv_scores)\n",
    "    print(\"Mean F1-macro score:\", np.mean(cv_scores))\n",
    "    print()\n",
    "    \n",
    "    # Checking if this combination has the best score\n",
    "    mean_score = np.mean(cv_scores)\n",
    "    if mean_score > best_score:\n",
    "        best_score = mean_score\n",
    "        best_combination = [model[0] for model in combination]\n",
    "\n",
    "# Printing the best combination and score\n",
    "print(\"Best combination:\", best_combination)\n",
    "print(\"Best F1-macro score:\", best_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6093b350-555b-4f74-bf72-ba54996a5f22",
   "metadata": {},
   "source": [
    "## Distressed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35eb2564-17c6-463e-845e-9d187db6acf7",
   "metadata": {},
   "source": [
    "This segment iterates in the number of base-learners and creates different unique combinations for three different meta-learners, RF, LGR and SVM meta-learners. \n",
    "\n",
    "The base-learners consist of the top 5 best models for Distress present detection after Hyperparameter tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa2af32c-ca2c-4622-83a3-17bd45eeb558",
   "metadata": {},
   "source": [
    "### Base-learner combinations for stacking classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e90578-650d-4742-ab8a-904058845371",
   "metadata": {},
   "source": [
    "#### RF-meta learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "a4f899dc-0811-4f9d-87f5-d81df8a49445",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating combination 1/31\n",
      "Cross-Validation F1-macro scores: [0.85397019 0.87739464 0.79545455 0.70841007 0.83849031]\n",
      "Mean F1-macro score: 0.8147439487988162\n",
      "\n",
      "Evaluating combination 2/31\n",
      "Cross-Validation F1-macro scores: [0.89152542 0.93181818 0.93181818 0.62842243 0.81904762]\n",
      "Mean F1-macro score: 0.8405263662890782\n",
      "\n",
      "Evaluating combination 3/31\n",
      "Cross-Validation F1-macro scores: [0.79372583 0.81609195 0.71121616 0.73314607 1.        ]\n",
      "Mean F1-macro score: 0.8108360013655262\n",
      "\n",
      "Evaluating combination 4/31\n",
      "Cross-Validation F1-macro scores: [0.70957983 0.83865546 0.59090909 0.75876079 0.79356801]\n",
      "Mean F1-macro score: 0.738294638242816\n",
      "\n",
      "Evaluating combination 5/31\n",
      "Cross-Validation F1-macro scores: [0.79555826 0.86363636 0.8459069  0.76866883 0.89142857]\n",
      "Mean F1-macro score: 0.8330397856263645\n",
      "\n",
      "Evaluating combination 6/31\n",
      "Cross-Validation F1-macro scores: [0.92295345 0.96384181 0.93181818 0.66226511 0.93175287]\n",
      "Mean F1-macro score: 0.8825262847053654\n",
      "\n",
      "Evaluating combination 7/31\n",
      "Cross-Validation F1-macro scores: [0.89152542 0.93181818 0.79372583 0.79356801 0.9122807 ]\n",
      "Mean F1-macro score: 0.8645836296907519\n",
      "\n",
      "Evaluating combination 8/31\n",
      "Cross-Validation F1-macro scores: [0.86363636 0.86363636 0.82222222 0.68333333 0.92288961]\n",
      "Mean F1-macro score: 0.8311435786435787\n",
      "\n",
      "Evaluating combination 9/31\n",
      "Cross-Validation F1-macro scores: [0.86363636 0.89152542 0.8459069  0.75876079 0.93175287]\n",
      "Mean F1-macro score: 0.8583164710590836\n",
      "\n",
      "Evaluating combination 10/31\n",
      "Cross-Validation F1-macro scores: [0.92295345 0.96384181 0.76886035 0.66226511 0.93175287]\n",
      "Mean F1-macro score: 0.8499347189677321\n",
      "\n",
      "Evaluating combination 11/31\n",
      "Cross-Validation F1-macro scores: [0.89152542 0.86363636 0.89152542 0.66226511 0.92288961]\n",
      "Mean F1-macro score: 0.8463683861352169\n",
      "\n",
      "Evaluating combination 12/31\n",
      "Cross-Validation F1-macro scores: [0.89152542 0.92295345 0.89152542 0.75876079 0.93175287]\n",
      "Mean F1-macro score: 0.8793035928689059\n",
      "\n",
      "Evaluating combination 13/31\n",
      "Cross-Validation F1-macro scores: [0.86363636 0.90319328 0.71121616 0.69155844 1.        ]\n",
      "Mean F1-macro score: 0.8339208481298523\n",
      "\n",
      "Evaluating combination 14/31\n",
      "Cross-Validation F1-macro scores: [0.8459069  0.93181818 0.79372583 0.87614081 1.        ]\n",
      "Mean F1-macro score: 0.8895183438988876\n",
      "\n",
      "Evaluating combination 15/31\n",
      "Cross-Validation F1-macro scores: [0.86363636 0.92295345 0.8762355  0.87614081 0.96380952]\n",
      "Mean F1-macro score: 0.9005551286361305\n",
      "\n",
      "Evaluating combination 16/31\n",
      "Cross-Validation F1-macro scores: [0.92295345 0.96384181 0.8459069  0.66226511 0.96380952]\n",
      "Mean F1-macro score: 0.8717553588083256\n",
      "\n",
      "Evaluating combination 17/31\n",
      "Cross-Validation F1-macro scores: [0.92295345 0.96384181 0.89152542 0.66226511 0.96380952]\n",
      "Mean F1-macro score: 0.8808790631367529\n",
      "\n",
      "Evaluating combination 18/31\n",
      "Cross-Validation F1-macro scores: [0.92295345 0.92295345 0.89152542 0.75876079 0.96380952]\n",
      "Mean F1-macro score: 0.892000528381072\n",
      "\n",
      "Evaluating combination 19/31\n",
      "Cross-Validation F1-macro scores: [0.89152542 0.93181818 0.79372583 0.66226511 1.        ]\n",
      "Mean F1-macro score: 0.8558669083969719\n",
      "\n",
      "Evaluating combination 20/31\n",
      "Cross-Validation F1-macro scores: [0.92295345 0.93181818 0.89152542 0.82209738 0.96769806]\n",
      "Mean F1-macro score: 0.90721849935024\n",
      "\n",
      "Evaluating combination 21/31\n",
      "Cross-Validation F1-macro scores: [0.89152542 0.92295345 0.82222222 0.68333333 0.96380952]\n",
      "Mean F1-macro score: 0.8567687908274465\n",
      "\n",
      "Evaluating combination 22/31\n",
      "Cross-Validation F1-macro scores: [0.92295345 0.90319328 0.8459069  0.66226511 0.92288961]\n",
      "Mean F1-macro score: 0.8514416700046068\n",
      "\n",
      "Evaluating combination 23/31\n",
      "Cross-Validation F1-macro scores: [0.92295345 0.96384181 0.79372583 0.82209738 0.96380952]\n",
      "Mean F1-macro score: 0.8932855976570002\n",
      "\n",
      "Evaluating combination 24/31\n",
      "Cross-Validation F1-macro scores: [0.92295345 0.92295345 0.92295345 0.68333333 0.96380952]\n",
      "Mean F1-macro score: 0.8832006420545747\n",
      "\n",
      "Evaluating combination 25/31\n",
      "Cross-Validation F1-macro scores: [0.92295345 0.89152542 0.79372583 0.87614081 1.        ]\n",
      "Mean F1-macro score: 0.8968691020723462\n",
      "\n",
      "Evaluating combination 26/31\n",
      "Cross-Validation F1-macro scores: [0.92295345 0.93181818 0.79372583 0.66226511 0.96380952]\n",
      "Mean F1-macro score: 0.8549144186217816\n",
      "\n",
      "Evaluating combination 27/31\n",
      "Cross-Validation F1-macro scores: [0.92295345 0.93181818 0.79372583 0.82209738 0.96380952]\n",
      "Mean F1-macro score: 0.8868808724387156\n",
      "\n",
      "Evaluating combination 28/31\n",
      "Cross-Validation F1-macro scores: [0.92295345 0.92295345 0.89152542 0.68333333 0.96380952]\n",
      "Mean F1-macro score: 0.8769150365916696\n",
      "\n",
      "Evaluating combination 29/31\n",
      "Cross-Validation F1-macro scores: [0.92295345 0.93181818 0.73333333 0.87614081 1.        ]\n",
      "Mean F1-macro score: 0.8928491549078104\n",
      "\n",
      "Evaluating combination 30/31\n",
      "Cross-Validation F1-macro scores: [0.92295345 0.92295345 0.79372583 0.82209738 0.96380952]\n",
      "Mean F1-macro score: 0.8851079262837469\n",
      "\n",
      "Evaluating combination 31/31\n",
      "Cross-Validation F1-macro scores: [0.92295345 0.92295345 0.79372583 0.75876079 0.96380952]\n",
      "Mean F1-macro score: 0.8724406090843854\n",
      "\n",
      "Best combination: ['Random Forest', 'KNN', 'LGBM']\n",
      "Best F1-macro score: 0.90721849935024\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import numpy as np\n",
    "\n",
    "# Defining preprocessing steps\n",
    "scaler = MinMaxScaler()\n",
    "imputer = SimpleImputer()\n",
    "\n",
    "# Defining stratified k-fold\n",
    "stratified_kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Defining base models\n",
    "rf_model = RandomForestClassifier(class_weight=None, max_depth=6, max_features='sqrt', min_samples_leaf=1, min_samples_split=2, n_estimators=66, random_state=42)\n",
    "svm_model = SVC(C=50, class_weight=None, coef0=0.3, degree=5, gamma='auto', kernel='sigmoid', max_iter=5000, probability=True, shrinking=True, random_state=42)\n",
    "knn_model = KNeighborsClassifier(algorithm='auto', leaf_size=5, n_neighbors=6, p=2, weights='distance', n_jobs=-1)\n",
    "xgb_model = xgb.XGBClassifier(colsample_bytree=1, learning_rate=0.3, max_depth=8, min_child_weight=1, n_estimators=63, subsample=1, random_state=42, n_jobs=-1)\n",
    "lgb_model = lgb.LGBMClassifier(boosting_type='dart', class_weight=None, colsample_bytree=0.7, learning_rate=0.2, max_depth=2, n_estimators=87, num_leaves=3, objective='binary', random_state=42, n_jobs=-1, verbose=-1)\n",
    "\n",
    "# Defining different combinations of models\n",
    "model_combinations = [\n",
    "    [('Random Forest', rf_model)], # 1\n",
    "    [('SVM', svm_model)], # 2\n",
    "    [('KNN', knn_model)], # 3\n",
    "    [('XGB', xgb_model)], # 4\n",
    "    [('LGBM', lgb_model)], # 5\n",
    "    [('Random Forest', rf_model), ('SVM', svm_model)], # 6\n",
    "    [('Random Forest', rf_model), ('KNN', knn_model)], # 7\n",
    "    [('Random Forest', rf_model), ('XGB', xgb_model)], # 8\n",
    "    [('Random Forest', rf_model), ('LGBM', lgb_model)], # 9\n",
    "    [('SVM', svm_model), ('KNN', knn_model)], # 10\n",
    "    [('SVM', svm_model), ('XGB', xgb_model)], # 11\n",
    "    [('SVM', svm_model), ('LGBM', lgb_model)], # 12\n",
    "    [('KNN', knn_model), ('XGB', xgb_model)], # 13\n",
    "    [('KNN', knn_model), ('LGBM', lgb_model)], # 14\n",
    "    [('XGB', xgb_model), ('LGBM', lgb_model)], # 15\n",
    "    [('Random Forest', rf_model), ('SVM', svm_model), ('KNN', knn_model)], # 16\n",
    "    [('Random Forest', rf_model), ('SVM', svm_model), ('XGB', xgb_model)], # 17\n",
    "    [('Random Forest', rf_model), ('SVM', svm_model), ('LGBM', lgb_model)], # 18\n",
    "    [('Random Forest', rf_model), ('KNN', knn_model), ('XGB', xgb_model)], # 19\n",
    "    [('Random Forest', rf_model), ('KNN', knn_model), ('LGBM', lgb_model)], # 20\n",
    "    [('Random Forest', rf_model), ('XGB', xgb_model), ('LGBM', lgb_model)], # 21\n",
    "    [('SVM', svm_model), ('KNN', knn_model), ('XGB', xgb_model)], # 22\n",
    "    [('SVM', svm_model), ('KNN', knn_model), ('LGBM', lgb_model)], # 23\n",
    "    [('SVM', svm_model), ('XGB', xgb_model), ('LGBM', lgb_model)], # 24\n",
    "    [('KNN', knn_model), ('XGB', xgb_model), ('LGBM', lgb_model)], # 25\n",
    "    [('Random Forest', rf_model), ('SVM', svm_model), ('KNN', knn_model), ('XGB', xgb_model)], # 26\n",
    "    [('Random Forest', rf_model), ('SVM', svm_model), ('KNN', knn_model), ('LGBM', lgb_model)], # 27\n",
    "    [('Random Forest', rf_model), ('SVM', svm_model), ('XGB', xgb_model), ('LGBM', lgb_model)], # 28\n",
    "    [('Random Forest', rf_model), ('KNN', knn_model), ('XGB', xgb_model), ('LGBM', lgb_model)], # 29\n",
    "    [('SVM', svm_model), ('KNN', knn_model), ('XGB', xgb_model), ('LGBM', lgb_model)], # 30 \n",
    "    [('Random Forest', rf_model), ('SVM', svm_model), ('KNN', knn_model), ('XGB', xgb_model), ('LGBM', lgb_model)] # 31\n",
    "]\n",
    "\n",
    "best_score = 0\n",
    "best_combination = None\n",
    "\n",
    "# Iterating over each combination and evaluate its performance\n",
    "for idx, combination in enumerate(model_combinations):\n",
    "    print(f\"Evaluating combination {idx + 1}/{len(model_combinations)}\")\n",
    "    \n",
    "    # Creating the stacking classifier with the current combination\n",
    "    stacking_clf = StackingClassifier(estimators=combination, final_estimator=RandomForestClassifier(random_state=42), cv=stratified_kfold)\n",
    "    \n",
    "    # Defining pipeline\n",
    "    pipeline = Pipeline(steps=[\n",
    "        ('Robust', scaler),\n",
    "        ('Simple', imputer),\n",
    "        ('Classifier', stacking_clf)\n",
    "    ])\n",
    "    \n",
    "    # Performing cross-validation\n",
    "    cv_scores = cross_val_score(pipeline, X_dis_selected, y_dis, cv=stratified_kfold, scoring='f1_macro')\n",
    "    \n",
    "    # Printing the cross-validation scores\n",
    "    print(\"Cross-Validation F1-macro scores:\", cv_scores)\n",
    "    print(\"Mean F1-macro score:\", np.mean(cv_scores))\n",
    "    print()\n",
    "    \n",
    "    # Checking if this combination has the best score\n",
    "    mean_score = np.mean(cv_scores)\n",
    "    if mean_score > best_score:\n",
    "        best_score = mean_score\n",
    "        best_combination = [model[0] for model in combination]\n",
    "\n",
    "# Printing the best combination and score\n",
    "print(\"Best combination:\", best_combination)\n",
    "print(\"Best F1-macro score:\", best_score)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98883bb6-3dbc-4ee4-a49f-88eab28c653d",
   "metadata": {},
   "source": [
    "#### LGR-meta learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "efa1d535-d353-43b7-a152-e05e429b9e61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating combination 1/31\n",
      "Cross-Validation F1-macro scores: [0.8762355  0.92295345 0.8762355  0.68333333 0.92288961]\n",
      "Mean F1-macro score: 0.8563294774921477\n",
      "\n",
      "Evaluating combination 2/31\n",
      "Cross-Validation F1-macro scores: [0.92295345 0.82222222 0.8459069  0.70841007 0.87614081]\n",
      "Mean F1-macro score: 0.8351266902445051\n",
      "\n",
      "Evaluating combination 3/31\n",
      "Cross-Validation F1-macro scores: [0.89152542 0.8762355  0.73333333 0.75876079 0.92288961]\n",
      "Mean F1-macro score: 0.8365489312158662\n",
      "\n",
      "Evaluating combination 4/31\n",
      "Cross-Validation F1-macro scores: [0.81920904 0.86363636 0.8459069  0.68333333 0.82209738]\n",
      "Mean F1-macro score: 0.8068366033763101\n",
      "\n",
      "Evaluating combination 5/31\n",
      "Cross-Validation F1-macro scores: [0.8459069  0.92295345 0.8459069  0.75876079 0.87614081]\n",
      "Mean F1-macro score: 0.8499337711682475\n",
      "\n",
      "Evaluating combination 6/31\n",
      "Cross-Validation F1-macro scores: [0.92295345 0.96384181 0.89152542 0.68333333 0.96380952]\n",
      "Mean F1-macro score: 0.8850927079649228\n",
      "\n",
      "Evaluating combination 7/31\n",
      "Cross-Validation F1-macro scores: [0.92295345 0.92295345 0.8459069  0.75876079 0.96380952]\n",
      "Mean F1-macro score: 0.8828768240526447\n",
      "\n",
      "Evaluating combination 8/31\n",
      "Cross-Validation F1-macro scores: [0.8762355  0.86363636 0.8459069  0.75876079 0.87614081]\n",
      "Mean F1-macro score: 0.8441360725389625\n",
      "\n",
      "Evaluating combination 9/31\n",
      "Cross-Validation F1-macro scores: [0.8762355  0.92295345 0.89152542 0.75876079 0.96380952]\n",
      "Mean F1-macro score: 0.8826569374418499\n",
      "\n",
      "Evaluating combination 10/31\n",
      "Cross-Validation F1-macro scores: [0.92295345 0.96384181 0.8459069  0.68333333 0.96380952]\n",
      "Mean F1-macro score: 0.8759690036364957\n",
      "\n",
      "Evaluating combination 11/31\n",
      "Cross-Validation F1-macro scores: [0.92295345 0.96384181 0.8459069  0.68333333 0.92288961]\n",
      "Mean F1-macro score: 0.8677850209525129\n",
      "\n",
      "Evaluating combination 12/31\n",
      "Cross-Validation F1-macro scores: [0.92295345 0.92295345 0.89152542 0.68333333 0.96380952]\n",
      "Mean F1-macro score: 0.8769150365916696\n",
      "\n",
      "Evaluating combination 13/31\n",
      "Cross-Validation F1-macro scores: [0.8762355  0.89152542 0.8459069  0.75876079 0.92288961]\n",
      "Mean F1-macro score: 0.859063644966535\n",
      "\n",
      "Evaluating combination 14/31\n",
      "Cross-Validation F1-macro scores: [0.92295345 0.92295345 0.8459069  0.75876079 0.96380952]\n",
      "Mean F1-macro score: 0.8828768240526447\n",
      "\n",
      "Evaluating combination 15/31\n",
      "Cross-Validation F1-macro scores: [0.81920904 0.92295345 0.89152542 0.82209738 0.92288961]\n",
      "Mean F1-macro score: 0.8757349805973877\n",
      "\n",
      "Evaluating combination 16/31\n",
      "Cross-Validation F1-macro scores: [0.92295345 0.96384181 0.8459069  0.68333333 0.96380952]\n",
      "Mean F1-macro score: 0.8759690036364957\n",
      "\n",
      "Evaluating combination 17/31\n",
      "Cross-Validation F1-macro scores: [0.92295345 0.96384181 0.89152542 0.68333333 0.96380952]\n",
      "Mean F1-macro score: 0.8850927079649228\n",
      "\n",
      "Evaluating combination 18/31\n",
      "Cross-Validation F1-macro scores: [0.92295345 0.92295345 0.89152542 0.68333333 0.96380952]\n",
      "Mean F1-macro score: 0.8769150365916696\n",
      "\n",
      "Evaluating combination 19/31\n",
      "Cross-Validation F1-macro scores: [0.92295345 0.92295345 0.89152542 0.75876079 0.96380952]\n",
      "Mean F1-macro score: 0.892000528381072\n",
      "\n",
      "Evaluating combination 20/31\n",
      "Cross-Validation F1-macro scores: [0.92295345 0.92295345 0.89152542 0.75876079 0.96380952]\n",
      "Mean F1-macro score: 0.892000528381072\n",
      "\n",
      "Evaluating combination 21/31\n",
      "Cross-Validation F1-macro scores: [0.8459069  0.92295345 0.89152542 0.75876079 0.96380952]\n",
      "Mean F1-macro score: 0.8765912185897398\n",
      "\n",
      "Evaluating combination 22/31\n",
      "Cross-Validation F1-macro scores: [0.92295345 0.96384181 0.8459069  0.68333333 0.96380952]\n",
      "Mean F1-macro score: 0.8759690036364957\n",
      "\n",
      "Evaluating combination 23/31\n",
      "Cross-Validation F1-macro scores: [0.92295345 0.92295345 0.89152542 0.68333333 0.96380952]\n",
      "Mean F1-macro score: 0.8769150365916696\n",
      "\n",
      "Evaluating combination 24/31\n",
      "Cross-Validation F1-macro scores: [0.92295345 0.92295345 0.89152542 0.68333333 0.96380952]\n",
      "Mean F1-macro score: 0.8769150365916696\n",
      "\n",
      "Evaluating combination 25/31\n",
      "Cross-Validation F1-macro scores: [0.92295345 0.92295345 0.89152542 0.75876079 0.92288961]\n",
      "Mean F1-macro score: 0.8838165456970893\n",
      "\n",
      "Evaluating combination 26/31\n",
      "Cross-Validation F1-macro scores: [0.92295345 0.96384181 0.8459069  0.68333333 0.96380952]\n",
      "Mean F1-macro score: 0.8759690036364957\n",
      "\n",
      "Evaluating combination 27/31\n",
      "Cross-Validation F1-macro scores: [0.92295345 0.96384181 0.89152542 0.75876079 0.96380952]\n",
      "Mean F1-macro score: 0.9001781997543252\n",
      "\n",
      "Evaluating combination 28/31\n",
      "Cross-Validation F1-macro scores: [0.92295345 0.96384181 0.89152542 0.68333333 0.96380952]\n",
      "Mean F1-macro score: 0.8850927079649228\n",
      "\n",
      "Evaluating combination 29/31\n",
      "Cross-Validation F1-macro scores: [0.92295345 0.92295345 0.93181818 0.75876079 0.96380952]\n",
      "Mean F1-macro score: 0.9000590799989456\n",
      "\n",
      "Evaluating combination 30/31\n",
      "Cross-Validation F1-macro scores: [0.92295345 0.96384181 0.89152542 0.75876079 0.96380952]\n",
      "Mean F1-macro score: 0.9001781997543252\n",
      "\n",
      "Evaluating combination 31/31\n",
      "Cross-Validation F1-macro scores: [0.92295345 0.96384181 0.89152542 0.75876079 0.96380952]\n",
      "Mean F1-macro score: 0.9001781997543252\n",
      "\n",
      "Best combination: ['Random Forest', 'SVM', 'KNN', 'LGBM']\n",
      "Best F1-macro score: 0.9001781997543252\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import numpy as np\n",
    "\n",
    "# Defining preprocessing steps\n",
    "scaler = MinMaxScaler()\n",
    "imputer = SimpleImputer()\n",
    "\n",
    "# Defining stratified k-fold\n",
    "stratified_kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Defining base models\n",
    "rf_model = RandomForestClassifier(class_weight=None, max_depth=6, max_features='sqrt', min_samples_leaf=1, min_samples_split=2, n_estimators=66, random_state=42)\n",
    "svm_model = SVC(C=50, class_weight=None, coef0=0.3, degree=5, gamma='auto', kernel='sigmoid', max_iter=5000, probability=True, shrinking=True, random_state=42)\n",
    "knn_model = KNeighborsClassifier(algorithm='auto', leaf_size=5, n_neighbors=6, p=2, weights='distance', n_jobs=-1)\n",
    "xgb_model = xgb.XGBClassifier(colsample_bytree=1, learning_rate=0.3, max_depth=8, min_child_weight=1, n_estimators=63, subsample=1, random_state=42, n_jobs=-1)\n",
    "lgb_model = lgb.LGBMClassifier(boosting_type='dart', class_weight=None, colsample_bytree=0.7, learning_rate=0.2, max_depth=2, n_estimators=87, num_leaves=3, objective='binary', random_state=42, n_jobs=-1, verbose=-1)\n",
    "\n",
    "# Defining different combinations of models\n",
    "model_combinations = [\n",
    "    [('Random Forest', rf_model)], # 1\n",
    "    [('SVM', svm_model)], # 2\n",
    "    [('KNN', knn_model)], # 3\n",
    "    [('XGB', xgb_model)], # 4\n",
    "    [('LGBM', lgb_model)], # 5\n",
    "    [('Random Forest', rf_model), ('SVM', svm_model)], # 6\n",
    "    [('Random Forest', rf_model), ('KNN', knn_model)], # 7\n",
    "    [('Random Forest', rf_model), ('XGB', xgb_model)], # 8\n",
    "    [('Random Forest', rf_model), ('LGBM', lgb_model)], # 9\n",
    "    [('SVM', svm_model), ('KNN', knn_model)], # 10\n",
    "    [('SVM', svm_model), ('XGB', xgb_model)], # 11\n",
    "    [('SVM', svm_model), ('LGBM', lgb_model)], # 12\n",
    "    [('KNN', knn_model), ('XGB', xgb_model)], # 13\n",
    "    [('KNN', knn_model), ('LGBM', lgb_model)], # 14\n",
    "    [('XGB', xgb_model), ('LGBM', lgb_model)], # 15\n",
    "    [('Random Forest', rf_model), ('SVM', svm_model), ('KNN', knn_model)], # 16\n",
    "    [('Random Forest', rf_model), ('SVM', svm_model), ('XGB', xgb_model)], # 17\n",
    "    [('Random Forest', rf_model), ('SVM', svm_model), ('LGBM', lgb_model)], # 18\n",
    "    [('Random Forest', rf_model), ('KNN', knn_model), ('XGB', xgb_model)], # 19\n",
    "    [('Random Forest', rf_model), ('KNN', knn_model), ('LGBM', lgb_model)], # 20\n",
    "    [('Random Forest', rf_model), ('XGB', xgb_model), ('LGBM', lgb_model)], # 21\n",
    "    [('SVM', svm_model), ('KNN', knn_model), ('XGB', xgb_model)], # 22\n",
    "    [('SVM', svm_model), ('KNN', knn_model), ('LGBM', lgb_model)], # 23\n",
    "    [('SVM', svm_model), ('XGB', xgb_model), ('LGBM', lgb_model)], # 24\n",
    "    [('KNN', knn_model), ('XGB', xgb_model), ('LGBM', lgb_model)], # 25\n",
    "    [('Random Forest', rf_model), ('SVM', svm_model), ('KNN', knn_model), ('XGB', xgb_model)], # 26\n",
    "    [('Random Forest', rf_model), ('SVM', svm_model), ('KNN', knn_model), ('LGBM', lgb_model)], # 27\n",
    "    [('Random Forest', rf_model), ('SVM', svm_model), ('XGB', xgb_model), ('LGBM', lgb_model)], # 28\n",
    "    [('Random Forest', rf_model), ('KNN', knn_model), ('XGB', xgb_model), ('LGBM', lgb_model)], # 29\n",
    "    [('SVM', svm_model), ('KNN', knn_model), ('XGB', xgb_model), ('LGBM', lgb_model)], # 30 \n",
    "    [('Random Forest', rf_model), ('SVM', svm_model), ('KNN', knn_model), ('XGB', xgb_model), ('LGBM', lgb_model)] # 31\n",
    "]\n",
    "\n",
    "best_score = 0\n",
    "best_combination = None\n",
    "\n",
    "# Iterating over each combination and evaluate its performance\n",
    "for idx, combination in enumerate(model_combinations):\n",
    "    print(f\"Evaluating combination {idx + 1}/{len(model_combinations)}\")\n",
    "    \n",
    "    # Creating the stacking classifier with the current combination\n",
    "    stacking_clf = StackingClassifier(estimators=combination, final_estimator=LogisticRegression(random_state=42), cv=stratified_kfold)\n",
    "    \n",
    "    # Defining pipeline\n",
    "    pipeline = Pipeline(steps=[\n",
    "        ('Robust', scaler),\n",
    "        ('Simple', imputer),\n",
    "        ('Classifier', stacking_clf)\n",
    "    ])\n",
    "    \n",
    "    # Performing cross-validation\n",
    "    cv_scores = cross_val_score(pipeline, X_dis_selected, y_dis, cv=stratified_kfold, scoring='f1_macro')\n",
    "    \n",
    "    # Printing the cross-validation scores\n",
    "    print(\"Cross-Validation F1-macro scores:\", cv_scores)\n",
    "    print(\"Mean F1-macro score:\", np.mean(cv_scores))\n",
    "    print()\n",
    "    \n",
    "    # Checking if this combination has the best score\n",
    "    mean_score = np.mean(cv_scores)\n",
    "    if mean_score > best_score:\n",
    "        best_score = mean_score\n",
    "        best_combination = [model[0] for model in combination]\n",
    "\n",
    "# Printing the best combination and score\n",
    "print(\"Best combination:\", best_combination)\n",
    "print(\"Best F1-macro score:\", best_score)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be2a1ce8-bf17-4e58-a128-807729287964",
   "metadata": {},
   "source": [
    "#### SVM-meta learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "6e94cf32-1531-4d82-8a0d-457b5e198419",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating combination 1/31\n",
      "Cross-Validation F1-macro scores: [0.89152542 0.93181818 0.93181818 0.75876079 1.        ]\n",
      "Mean F1-macro score: 0.9027845159291047\n",
      "\n",
      "Evaluating combination 2/31\n",
      "Cross-Validation F1-macro scores: [0.92295345 0.96384181 0.93181818 0.68333333 0.92288961]\n",
      "Mean F1-macro score: 0.8849672768988137\n",
      "\n",
      "Evaluating combination 3/31\n",
      "Cross-Validation F1-macro scores: [0.86363636 0.87739464 0.73333333 0.69155844 0.96769806]\n",
      "Mean F1-macro score: 0.8267241672854354\n",
      "\n",
      "Evaluating combination 4/31\n",
      "Cross-Validation F1-macro scores: [0.81920904 0.86363636 0.82222222 0.82209738 0.82209738]\n",
      "Mean F1-macro score: 0.8298524763921831\n",
      "\n",
      "Evaluating combination 5/31\n",
      "Cross-Validation F1-macro scores: [0.83865546 0.92295345 0.93181818 0.87614081 0.96380952]\n",
      "Mean F1-macro score: 0.9066754854400234\n",
      "\n",
      "Evaluating combination 6/31\n",
      "Cross-Validation F1-macro scores: [0.92295345 0.96384181 0.93181818 0.68333333 0.96380952]\n",
      "Mean F1-macro score: 0.8931512595827964\n",
      "\n",
      "Evaluating combination 7/31\n",
      "Cross-Validation F1-macro scores: [0.89152542 0.93181818 0.73333333 0.75876079 1.        ]\n",
      "Mean F1-macro score: 0.8630875462321349\n",
      "\n",
      "Evaluating combination 8/31\n",
      "Cross-Validation F1-macro scores: [0.92295345 0.90319328 0.89152542 0.68333333 1.        ]\n",
      "Mean F1-macro score: 0.880201097083282\n",
      "\n",
      "Evaluating combination 9/31\n",
      "Cross-Validation F1-macro scores: [0.83865546 0.92295345 0.93181818 0.82209738 1.        ]\n",
      "Mean F1-macro score: 0.9031048946647097\n",
      "\n",
      "Evaluating combination 10/31\n",
      "Cross-Validation F1-macro scores: [0.89152542 0.96384181 0.8459069  0.68333333 0.96380952]\n",
      "Mean F1-macro score: 0.8696833981735906\n",
      "\n",
      "Evaluating combination 11/31\n",
      "Cross-Validation F1-macro scores: [0.92295345 0.90319328 0.93181818 0.66226511 0.96380952]\n",
      "Mean F1-macro score: 0.8768079086348906\n",
      "\n",
      "Evaluating combination 12/31\n",
      "Cross-Validation F1-macro scores: [0.89152542 0.96384181 0.93181818 0.75876079 0.96380952]\n",
      "Mean F1-macro score: 0.9019511459092937\n",
      "\n",
      "Evaluating combination 13/31\n",
      "Cross-Validation F1-macro scores: [0.89152542 0.90319328 0.73333333 0.75876079 0.92288961]\n",
      "Mean F1-macro score: 0.8419404874086054\n",
      "\n",
      "Evaluating combination 14/31\n",
      "Cross-Validation F1-macro scores: [0.89152542 0.96384181 0.8459069  0.87614081 1.        ]\n",
      "Mean F1-macro score: 0.9154829884138588\n",
      "\n",
      "Evaluating combination 15/31\n",
      "Cross-Validation F1-macro scores: [0.83865546 0.92295345 0.86363636 0.87614081 0.96380952]\n",
      "Mean F1-macro score: 0.8930391218036597\n",
      "\n",
      "Evaluating combination 16/31\n",
      "Cross-Validation F1-macro scores: [0.92295345 0.96384181 0.8459069  0.68333333 0.96380952]\n",
      "Mean F1-macro score: 0.8759690036364957\n",
      "\n",
      "Evaluating combination 17/31\n",
      "Cross-Validation F1-macro scores: [0.92295345 0.90319328 0.93181818 0.68333333 0.96380952]\n",
      "Mean F1-macro score: 0.8810215534630604\n",
      "\n",
      "Evaluating combination 18/31\n",
      "Cross-Validation F1-macro scores: [0.89152542 0.96384181 0.93181818 0.68333333 0.96380952]\n",
      "Mean F1-macro score: 0.8868656541198915\n",
      "\n",
      "Evaluating combination 19/31\n",
      "Cross-Validation F1-macro scores: [0.92295345 0.90319328 0.79372583 0.75876079 1.        ]\n",
      "Mean F1-macro score: 0.8757266695759978\n",
      "\n",
      "Evaluating combination 20/31\n",
      "Cross-Validation F1-macro scores: [0.89152542 0.92295345 0.89152542 0.75876079 1.        ]\n",
      "Mean F1-macro score: 0.8929530181562623\n",
      "\n",
      "Evaluating combination 21/31\n",
      "Cross-Validation F1-macro scores: [0.89152542 0.92295345 0.89152542 0.82209738 1.        ]\n",
      "Mean F1-macro score: 0.9056203353556238\n",
      "\n",
      "Evaluating combination 22/31\n",
      "Cross-Validation F1-macro scores: [0.89152542 0.90319328 0.89152542 0.66226511 0.96380952]\n",
      "Mean F1-macro score: 0.8624637515541117\n",
      "\n",
      "Evaluating combination 23/31\n",
      "Cross-Validation F1-macro scores: [0.92295345 0.96384181 0.8459069  0.75876079 0.96380952]\n",
      "Mean F1-macro score: 0.8910544954258979\n",
      "\n",
      "Evaluating combination 24/31\n",
      "Cross-Validation F1-macro scores: [0.89152542 0.92295345 0.93181818 0.75876079 0.96380952]\n",
      "Mean F1-macro score: 0.8937734745360407\n",
      "\n",
      "Evaluating combination 25/31\n",
      "Cross-Validation F1-macro scores: [0.92295345 0.92295345 0.8459069  0.82209738 1.        ]\n",
      "Mean F1-macro score: 0.9027822364901017\n",
      "\n",
      "Evaluating combination 26/31\n",
      "Cross-Validation F1-macro scores: [0.92295345 0.90319328 0.89152542 0.68333333 0.96380952]\n",
      "Mean F1-macro score: 0.8729630018451868\n",
      "\n",
      "Evaluating combination 27/31\n",
      "Cross-Validation F1-macro scores: [0.92295345 0.96384181 0.8459069  0.75876079 0.96380952]\n",
      "Mean F1-macro score: 0.8910544954258979\n",
      "\n",
      "Evaluating combination 28/31\n",
      "Cross-Validation F1-macro scores: [0.89152542 0.92295345 0.93181818 0.68333333 0.96380952]\n",
      "Mean F1-macro score: 0.8786879827466384\n",
      "\n",
      "Evaluating combination 29/31\n",
      "Cross-Validation F1-macro scores: [0.89152542 0.92295345 0.8459069  0.82209738 1.        ]\n",
      "Mean F1-macro score: 0.8964966310271965\n",
      "\n",
      "Evaluating combination 30/31\n",
      "Cross-Validation F1-macro scores: [0.92295345 0.92295345 0.89152542 0.75876079 0.96380952]\n",
      "Mean F1-macro score: 0.892000528381072\n",
      "\n",
      "Evaluating combination 31/31\n",
      "Cross-Validation F1-macro scores: [0.92295345 0.92295345 0.89152542 0.75876079 0.96380952]\n",
      "Mean F1-macro score: 0.892000528381072\n",
      "\n",
      "Best combination: ['KNN', 'LGBM']\n",
      "Best F1-macro score: 0.9154829884138588\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import numpy as np\n",
    "\n",
    "# Defining preprocessing steps\n",
    "scaler = MinMaxScaler()\n",
    "imputer = SimpleImputer()\n",
    "\n",
    "# Defining stratified k-fold\n",
    "stratified_kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Defining base models\n",
    "rf_model = RandomForestClassifier(class_weight=None, max_depth=6, max_features='sqrt', min_samples_leaf=1, min_samples_split=2, n_estimators=66, random_state=42)\n",
    "svm_model = SVC(C=50, class_weight=None, coef0=0.3, degree=5, gamma='auto', kernel='sigmoid', max_iter=5000, probability=True, shrinking=True, random_state=42)\n",
    "knn_model = KNeighborsClassifier(algorithm='auto', leaf_size=5, n_neighbors=6, p=2, weights='distance', n_jobs=-1)\n",
    "xgb_model = xgb.XGBClassifier(colsample_bytree=1, learning_rate=0.3, max_depth=8, min_child_weight=1, n_estimators=63, subsample=1, random_state=42, n_jobs=-1)\n",
    "lgb_model = lgb.LGBMClassifier(boosting_type='dart', class_weight=None, colsample_bytree=0.7, learning_rate=0.2, max_depth=2, n_estimators=87, num_leaves=3, objective='binary', random_state=42, n_jobs=-1, verbose=-1)\n",
    "\n",
    "# Defining different combinations of models\n",
    "model_combinations = [\n",
    "    [('Random Forest', rf_model)], # 1\n",
    "    [('SVM', svm_model)], # 2\n",
    "    [('KNN', knn_model)], # 3\n",
    "    [('XGB', xgb_model)], # 4\n",
    "    [('LGBM', lgb_model)], # 5\n",
    "    [('Random Forest', rf_model), ('SVM', svm_model)], # 6\n",
    "    [('Random Forest', rf_model), ('KNN', knn_model)], # 7\n",
    "    [('Random Forest', rf_model), ('XGB', xgb_model)], # 8\n",
    "    [('Random Forest', rf_model), ('LGBM', lgb_model)], # 9\n",
    "    [('SVM', svm_model), ('KNN', knn_model)], # 10\n",
    "    [('SVM', svm_model), ('XGB', xgb_model)], # 11\n",
    "    [('SVM', svm_model), ('LGBM', lgb_model)], # 12\n",
    "    [('KNN', knn_model), ('XGB', xgb_model)], # 13\n",
    "    [('KNN', knn_model), ('LGBM', lgb_model)], # 14\n",
    "    [('XGB', xgb_model), ('LGBM', lgb_model)], # 15\n",
    "    [('Random Forest', rf_model), ('SVM', svm_model), ('KNN', knn_model)], # 16\n",
    "    [('Random Forest', rf_model), ('SVM', svm_model), ('XGB', xgb_model)], # 17\n",
    "    [('Random Forest', rf_model), ('SVM', svm_model), ('LGBM', lgb_model)], # 18\n",
    "    [('Random Forest', rf_model), ('KNN', knn_model), ('XGB', xgb_model)], # 19\n",
    "    [('Random Forest', rf_model), ('KNN', knn_model), ('LGBM', lgb_model)], # 20\n",
    "    [('Random Forest', rf_model), ('XGB', xgb_model), ('LGBM', lgb_model)], # 21\n",
    "    [('SVM', svm_model), ('KNN', knn_model), ('XGB', xgb_model)], # 22\n",
    "    [('SVM', svm_model), ('KNN', knn_model), ('LGBM', lgb_model)], # 23\n",
    "    [('SVM', svm_model), ('XGB', xgb_model), ('LGBM', lgb_model)], # 24\n",
    "    [('KNN', knn_model), ('XGB', xgb_model), ('LGBM', lgb_model)], # 25\n",
    "    [('Random Forest', rf_model), ('SVM', svm_model), ('KNN', knn_model), ('XGB', xgb_model)], # 26\n",
    "    [('Random Forest', rf_model), ('SVM', svm_model), ('KNN', knn_model), ('LGBM', lgb_model)], # 27\n",
    "    [('Random Forest', rf_model), ('SVM', svm_model), ('XGB', xgb_model), ('LGBM', lgb_model)], # 28\n",
    "    [('Random Forest', rf_model), ('KNN', knn_model), ('XGB', xgb_model), ('LGBM', lgb_model)], # 29\n",
    "    [('SVM', svm_model), ('KNN', knn_model), ('XGB', xgb_model), ('LGBM', lgb_model)], # 30 \n",
    "    [('Random Forest', rf_model), ('SVM', svm_model), ('KNN', knn_model), ('XGB', xgb_model), ('LGBM', lgb_model)] # 31\n",
    "]\n",
    "\n",
    "best_score = 0\n",
    "best_combination = None\n",
    "\n",
    "# Iterating over each combination and evaluate its performance\n",
    "for idx, combination in enumerate(model_combinations):\n",
    "    print(f\"Evaluating combination {idx + 1}/{len(model_combinations)}\")\n",
    "    \n",
    "    # Creating the stacking classifier with the current combination\n",
    "    stacking_clf = StackingClassifier(estimators=combination, final_estimator=SVC(random_state=42), cv=stratified_kfold)\n",
    "    \n",
    "    # Defining pipeline\n",
    "    pipeline = Pipeline(steps=[\n",
    "        ('Robust', scaler),\n",
    "        ('Simple', imputer),\n",
    "        ('Classifier', stacking_clf)\n",
    "    ])\n",
    "    \n",
    "    # Performing cross-validation\n",
    "    cv_scores = cross_val_score(pipeline, X_dis_selected, y_dis, cv=stratified_kfold, scoring='f1_macro')\n",
    "    \n",
    "    # Printing the cross-validation scores\n",
    "    print(\"Cross-Validation F1-macro scores:\", cv_scores)\n",
    "    print(\"Mean F1-macro score:\", np.mean(cv_scores))\n",
    "    print()\n",
    "    \n",
    "    # Checking if this combination has the best score\n",
    "    mean_score = np.mean(cv_scores)\n",
    "    if mean_score > best_score:\n",
    "        best_score = mean_score\n",
    "        best_combination = [model[0] for model in combination]\n",
    "\n",
    "# Printing the best combination and score\n",
    "print(\"Best combination:\", best_combination)\n",
    "print(\"Best F1-macro score:\", best_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c17e99c9-5e87-4739-9ed5-ec45102f66a5",
   "metadata": {},
   "source": [
    "# Voting classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5935d139-e594-462f-8558-49bd5a0110c5",
   "metadata": {},
   "source": [
    "## Anxiety"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ffafa5-fcac-4cea-8eb1-193d10ba04ba",
   "metadata": {},
   "source": [
    "### Soft voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "73f1870e-1caa-4264-b77c-2f16f9c1346b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 19 candidates, totalling 95 fits\n",
      "Best parameters: {'Classifier__weights': [0.1, 0.1, 0.1, 10, 0.1]}\n",
      "Best F1-macro score: 0.9209335388734046\n",
      "All results:\n",
      "Mean F1-macro: 0.9198 (Std: 0.0842) with params: {'Classifier__weights': None}\n",
      "Mean F1-macro: 0.9198 (Std: 0.0842) with params: {'Classifier__weights': [1, 1, 1, 1, 1]}\n",
      "Mean F1-macro: 0.9103 (Std: 0.0819) with params: {'Classifier__weights': [2, 1, 1, 1, 2]}\n",
      "Mean F1-macro: 0.9198 (Std: 0.0842) with params: {'Classifier__weights': [1, 1, 2, 1, 1]}\n",
      "Mean F1-macro: 0.9198 (Std: 0.0842) with params: {'Classifier__weights': [1, 2, 1, 2, 1]}\n",
      "Mean F1-macro: 0.9198 (Std: 0.0842) with params: {'Classifier__weights': [1, 1, 1, 2, 2]}\n",
      "Mean F1-macro: 0.9198 (Std: 0.0842) with params: {'Classifier__weights': [0.5, 1, 1.5, 1, 0.5]}\n",
      "Mean F1-macro: 0.9198 (Std: 0.0842) with params: {'Classifier__weights': [0.8, 0.8, 1.2, 0.8, 0.8]}\n",
      "Mean F1-macro: 0.9198 (Std: 0.0842) with params: {'Classifier__weights': [1, 0.5, 1.5, 0.5, 1]}\n",
      "Mean F1-macro: 0.9198 (Std: 0.0842) with params: {'Classifier__weights': [1, 1, 1, 0.5, 0.5]}\n",
      "Mean F1-macro: 0.9101 (Std: 0.0714) with params: {'Classifier__weights': [0.1, 0.1, 0.1, 0.1, 10]}\n",
      "Mean F1-macro: 0.9209 (Std: 0.0521) with params: {'Classifier__weights': [100, 1, 1, 1, 1]}\n",
      "Mean F1-macro: 0.9209 (Std: 0.0521) with params: {'Classifier__weights': [10, 1, 0.1, 0.1, 0.1]}\n",
      "Mean F1-macro: 0.9021 (Std: 0.0741) with params: {'Classifier__weights': [0.1, 10, 0.1, 0.1, 0.1]}\n",
      "Mean F1-macro: 0.9126 (Std: 0.0783) with params: {'Classifier__weights': [0.1, 0.1, 10, 0.1, 0.1]}\n",
      "Mean F1-macro: 0.9209 (Std: 0.0881) with params: {'Classifier__weights': [0.1, 0.1, 0.1, 10, 0.1]}\n",
      "Mean F1-macro: 0.9101 (Std: 0.0714) with params: {'Classifier__weights': [0.1, 0.1, 0.1, 0.1, 10]}\n",
      "Mean F1-macro: 0.9137 (Std: 0.0826) with params: {'Classifier__weights': [10, 1, 1, 10, 1]}\n",
      "Mean F1-macro: 0.9198 (Std: 0.0842) with params: {'Classifier__weights': [2, 5, 3, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold, GridSearchCV\n",
    "import numpy as np\n",
    "\n",
    "# Defining preprocessing steps\n",
    "scaler = RobustScaler()\n",
    "imputer = SimpleImputer()\n",
    "\n",
    "# Defining stratified k-fold\n",
    "stratified_kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Creating the voting classifier\n",
    "voting_clf = VotingClassifier(estimators=anx_models, voting='soft')\n",
    "\n",
    "# Defining pipeline\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('Robust', scaler),\n",
    "    ('Simple', imputer),\n",
    "    ('Classifier', voting_clf)\n",
    "])\n",
    "\n",
    "weights_combinations = [\n",
    "    None,  # No weights\n",
    "    [1, 1, 1, 1, 1],  # Equal weights\n",
    "    [2, 1, 1, 1, 2],  # Higher weight for first and last estimator\n",
    "    [1, 1, 2, 1, 1],  # Higher weight for third estimator\n",
    "    [1, 2, 1, 2, 1],  # Higher weight for second and fourth estimator\n",
    "    [1, 1, 1, 2, 2],  # Higher weight for last two estimators\n",
    "    [0.5, 1, 1.5, 1, 0.5],  # Fractional weights with emphasis on third estimator\n",
    "    [0.8, 0.8, 1.2, 0.8, 0.8],  # Slightly different weights\n",
    "    [1, 0.5, 1.5, 0.5, 1],  # Unequal weights with emphasis on third estimator\n",
    "    [1, 1, 1, 0.5, 0.5],  # Unequal weights with emphasis on last two estimators\n",
    "    [0.1, 0.1, 0.1, 0.1, 10],  # Extreme weight for last estimator\n",
    "    [5, 1, 1, 1, 1],\n",
    "    [7, 1, 0.1, 0.1, 0.1],  # Dominant weight for first estimator, toned down weights for others\n",
    "    [0.1, 10, 0.1, 0.1, 0.1],  # Dominant weight for second estimator, toned down weights for others\n",
    "    [0.1, 0.1, 10, 0.1, 0.1],  # Dominant weight for third estimator, toned down weights for others\n",
    "    [0.1, 0.1, 0.1, 10, 0.1],  # Dominant weight for fourth estimator, toned down weights for others\n",
    "    [0.1, 0.1, 0.1, 0.1, 10],  # Dominant weight for fifth estimator, toned down weights for others# Extreme weight for first estimator\n",
    "    [10, 1, 1, 10, 1],\n",
    "    [2, 5, 3, 1, 1]\n",
    "]\n",
    "\n",
    "\n",
    "# Defining parameter grid for hyperparameter tuning\n",
    "param_grid = {\n",
    "    'Classifier__weights': weights_combinations,  # Weights for soft voting\n",
    "}\n",
    "\n",
    "# Performing grid search with cross-validation\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=stratified_kfold, scoring='f1_macro', verbose=1, n_jobs=-1)\n",
    "grid_search.fit(X_anx_selected, y_anx)\n",
    "\n",
    "# Printing the best parameters found\n",
    "print(\"Best parameters:\", grid_search.best_params_)\n",
    "print(\"Best F1-macro score:\", grid_search.best_score_)\n",
    "\n",
    "# Printing results\n",
    "print(\"All results:\")\n",
    "means = grid_search.cv_results_['mean_test_score']\n",
    "stds = grid_search.cv_results_['std_test_score']\n",
    "for mean, std, params in zip(means, stds, grid_search.cv_results_['params']):\n",
    "    print(f\"Mean F1-macro: {mean:.4f} (Std: {std:.4f}) with params: {params}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fc8a6fa-d318-4ec0-af84-6f9d30584452",
   "metadata": {},
   "source": [
    "### Hard voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6da3f10d-2ad6-4f51-865a-d20f38f084cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 19 candidates, totalling 95 fits\n",
      "Best parameters: {'Classifier__weights': [0.1, 0.1, 0.1, 10, 0.1]}\n",
      "Best F1-macro score: 0.9209335388734046\n",
      "All results:\n",
      "Mean F1-macro: 0.9198 (Std: 0.0842) with params: {'Classifier__weights': None}\n",
      "Mean F1-macro: 0.9198 (Std: 0.0842) with params: {'Classifier__weights': [1, 1, 1, 1, 1]}\n",
      "Mean F1-macro: 0.9198 (Std: 0.0842) with params: {'Classifier__weights': [2, 1, 1, 1, 2]}\n",
      "Mean F1-macro: 0.9198 (Std: 0.0842) with params: {'Classifier__weights': [1, 1, 2, 1, 1]}\n",
      "Mean F1-macro: 0.9198 (Std: 0.0842) with params: {'Classifier__weights': [1, 2, 1, 2, 1]}\n",
      "Mean F1-macro: 0.9198 (Std: 0.0842) with params: {'Classifier__weights': [1, 1, 1, 2, 2]}\n",
      "Mean F1-macro: 0.9198 (Std: 0.0842) with params: {'Classifier__weights': [0.5, 1, 1.5, 1, 0.5]}\n",
      "Mean F1-macro: 0.9198 (Std: 0.0842) with params: {'Classifier__weights': [0.8, 0.8, 1.2, 0.8, 0.8]}\n",
      "Mean F1-macro: 0.9198 (Std: 0.0842) with params: {'Classifier__weights': [1, 0.5, 1.5, 0.5, 1]}\n",
      "Mean F1-macro: 0.9198 (Std: 0.0842) with params: {'Classifier__weights': [1, 1, 1, 0.5, 0.5]}\n",
      "Mean F1-macro: 0.9101 (Std: 0.0714) with params: {'Classifier__weights': [0.1, 0.1, 0.1, 0.1, 10]}\n",
      "Mean F1-macro: 0.9209 (Std: 0.0521) with params: {'Classifier__weights': [5, 1, 1, 1, 1]}\n",
      "Mean F1-macro: 0.9209 (Std: 0.0521) with params: {'Classifier__weights': [7, 1, 0.1, 0.1, 0.1]}\n",
      "Mean F1-macro: 0.9127 (Std: 0.0820) with params: {'Classifier__weights': [0.1, 10, 0.1, 0.1, 0.1]}\n",
      "Mean F1-macro: 0.9126 (Std: 0.0783) with params: {'Classifier__weights': [0.1, 0.1, 10, 0.1, 0.1]}\n",
      "Mean F1-macro: 0.9209 (Std: 0.0881) with params: {'Classifier__weights': [0.1, 0.1, 0.1, 10, 0.1]}\n",
      "Mean F1-macro: 0.9101 (Std: 0.0714) with params: {'Classifier__weights': [0.1, 0.1, 0.1, 0.1, 10]}\n",
      "Mean F1-macro: 0.9209 (Std: 0.0881) with params: {'Classifier__weights': [10, 1, 1, 10, 1]}\n",
      "Mean F1-macro: 0.9198 (Std: 0.0842) with params: {'Classifier__weights': [2, 5, 3, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold, GridSearchCV\n",
    "import numpy as np\n",
    "\n",
    "# Defining preprocessing steps\n",
    "scaler = RobustScaler()\n",
    "imputer = SimpleImputer()\n",
    "\n",
    "# Defining stratified k-fold\n",
    "stratified_kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Creating the voting classifier\n",
    "voting_clf = VotingClassifier(estimators=anx_models, voting='hard')\n",
    "\n",
    "# Defining pipeline\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('Robust', scaler),\n",
    "    ('Simple', imputer),\n",
    "    ('Classifier', voting_clf)\n",
    "])\n",
    "\n",
    "weights_combinations = [\n",
    "    None,  # No weights\n",
    "    [1, 1, 1, 1, 1],  # Equal weights\n",
    "    [2, 1, 1, 1, 2],  # Higher weight for first and last estimator\n",
    "    [1, 1, 2, 1, 1],  # Higher weight for third estimator\n",
    "    [1, 2, 1, 2, 1],  # Higher weight for second and fourth estimator\n",
    "    [1, 1, 1, 2, 2],  # Higher weight for last two estimators\n",
    "    [0.5, 1, 1.5, 1, 0.5],  # Fractional weights with emphasis on third estimator\n",
    "    [0.8, 0.8, 1.2, 0.8, 0.8],  # Slightly different weights\n",
    "    [1, 0.5, 1.5, 0.5, 1],  # Unequal weights with emphasis on third estimator\n",
    "    [1, 1, 1, 0.5, 0.5],  # Unequal weights with emphasis on last two estimators\n",
    "    [0.1, 0.1, 0.1, 0.1, 10],  # Extreme weight for last estimator\n",
    "    [5, 1, 1, 1, 1],\n",
    "    [7, 1, 0.1, 0.1, 0.1],  # Dominant weight for first estimator, toned down weights for others\n",
    "    [0.1, 10, 0.1, 0.1, 0.1],  # Dominant weight for second estimator, toned down weights for others\n",
    "    [0.1, 0.1, 10, 0.1, 0.1],  # Dominant weight for third estimator, toned down weights for others\n",
    "    [0.1, 0.1, 0.1, 10, 0.1],  # Dominant weight for fourth estimator, toned down weights for others\n",
    "    [0.1, 0.1, 0.1, 0.1, 10],  # Dominant weight for fifth estimator, toned down weights for others# Extreme weight for first estimator\n",
    "    [10, 1, 1, 10, 1],\n",
    "    [2, 5, 3, 1, 1]\n",
    "]\n",
    "\n",
    "\n",
    "# Defining parameter grid for hyperparameter tuning\n",
    "param_grid = {\n",
    "    'Classifier__weights': weights_combinations,  # Weights for soft voting\n",
    "}\n",
    "\n",
    "# Performing grid search with cross-validation\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=stratified_kfold, scoring='f1_macro', verbose=1, n_jobs=-1)\n",
    "grid_search.fit(X_anx_selected, y_anx)\n",
    "\n",
    "# Printing the best parameters found\n",
    "print(\"Best parameters:\", grid_search.best_params_)\n",
    "print(\"Best F1-macro score:\", grid_search.best_score_)\n",
    "\n",
    "# Printing results\n",
    "print(\"All results:\")\n",
    "means = grid_search.cv_results_['mean_test_score']\n",
    "stds = grid_search.cv_results_['std_test_score']\n",
    "for mean, std, params in zip(means, stds, grid_search.cv_results_['params']):\n",
    "    print(f\"Mean F1-macro: {mean:.4f} (Std: {std:.4f}) with params: {params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9384395-2c72-4ba7-9f63-715b858167ea",
   "metadata": {},
   "source": [
    "### Stacking classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "8a91572a-fbad-487b-8bcc-52aadb04cdd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 540 candidates, totalling 2700 fits\n",
      "Best parameters: {'classifier__final_estimator__C': 1.0, 'classifier__final_estimator__class_weight': None, 'classifier__final_estimator__coef0': 0.05, 'classifier__final_estimator__degree': 14, 'classifier__final_estimator__gamma': 'scale', 'classifier__final_estimator__kernel': 'rbf', 'classifier__final_estimator__max_iter': 7000, 'classifier__final_estimator__probability': False, 'classifier__final_estimator__shrinking': True}\n",
      "Best F1-macro score: 0.9258665341660759\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "import numpy as np\n",
    "\n",
    "# Defining preprocessing steps\n",
    "scaler = RobustScaler()\n",
    "imputer = SimpleImputer()\n",
    "\n",
    "# Defining stratified k-fold\n",
    "stratified_kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Defining hypertuned base learners for stacking classifier\n",
    "anx_models_comb = [\n",
    "    ('Random Forest', RandomForestClassifier(class_weight = 'balanced', max_depth = None, max_features = 'log2', min_samples_leaf = 3, min_samples_split = 7, n_estimators = 90, random_state=42)),\n",
    "    ('SVM', SVC(C = 1.0, class_weight = None, coef0 = 0.05, degree = 12, gamma = 'auto', kernel = 'rbf', max_iter = 7000, probability = True, shrinking = True, random_state=42)),\n",
    "    ('KNN', KNeighborsClassifier(algorithm = 'auto', leaf_size = 10, n_neighbors = 5, p = 1, weights = 'uniform',  n_jobs=-1)),\n",
    "    ('XGB', xgb.XGBClassifier(colsample_bytree = 0.9, learning_rate = 0.4, max_depth = 3, min_child_weight = 1, n_estimators = 162, subsample = 0.9, random_state=42, n_jobs=-1)),\n",
    "    ]\n",
    "\n",
    "# Creating the stacking classifier with SVM as final estimator\n",
    "svm_model = SVC(random_state=42)  # Change this according to your desired SVM parameters\n",
    "stacking_clf = StackingClassifier(estimators=anx_models_comb, final_estimator=svm_model, cv=stratified_kfold)\n",
    "\n",
    "# Defining pipeline\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('Robust', scaler),\n",
    "    ('Simple', imputer),\n",
    "    ('classifier', stacking_clf)\n",
    "])\n",
    "\n",
    "# Defining parameter grid for hyperparameter tuning\n",
    "params_svc = {\n",
    "    'classifier__final_estimator__C': [0.5, 0.75, 0.875, 1.0, 1.125],                      #1.0\n",
    "    'classifier__final_estimator__kernel': ['rbf'],                                  #rbf\n",
    "    'classifier__final_estimator__degree': [14, 15, 16],             #15\n",
    "    'classifier__final_estimator__gamma': ['scale', 'auto'],                         #auto\n",
    "    'classifier__final_estimator__coef0': [0.04, 0.05, 0.1],                                                   #0.1\n",
    "    'classifier__final_estimator__shrinking': [True],                                                      #True\n",
    "    'classifier__final_estimator__probability': [False],                                                    #False\n",
    "    'classifier__final_estimator__class_weight': ['balanced', None],                                              #None\n",
    "    'classifier__final_estimator__max_iter': [6000, 7000, 8000]   #8000\n",
    "}\n",
    "\n",
    "\n",
    "# Performing grid search with cross-validation\n",
    "grid_search = GridSearchCV(pipeline, params_svc, cv=stratified_kfold, scoring='f1_macro', verbose=1, n_jobs=-1)\n",
    "grid_search.fit(X_anx_selected, y_anx)\n",
    "\n",
    "# Printing the best parameters found\n",
    "print(\"Best parameters:\", grid_search.best_params_)\n",
    "print(\"Best F1-macro score:\", grid_search.best_score_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a381eb4-6451-4c81-804d-91e11b6a5c4e",
   "metadata": {},
   "source": [
    "## Distressed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73bd341e-8f38-4ff6-b41f-1dc3b3473613",
   "metadata": {},
   "source": [
    "### Soft voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "ccf42204-59f9-4063-83e8-9ac977d5a3ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 19 candidates, totalling 95 fits\n",
      "Best parameters: {'Classifier__weights': [0.1, 0.1, 0.1, 0.1, 10]}\n",
      "Best F1-macro score: 0.9270874323416697\n",
      "All results:\n",
      "Mean F1-macro: 0.9082 (Std: 0.0765) with params: {'Classifier__weights': None}\n",
      "Mean F1-macro: 0.9082 (Std: 0.0765) with params: {'Classifier__weights': [1, 1, 1, 1, 1]}\n",
      "Mean F1-macro: 0.9020 (Std: 0.0764) with params: {'Classifier__weights': [2, 1, 1, 1, 2]}\n",
      "Mean F1-macro: 0.9082 (Std: 0.0765) with params: {'Classifier__weights': [1, 1, 2, 1, 1]}\n",
      "Mean F1-macro: 0.8955 (Std: 0.0721) with params: {'Classifier__weights': [1, 2, 1, 2, 1]}\n",
      "Mean F1-macro: 0.9082 (Std: 0.0488) with params: {'Classifier__weights': [1, 1, 1, 2, 2]}\n",
      "Mean F1-macro: 0.9082 (Std: 0.0765) with params: {'Classifier__weights': [0.5, 1, 1.5, 1, 0.5]}\n",
      "Mean F1-macro: 0.9082 (Std: 0.0765) with params: {'Classifier__weights': [0.8, 0.8, 1.2, 0.8, 0.8]}\n",
      "Mean F1-macro: 0.9082 (Std: 0.0765) with params: {'Classifier__weights': [1, 0.5, 1.5, 0.5, 1]}\n",
      "Mean F1-macro: 0.9002 (Std: 0.0758) with params: {'Classifier__weights': [1, 1, 1, 0.5, 0.5]}\n",
      "Mean F1-macro: 0.9271 (Std: 0.0516) with params: {'Classifier__weights': [0.1, 0.1, 0.1, 0.1, 10]}\n",
      "Mean F1-macro: 0.9155 (Std: 0.0829) with params: {'Classifier__weights': [100, 1, 1, 1, 1]}\n",
      "Mean F1-macro: 0.8851 (Std: 0.1045) with params: {'Classifier__weights': [10, 1, 0.1, 0.1, 0.1]}\n",
      "Mean F1-macro: 0.8889 (Std: 0.1145) with params: {'Classifier__weights': [0.1, 10, 0.1, 0.1, 0.1]}\n",
      "Mean F1-macro: 0.8930 (Std: 0.0779) with params: {'Classifier__weights': [0.1, 0.1, 10, 0.1, 0.1]}\n",
      "Mean F1-macro: 0.8953 (Std: 0.0686) with params: {'Classifier__weights': [0.1, 0.1, 0.1, 10, 0.1]}\n",
      "Mean F1-macro: 0.9271 (Std: 0.0516) with params: {'Classifier__weights': [0.1, 0.1, 0.1, 0.1, 10]}\n",
      "Mean F1-macro: 0.8879 (Std: 0.0814) with params: {'Classifier__weights': [10, 1, 1, 10, 1]}\n",
      "Mean F1-macro: 0.8851 (Std: 0.1045) with params: {'Classifier__weights': [2, 5, 3, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold, GridSearchCV\n",
    "import numpy as np\n",
    "\n",
    "# Defining preprocessing steps\n",
    "scaler = MinMaxScaler()\n",
    "imputer = SimpleImputer()\n",
    "\n",
    "# Defining stratified k-fold\n",
    "stratified_kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Creating the voting classifier\n",
    "voting_clf = VotingClassifier(estimators=dis_models, voting='soft')\n",
    "\n",
    "# Defining pipeline\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('Robust', scaler),\n",
    "    ('Simple', imputer),\n",
    "    ('Classifier', voting_clf)\n",
    "])\n",
    "\n",
    "weights_combinations = [\n",
    "    None,  # No weights\n",
    "    [1, 1, 1, 1, 1],  # Equal weights\n",
    "    [2, 1, 1, 1, 2],  # Higher weight for first and last estimator\n",
    "    [1, 1, 2, 1, 1],  # Higher weight for third estimator\n",
    "    [1, 2, 1, 2, 1],  # Higher weight for second and fourth estimator\n",
    "    [1, 1, 1, 2, 2],  # Higher weight for last two estimators\n",
    "    [0.5, 1, 1.5, 1, 0.5],  # Fractional weights with emphasis on third estimator\n",
    "    [0.8, 0.8, 1.2, 0.8, 0.8],  # Slightly different weights\n",
    "    [1, 0.5, 1.5, 0.5, 1],  # Unequal weights with emphasis on third estimator\n",
    "    [1, 1, 1, 0.5, 0.5],  # Unequal weights with emphasis on last two estimators\n",
    "    [0.1, 0.1, 0.1, 0.1, 10],  # Extreme weight for last estimator\n",
    "    [5, 1, 1, 1, 1],\n",
    "    [7, 1, 0.1, 0.1, 0.1],  # Dominant weight for first estimator, toned down weights for others\n",
    "    [0.1, 10, 0.1, 0.1, 0.1],  # Dominant weight for second estimator, toned down weights for others\n",
    "    [0.1, 0.1, 10, 0.1, 0.1],  # Dominant weight for third estimator, toned down weights for others\n",
    "    [0.1, 0.1, 0.1, 10, 0.1],  # Dominant weight for fourth estimator, toned down weights for others\n",
    "    [0.1, 0.1, 0.1, 0.1, 10],  # Dominant weight for fifth estimator, toned down weights for others# Extreme weight for first estimator\n",
    "    [10, 1, 1, 10, 1],\n",
    "    [2, 5, 3, 1, 1]\n",
    "]\n",
    "\n",
    "\n",
    "# Defining parameter grid for hyperparameter tuning\n",
    "param_grid = {\n",
    "    'Classifier__weights': weights_combinations,  # Weights for soft voting\n",
    "}\n",
    "\n",
    "# Performing grid search with cross-validation\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=stratified_kfold, scoring='f1_macro', verbose=1, n_jobs=-1)\n",
    "grid_search.fit(X_dis_selected, y_dis)\n",
    "\n",
    "# Printing the best parameters found\n",
    "print(\"Best parameters:\", grid_search.best_params_)\n",
    "print(\"Best F1-macro score:\", grid_search.best_score_)\n",
    "\n",
    "# Printing results\n",
    "print(\"All results:\")\n",
    "means = grid_search.cv_results_['mean_test_score']\n",
    "stds = grid_search.cv_results_['std_test_score']\n",
    "for mean, std, params in zip(means, stds, grid_search.cv_results_['params']):\n",
    "    print(f\"Mean F1-macro: {mean:.4f} (Std: {std:.4f}) with params: {params}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f32f623-7aed-40e4-a8dd-362b1ba727c8",
   "metadata": {},
   "source": [
    "### Hard voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c9d96c7b-1366-4ce2-9a25-d05fffed9deb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 19 candidates, totalling 95 fits\n",
      "Best parameters: {'Classifier__weights': [0.1, 0.1, 0.1, 0.1, 10]}\n",
      "Best F1-macro score: 0.9270874323416697\n",
      "All results:\n",
      "Mean F1-macro: 0.9155 (Std: 0.0829) with params: {'Classifier__weights': None}\n",
      "Mean F1-macro: 0.9155 (Std: 0.0829) with params: {'Classifier__weights': [1, 1, 1, 1, 1]}\n",
      "Mean F1-macro: 0.9155 (Std: 0.0829) with params: {'Classifier__weights': [2, 1, 1, 1, 2]}\n",
      "Mean F1-macro: 0.9155 (Std: 0.0829) with params: {'Classifier__weights': [1, 1, 2, 1, 1]}\n",
      "Mean F1-macro: 0.9155 (Std: 0.0829) with params: {'Classifier__weights': [1, 2, 1, 2, 1]}\n",
      "Mean F1-macro: 0.9163 (Std: 0.0650) with params: {'Classifier__weights': [1, 1, 1, 2, 2]}\n",
      "Mean F1-macro: 0.9155 (Std: 0.0829) with params: {'Classifier__weights': [0.5, 1, 1.5, 1, 0.5]}\n",
      "Mean F1-macro: 0.9155 (Std: 0.0829) with params: {'Classifier__weights': [0.8, 0.8, 1.2, 0.8, 0.8]}\n",
      "Mean F1-macro: 0.9155 (Std: 0.0829) with params: {'Classifier__weights': [1, 0.5, 1.5, 0.5, 1]}\n",
      "Mean F1-macro: 0.9155 (Std: 0.0829) with params: {'Classifier__weights': [1, 1, 1, 0.5, 0.5]}\n",
      "Mean F1-macro: 0.9271 (Std: 0.0516) with params: {'Classifier__weights': [0.1, 0.1, 0.1, 0.1, 10]}\n",
      "Mean F1-macro: 0.9155 (Std: 0.0829) with params: {'Classifier__weights': [5, 1, 1, 1, 1]}\n",
      "Mean F1-macro: 0.9155 (Std: 0.0829) with params: {'Classifier__weights': [7, 1, 0.1, 0.1, 0.1]}\n",
      "Mean F1-macro: 0.8889 (Std: 0.1145) with params: {'Classifier__weights': [0.1, 10, 0.1, 0.1, 0.1]}\n",
      "Mean F1-macro: 0.8930 (Std: 0.0779) with params: {'Classifier__weights': [0.1, 0.1, 10, 0.1, 0.1]}\n",
      "Mean F1-macro: 0.8953 (Std: 0.0686) with params: {'Classifier__weights': [0.1, 0.1, 0.1, 10, 0.1]}\n",
      "Mean F1-macro: 0.9271 (Std: 0.0516) with params: {'Classifier__weights': [0.1, 0.1, 0.1, 0.1, 10]}\n",
      "Mean F1-macro: 0.9155 (Std: 0.0829) with params: {'Classifier__weights': [10, 1, 1, 10, 1]}\n",
      "Mean F1-macro: 0.9155 (Std: 0.0829) with params: {'Classifier__weights': [2, 5, 3, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold, GridSearchCV\n",
    "import numpy as np\n",
    "\n",
    "# Defining preprocessing steps\n",
    "scaler = MinMaxScaler()\n",
    "imputer = SimpleImputer()\n",
    "\n",
    "# Defining stratified k-fold\n",
    "stratified_kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Creating the voting classifier\n",
    "voting_clf = VotingClassifier(estimators=dis_models, voting='hard')\n",
    "\n",
    "# Defining pipeline\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('Robust', scaler),\n",
    "    ('Simple', imputer),\n",
    "    ('Classifier', voting_clf)\n",
    "])\n",
    "\n",
    "weights_combinations = [\n",
    "    None,  # No weights\n",
    "    [1, 1, 1, 1, 1],  # Equal weights\n",
    "    [2, 1, 1, 1, 2],  # Higher weight for first and last estimator\n",
    "    [1, 1, 2, 1, 1],  # Higher weight for third estimator\n",
    "    [1, 2, 1, 2, 1],  # Higher weight for second and fourth estimator\n",
    "    [1, 1, 1, 2, 2],  # Higher weight for last two estimators\n",
    "    [0.5, 1, 1.5, 1, 0.5],  # Fractional weights with emphasis on third estimator\n",
    "    [0.8, 0.8, 1.2, 0.8, 0.8],  # Slightly different weights\n",
    "    [1, 0.5, 1.5, 0.5, 1],  # Unequal weights with emphasis on third estimator\n",
    "    [1, 1, 1, 0.5, 0.5],  # Unequal weights with emphasis on last two estimators\n",
    "    [0.1, 0.1, 0.1, 0.1, 10],  # Extreme weight for last estimator\n",
    "    [5, 1, 1, 1, 1],\n",
    "    [7, 1, 0.1, 0.1, 0.1],  # Dominant weight for first estimator, toned down weights for others\n",
    "    [0.1, 10, 0.1, 0.1, 0.1],  # Dominant weight for second estimator, toned down weights for others\n",
    "    [0.1, 0.1, 10, 0.1, 0.1],  # Dominant weight for third estimator, toned down weights for others\n",
    "    [0.1, 0.1, 0.1, 10, 0.1],  # Dominant weight for fourth estimator, toned down weights for others\n",
    "    [0.1, 0.1, 0.1, 0.1, 10],  # Dominant weight for fifth estimator, toned down weights for others# Extreme weight for first estimator\n",
    "    [10, 1, 1, 10, 1],\n",
    "    [2, 5, 3, 1, 1]\n",
    "]\n",
    "\n",
    "\n",
    "# Defining parameter grid for hyperparameter tuning\n",
    "param_grid = {\n",
    "    'Classifier__weights': weights_combinations,  # Weights for soft voting\n",
    "}\n",
    "\n",
    "# Performing grid search with cross-validation\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=stratified_kfold, scoring='f1_macro', verbose=1, n_jobs=-1)\n",
    "grid_search.fit(X_dis_selected, y_dis)\n",
    "\n",
    "# Printing the best parameters found\n",
    "print(\"Best parameters:\", grid_search.best_params_)\n",
    "print(\"Best F1-macro score:\", grid_search.best_score_)\n",
    "\n",
    "# Printing results\n",
    "print(\"All results:\")\n",
    "means = grid_search.cv_results_['mean_test_score']\n",
    "stds = grid_search.cv_results_['std_test_score']\n",
    "for mean, std, params in zip(means, stds, grid_search.cv_results_['params']):\n",
    "    print(f\"Mean F1-macro: {mean:.4f} (Std: {std:.4f}) with params: {params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a5dd8cc-49e9-4970-aadd-edd15ec00ccc",
   "metadata": {},
   "source": [
    "### Stacking classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c815c2b1-73b0-4d2c-a2d0-b869cc883e62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1792 candidates, totalling 8960 fits\n",
      "Best parameters: {'classifier__final_estimator__C': 0.5, 'classifier__final_estimator__class_weight': None, 'classifier__final_estimator__coef0': 0.04, 'classifier__final_estimator__degree': 14, 'classifier__final_estimator__gamma': 'scale', 'classifier__final_estimator__kernel': 'rbf', 'classifier__final_estimator__max_iter': 6000, 'classifier__final_estimator__probability': False, 'classifier__final_estimator__shrinking': True}\n",
      "Best F1-macro score: 0.9009176453764051\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "import numpy as np\n",
    "\n",
    "# Defining preprocessing steps\n",
    "scaler = RobustScaler()\n",
    "imputer = SimpleImputer()\n",
    "\n",
    "# Defining stratified k-fold\n",
    "stratified_kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "dis_models_comb = [\n",
    "    ('KNN', KNeighborsClassifier(algorithm='auto', leaf_size=5, n_neighbors=6, p=2, weights='distance', n_jobs=-1)),\n",
    "    ('LGBM', lgb.LGBMClassifier(boosting_type='dart', class_weight=None, colsample_bytree=0.7, learning_rate=0.2, max_depth=2, n_estimators=87, num_leaves=3, objective='binary', random_state=42, n_jobs=-1, verbose=-1))\n",
    "]\n",
    "\n",
    "# Creating the stacking classifier with SVM as final estimator\n",
    "svm_model = SVC(random_state=42)  # Change this according to your desired SVM parameters\n",
    "stacking_clf = StackingClassifier(estimators=dis_models_comb, final_estimator=svm_model, cv=stratified_kfold)\n",
    "\n",
    "# Defining pipeline\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('Robust', scaler),\n",
    "    ('Simple', imputer),\n",
    "    ('classifier', stacking_clf)\n",
    "])\n",
    "\n",
    "# Defining parameter grid for hyperparameter tuning\n",
    "params_svc = {\n",
    "    'classifier__final_estimator__C': [0.5, 0.75, 0.875, 1.0, 1.125, 2, 3],                      #1.0\n",
    "    'classifier__final_estimator__kernel': ['rbf'],                                  #rbf\n",
    "    'classifier__final_estimator__degree': [14, 15, 16, 25],             #15\n",
    "    'classifier__final_estimator__gamma': ['scale', 'auto'],                         #auto\n",
    "    'classifier__final_estimator__coef0': [0.04, 0.05, 0.1, 10],                                                   #0.1\n",
    "    'classifier__final_estimator__shrinking': [True],                                                      #True\n",
    "    'classifier__final_estimator__probability': [False],                                                    #False\n",
    "    'classifier__final_estimator__class_weight': ['balanced', None],                                              #None\n",
    "    'classifier__final_estimator__max_iter': [6000, 7000, 8000, 9000]   #8000\n",
    "}\n",
    "\n",
    "\n",
    "# Performing grid search with cross-validation\n",
    "grid_search = GridSearchCV(pipeline, params_svc, cv=stratified_kfold, scoring='f1_macro', verbose=1, n_jobs=-1)\n",
    "grid_search.fit(X_anx_selected, y_anx)\n",
    "\n",
    "# Printing the best parameters found\n",
    "print(\"Best parameters:\", grid_search.best_params_)\n",
    "print(\"Best F1-macro score:\", grid_search.best_score_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b36259-a16c-45d9-be5f-fbc70247a0f7",
   "metadata": {},
   "source": [
    "# Tuning Stacking classifier best meta learner"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "889e8679-98a2-43ee-9620-ae6a320b92ab",
   "metadata": {},
   "source": [
    "## Anxiety"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "878b3ce4-d09b-4533-87e3-d09f2e3e80c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 540 candidates, totalling 2700 fits\n",
      "Best parameters: {'classifier__final_estimator__C': 1.0, 'classifier__final_estimator__class_weight': None, 'classifier__final_estimator__coef0': 0.05, 'classifier__final_estimator__degree': 14, 'classifier__final_estimator__gamma': 'scale', 'classifier__final_estimator__kernel': 'rbf', 'classifier__final_estimator__max_iter': 7000, 'classifier__final_estimator__probability': False, 'classifier__final_estimator__shrinking': True}\n",
      "Best F1-macro score: 0.9258665341660759\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "import numpy as np\n",
    "\n",
    "# Defining preprocessing steps\n",
    "scaler = RobustScaler()\n",
    "imputer = SimpleImputer()\n",
    "\n",
    "# Defining stratified k-fold\n",
    "stratified_kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Defining hypertuned base learners for stacking classifier\n",
    "anx_models_comb = [\n",
    "    ('Random Forest', RandomForestClassifier(class_weight = 'balanced', max_depth = None, max_features = 'log2', min_samples_leaf = 3, min_samples_split = 7, n_estimators = 90, random_state=42)),\n",
    "    ('SVM', SVC(C = 1.0, class_weight = None, coef0 = 0.05, degree = 12, gamma = 'auto', kernel = 'rbf', max_iter = 7000, probability = True, shrinking = True, random_state=42)),\n",
    "    ('KNN', KNeighborsClassifier(algorithm = 'auto', leaf_size = 10, n_neighbors = 5, p = 1, weights = 'uniform',  n_jobs=-1)),\n",
    "    ('XGB', xgb.XGBClassifier(colsample_bytree = 0.9, learning_rate = 0.4, max_depth = 3, min_child_weight = 1, n_estimators = 162, subsample = 0.9, random_state=42, n_jobs=-1)),\n",
    "]\n",
    "\n",
    "# Creating the stacking classifier with SVM as final estimator\n",
    "svm_model = SVC(random_state=42)  # Change this according to your desired SVM parameters\n",
    "stacking_clf = StackingClassifier(estimators=anx_models_comb, final_estimator=svm_model, cv=stratified_kfold)\n",
    "\n",
    "# Defining pipeline\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('Robust', scaler),\n",
    "    ('Simple', imputer),\n",
    "    ('classifier', stacking_clf)\n",
    "])\n",
    "\n",
    "\n",
    "# Defining parameter grid for hyperparameter tuning\n",
    "params_svc = {\n",
    "    'classifier__final_estimator__C': [0.5, 0.75, 0.875, 1.0, 1.125],                      #1.0\n",
    "    'classifier__final_estimator__kernel': ['rbf'],                                  #rbf\n",
    "    'classifier__final_estimator__degree': [14, 15, 16],             #15\n",
    "    'classifier__final_estimator__gamma': ['scale', 'auto'],                         #auto\n",
    "    'classifier__final_estimator__coef0': [0.04, 0.05, 0.1],                                                   #0.1\n",
    "    'classifier__final_estimator__shrinking': [True],                                                      #True\n",
    "    'classifier__final_estimator__probability': [False],                                                    #False\n",
    "    'classifier__final_estimator__class_weight': ['balanced', None],                                              #None\n",
    "    'classifier__final_estimator__max_iter': [6000, 7000, 8000]   #8000\n",
    "}\n",
    "\n",
    "# Performing grid search with cross-validation\n",
    "grid_search = GridSearchCV(pipeline, params_svc, cv=stratified_kfold, scoring='f1_macro', verbose=1, n_jobs=-1)\n",
    "grid_search.fit(X_anx_selected, y_anx)\n",
    "\n",
    "# Printing the best parameters found\n",
    "print(\"Best parameters:\", grid_search.best_params_)\n",
    "print(\"Best F1-macro score:\", grid_search.best_score_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e43f97b9-1b8b-4203-bdfc-02c7e41aca3f",
   "metadata": {},
   "source": [
    "## Distress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b1ea41a1-5d8e-48ea-b28c-4d5d84f33dd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1792 candidates, totalling 8960 fits\n",
      "Best parameters: {'classifier__final_estimator__C': 0.5, 'classifier__final_estimator__class_weight': None, 'classifier__final_estimator__coef0': 0.04, 'classifier__final_estimator__degree': 14, 'classifier__final_estimator__gamma': 'scale', 'classifier__final_estimator__kernel': 'rbf', 'classifier__final_estimator__max_iter': 6000, 'classifier__final_estimator__probability': False, 'classifier__final_estimator__shrinking': True}\n",
      "Best F1-macro score: 0.9009176453764051\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "import numpy as np\n",
    "\n",
    "# Defining preprocessing steps\n",
    "scaler = RobustScaler()\n",
    "imputer = SimpleImputer()\n",
    "\n",
    "# Defining stratified k-fold\n",
    "stratified_kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Defining hypertuned base learners for stacking classifier\n",
    "dis_models_comb = [\n",
    "    ('KNN', KNeighborsClassifier(algorithm='auto', leaf_size=5, n_neighbors=6, p=2, weights='distance', n_jobs=-1)),\n",
    "    ('LGBM', lgb.LGBMClassifier(boosting_type='dart', class_weight=None, colsample_bytree=0.7, learning_rate=0.2, max_depth=2, n_estimators=87, num_leaves=3, objective='binary', random_state=42, n_jobs=-1, verbose=-1))\n",
    "]\n",
    "\n",
    "# Creating the stacking classifier with SVM as final estimator\n",
    "svm_model = SVC(random_state=42)  # Change this according to your desired SVM parameters\n",
    "stacking_clf = StackingClassifier(estimators=dis_models_comb, final_estimator=svm_model, cv=stratified_kfold)\n",
    "\n",
    "# Defining pipeline\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('Robust', scaler),\n",
    "    ('Simple', imputer),\n",
    "    ('classifier', stacking_clf)\n",
    "])\n",
    "\n",
    "# Defining parameter grid for hyperparameter tuning\n",
    "params_svc = {\n",
    "    'classifier__final_estimator__C': [0.5, 0.75, 0.875, 1.0, 1.125, 2, 3],                      #1.0\n",
    "    'classifier__final_estimator__kernel': ['rbf'],                                  #rbf\n",
    "    'classifier__final_estimator__degree': [14, 15, 16, 25],             #15\n",
    "    'classifier__final_estimator__gamma': ['scale', 'auto'],                         #auto\n",
    "    'classifier__final_estimator__coef0': [0.04, 0.05, 0.1, 10],                                                   #0.1\n",
    "    'classifier__final_estimator__shrinking': [True],                                                      #True\n",
    "    'classifier__final_estimator__probability': [False],                                                    #False\n",
    "    'classifier__final_estimator__class_weight': ['balanced', None],                                              #None\n",
    "    'classifier__final_estimator__max_iter': [6000, 7000, 8000, 9000]   #8000\n",
    "}\n",
    "\n",
    "\n",
    "# Performing grid search with cross-validation\n",
    "grid_search = GridSearchCV(pipeline, params_svc, cv=stratified_kfold, scoring='f1_macro', verbose=1, n_jobs=-1)\n",
    "grid_search.fit(X_anx_selected, y_anx)\n",
    "\n",
    "# Printing the best parameters found\n",
    "print(\"Best parameters:\", grid_search.best_params_)\n",
    "print(\"Best F1-macro score:\", grid_search.best_score_)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 ",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
