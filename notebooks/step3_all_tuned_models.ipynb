{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e3bf074-96f2-46d4-afeb-0ab3b66433be",
   "metadata": {},
   "source": [
    "# Handling libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8697ca5a-481e-48e1-a2cc-def2940b33a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Installing libraries without displaying output\n",
    "!pip install scikit-learn\n",
    "!pip install missingno\n",
    "!pip install xgboost\n",
    "!pip install imbalanced-learn\n",
    "!pip install fancyimpute\n",
    "!pip install tensorflow\n",
    "!pip install tabulate\n",
    "!pip install statsmodels\n",
    "!pip install lightgbm\n",
    "#!pip install yellowbrick\n",
    "!pip install scikit-optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f86d3927-c2bf-4324-84b6-89dafbf554c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import datasets\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from collections import Counter\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from imblearn.combine import SMOTEENN\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.feature_selection import RFECV\n",
    "from imblearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from imblearn.combine import SMOTEENN\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from imblearn.over_sampling import SMOTENC\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "#from yellowbrick.features import RFECV\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "import numpy as np\n",
    "\n",
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53090f13-b168-4943-a57d-1ebb6b5720d3",
   "metadata": {},
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc0a4d88-7737-4ed4-bd5d-1f9967b6b7f4",
   "metadata": {},
   "source": [
    "## Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "725f08b4-0ffc-47d2-9281-fe8396db0483",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Anxiety ------------------------------------\n",
    "# loading feature engineered variables\n",
    "anx_data=pd.read_csv('df_anx_t6_2.csv')\n",
    "\n",
    "seed = 42 \n",
    "\n",
    "# Loading in data\n",
    "X_anx = anx_data.iloc[:, :-1]\n",
    "y_anx = anx_data.iloc[:, -1]\n",
    "\n",
    "\n",
    "# Distressed ---------------------------------\n",
    "# loading feature engineered variables\n",
    "dis_data=pd.read_csv('data_2.csv')\n",
    "\n",
    "seed = 42 \n",
    "\n",
    "# Loading in data\n",
    "X_dis = dis_data.iloc[:, :-1]\n",
    "y_dis = dis_data.iloc[:, -1]\n",
    "\n",
    "# Removing redundant features\n",
    "X_dis = X_dis.drop(X_dis.columns[0], axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c7df58a-1824-460e-b907-99710fe6ffce",
   "metadata": {},
   "source": [
    "# Bayesian optimization for selected models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ac9207-1dd8-45cd-9544-27c642c5cc8b",
   "metadata": {},
   "source": [
    "## Anxiety"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b14c3081-be05-41a3-bd71-5530439cdf9a",
   "metadata": {},
   "source": [
    "### RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "3bc1d8b2-61df-42bd-a52c-3253a1f34704",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Best hyperparameters found:\n",
      "OrderedDict([('Classifier__class_weight', None), ('Classifier__max_features', 'log2'), ('Classifier__min_samples_leaf', 1), ('Classifier__min_samples_split', 6), ('Classifier__n_estimators', 116)])\n",
      "Best RandomForestClassifier cross-validation F1-macro score: 0.9176092547223558\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from skopt import BayesSearchCV\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "# Selected features for anxiety is used\n",
    "selected_features = ['ucla_t6_sum', 'summary_score_kccq_base', 'eq5d5l_index_t6', 'MCS_t6', 'ImplWeight', 'personality_type_D']\n",
    "X_selected = X_anx[selected_features]\n",
    "\n",
    "# Defining search spaces for RF-classifier\n",
    "rf_search_space = {\n",
    "    \"Classifier__n_estimators\": (5, 150),\n",
    "    \"Classifier__max_depth\": (None, 1, 50),\n",
    "    \"Classifier__min_samples_split\": (2, 10),\n",
    "    \"Classifier__min_samples_leaf\": (1, 10),\n",
    "    'Classifier__max_features': ['log2', 'sqrt'],\n",
    "    'Classifier__class_weight': ['balanced', 'balanced_subsample', None]\n",
    "    }\n",
    "\n",
    "# Defining Classifier\n",
    "classifier = RandomForestClassifier(random_state=42)\n",
    "stratified_kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "\n",
    "# Defining preprocessing steps\n",
    "scaler = RobustScaler()\n",
    "imputer = SimpleImputer()\n",
    "\n",
    "# Defining pipeline\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('Robust', scaler),\n",
    "    ('Simple', imputer),\n",
    "    ('Classifier', classifier)\n",
    "])\n",
    "\n",
    "# Performing Bayesian optimization with F1-macro score\n",
    "opt = BayesSearchCV(\n",
    "    pipeline,\n",
    "    rf_search_space,\n",
    "    scoring='f1_macro',\n",
    "    n_iter=30,  # Adjust this based on computational resources and time\n",
    "    cv=stratified_kfold,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "opt.fit(X_selected, y_anx)\n",
    "\n",
    "print(\"Best hyperparameters found:\")\n",
    "print(opt.best_params_)\n",
    "print(f\"Best {classifier.__class__.__name__} cross-validation F1-macro score: {opt.best_score_}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f50b945-13cf-4acd-aed5-264ac8438e76",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "aae29467-ea67-4981-8429-99fa2684e1d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Best hyperparameters found:\n",
      "OrderedDict([('classifier__C', 100), ('classifier__class_weight', None), ('classifier__coef0', 1), ('classifier__degree', 2), ('classifier__gamma', 'auto'), ('classifier__kernel', 'linear'), ('classifier__max_iter', 5000), ('classifier__probability', True), ('classifier__shrinking', True)])\n",
      "Best SVC cross-validation F1-macro score: 0.9063969665003148\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/sklearn/svm/_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from skopt import BayesSearchCV\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "selected_features = ['ucla_t6_sum', 'summary_score_kccq_base', 'eq5d5l_index_t6', 'MCS_t6', 'ImplWeight', 'personality_type_D']\n",
    "X_selected = X_anx[selected_features]\n",
    "\n",
    "\n",
    "# Defining search spaces for each model\n",
    "params_svc = {\n",
    "    'classifier__C': (90, 100),\n",
    "    'classifier__kernel': ['linear', 'rbf', 'sigmoid'],\n",
    "    'classifier__degree': (2, 100),\n",
    "    'classifier__gamma': ['auto'],\n",
    "    'classifier__coef0': (0, 1),\n",
    "    'classifier__shrinking': [True, False],\n",
    "    'classifier__probability': [True, False],\n",
    "    'classifier__class_weight': ['balanced', None],\n",
    "    'classifier__max_iter': (4000, 5000)\n",
    "}\n",
    "\n",
    "classifier = SVC(random_state=42)\n",
    "stratified_kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "\n",
    "# Defining preprocessing steps\n",
    "scaler = RobustScaler()\n",
    "imputer = SimpleImputer()\n",
    "\n",
    "# Defining pipeline\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('Robust', scaler),\n",
    "    ('Simple', imputer),\n",
    "    ('classifier', classifier)\n",
    "])\n",
    "\n",
    "# Performing Bayesian optimization with F1-macro score\n",
    "opt = BayesSearchCV(\n",
    "    pipeline,\n",
    "    params_svc,\n",
    "    scoring='f1_macro',\n",
    "    n_iter=30,\n",
    "    cv=stratified_kfold,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "opt.fit(X_selected, y_anx)\n",
    "\n",
    "print(\"Best hyperparameters found:\")\n",
    "print(opt.best_params_)\n",
    "print(f\"Best {classifier.__class__.__name__} cross-validation F1-macro score: {opt.best_score_}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0124986-7513-46e7-a763-620960ff9bf3",
   "metadata": {},
   "source": [
    "### KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "7ace2832-f448-4b5d-a558-47e8aae1d607",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/skopt/space/space.py:110: UserWarning: Dimension [1, 2] was inferred to Integer(low=1, high=2, prior='uniform', transform='identity'). In upcoming versions of scikit-optimize, it will be inferred to Categorical(categories=(1, 2), prior=None). See the documentation of the check_dimension function for the upcoming API.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/skopt/space/space.py:110: UserWarning: Dimension [1, 2] was inferred to Integer(low=1, high=2, prior='uniform', transform='identity'). In upcoming versions of scikit-optimize, it will be inferred to Categorical(categories=(1, 2), prior=None). See the documentation of the check_dimension function for the upcoming API.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/skopt/space/space.py:110: UserWarning: Dimension [1, 2] was inferred to Integer(low=1, high=2, prior='uniform', transform='identity'). In upcoming versions of scikit-optimize, it will be inferred to Categorical(categories=(1, 2), prior=None). See the documentation of the check_dimension function for the upcoming API.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Best hyperparameters found:\n",
      "OrderedDict([('classifier__algorithm', 'kd_tree'), ('classifier__leaf_size', 20), ('classifier__n_neighbors', 5), ('classifier__p', 1), ('classifier__weights', 'distance')])\n",
      "Best KNeighborsClassifier cross-validation F1-macro score: 0.912602819606272\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from skopt import BayesSearchCV\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "# Selected features Anxiety present detection\n",
    "selected_features = ['ucla_t6_sum', 'summary_score_kccq_base', 'eq5d5l_index_t6', 'MCS_t6', 'ImplWeight', 'personality_type_D']\n",
    "X_selected = X_anx[selected_features]\n",
    "\n",
    "\n",
    "# Defining search spaces for each model\n",
    "params_knn = {\n",
    "    'classifier__n_neighbors': (1, 10),\n",
    "    'classifier__weights': ['uniform', 'distance'],\n",
    "    'classifier__algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'],\n",
    "    'classifier__leaf_size': (1, 20),\n",
    "    'classifier__p': [1, 2],\n",
    "}\n",
    "\n",
    "classifier = KNeighborsClassifier(n_jobs=-1)\n",
    "stratified_kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "\n",
    "# Defining preprocessing steps\n",
    "scaler = RobustScaler()\n",
    "imputer = SimpleImputer()\n",
    "\n",
    "# Defining pipeline\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('Robust', scaler),\n",
    "    ('Simple', imputer),\n",
    "    ('classifier', classifier)\n",
    "])\n",
    "\n",
    "# Performing Bayesian optimization with F1-macro score\n",
    "opt = BayesSearchCV(\n",
    "    pipeline,\n",
    "    params_knn,\n",
    "    scoring='f1_macro',\n",
    "    n_iter=30, \n",
    "    cv=stratified_kfold,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "opt.fit(X_selected, y_anx)\n",
    "\n",
    "print(\"Best hyperparameters found:\")\n",
    "print(opt.best_params_)\n",
    "print(f\"Best {classifier.__class__.__name__} cross-validation F1-macro score: {opt.best_score_}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b13bcc7-4b24-4477-b11b-7036a6ad7447",
   "metadata": {},
   "source": [
    "### XGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2bd016fe-ff8b-4fb5-bc73-fa9bc370cf76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Best hyperparameters found:\n",
      "OrderedDict([('classifier__colsample_bytree', 0.85), ('classifier__learning_rate', 1.0), ('classifier__max_depth', 44), ('classifier__min_child_weight', 2), ('classifier__n_estimators', 200), ('classifier__n_jobs', -1), ('classifier__subsample', 0.6)])\n",
      "Best XGBClassifier cross-validation F1-macro score: 0.9115101955772348\n",
      "\n"
     ]
    }
   ],
   "source": [
    "selected_features = ['ucla_t6_sum', 'summary_score_kccq_base', 'eq5d5l_index_t6', 'MCS_t6', 'ImplWeight', 'personality_type_D']\n",
    "X_selected = X_anx[selected_features]\n",
    "\n",
    "\n",
    "# Defining search spaces for each model\n",
    "params_xgb = {\n",
    "    'classifier__n_estimators': (5, 200),\n",
    "    'classifier__max_depth': (1, 50),\n",
    "    'classifier__learning_rate': (0.0001, 1),\n",
    "    'classifier__min_child_weight': (1, 15),\n",
    "    'classifier__subsample': [0.3, 0.5, 0.6, 0.7, 0.8, 0.9, 1],\n",
    "    'classifier__colsample_bytree': [0.3, 0.5, 0.7, 0.75, 0.8, 0.85, 0.9, 1],\n",
    "    'classifier__n_jobs': [-1]\n",
    "}\n",
    "\n",
    "# Defining classifier\n",
    "classifier = xgb.XGBClassifier(random_state=42, n_jobs=-1)\n",
    "stratified_kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "\n",
    "# Defining preprocessing steps\n",
    "scaler = RobustScaler()\n",
    "imputer = SimpleImputer()\n",
    "\n",
    "# Defining pipeline\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('Robust', scaler),\n",
    "    ('Simple', imputer),\n",
    "    ('classifier', classifier)\n",
    "])\n",
    "\n",
    "# Performing Bayesian optimization with F1-macro score\n",
    "opt = BayesSearchCV(\n",
    "    pipeline,\n",
    "    params_xgb,\n",
    "    scoring='f1_macro',\n",
    "    n_iter=50, \n",
    "    cv=stratified_kfold,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "opt.fit(X_selected, y_anx)\n",
    "\n",
    "print(\"Best hyperparameters found:\")\n",
    "print(opt.best_params_)\n",
    "print(f\"Best {classifier.__class__.__name__} cross-validation F1-macro score: {opt.best_score_}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b3f1c1-181b-4322-a6f3-04214f9d5a74",
   "metadata": {},
   "source": [
    "### LGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "e2cca9d7-aad8-41a5-902c-3695f73830ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "[LightGBM] [Info] Number of positive: 35, number of negative: 443\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000081 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 309\n",
      "[LightGBM] [Info] Number of data points in the train set: 478, number of used features: 6\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.073222 -> initscore=-2.538222\n",
      "[LightGBM] [Info] Start training from score -2.538222\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "Best hyperparameters found:\n",
      "OrderedDict([('classifier__boosting_type', 'dart'), ('classifier__learning_rate', 0.05), ('classifier__max_depth', -1), ('classifier__n_estimators', 125), ('classifier__num_leaves', 50), ('classifier__objective', 'binary')])\n",
      "Best LGBMClassifier cross-validation F1-macro score: 0.8970533755610927\n",
      "\n"
     ]
    }
   ],
   "source": [
    "selected_features = ['ucla_t6_sum', 'summary_score_kccq_base', 'eq5d5l_index_t6', 'MCS_t6', 'ImplWeight', 'personality_type_D']\n",
    "X_selected = X_anx[selected_features]\n",
    "\n",
    "\n",
    "# Defining search spaces for each model\n",
    "params_lgbm = {\n",
    "    'classifier__n_estimators': (5, 300),\n",
    "    'classifier__boosting_type': ['gbdt', 'dart'],\n",
    "    'classifier__num_leaves': (2, 50),\n",
    "    'classifier__max_depth': (-1, 50),  #Default = -1\n",
    "    'classifier__learning_rate': [0.001, 0.05, 0.1, 0.2, 0.3, 0.5, 1],  #Default = 0.1\n",
    "    'classifier__objective': ['binary', None], #Default = None. \n",
    "    }\n",
    "\n",
    "# Defining classifier\n",
    "classifier = lgb.LGBMClassifier(random_state=42, n_jobs =-1)\n",
    "stratified_kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "\n",
    "# Defining preprocessing steps\n",
    "scaler = RobustScaler()\n",
    "imputer = SimpleImputer()\n",
    "\n",
    "# Defining pipeline\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('Robust', scaler),\n",
    "    ('Simple', imputer),\n",
    "    ('classifier', classifier)\n",
    "])\n",
    "\n",
    "# Performing Bayesian optimization with F1-macro score\n",
    "opt = BayesSearchCV(\n",
    "    pipeline,\n",
    "    params_lgbm,\n",
    "    scoring='f1_macro',\n",
    "    n_iter=30,\n",
    "    cv=stratified_kfold,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "opt.fit(X_selected, y_anx)\n",
    "\n",
    "print(\"Best hyperparameters found:\")\n",
    "print(opt.best_params_)\n",
    "print(f\"Best {classifier.__class__.__name__} cross-validation F1-macro score: {opt.best_score_}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cde2fec6-a20b-4782-ab79-381c4c6c4f3d",
   "metadata": {},
   "source": [
    "## Distress"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f168e3-eae6-44e3-82d9-cee63ab83722",
   "metadata": {},
   "source": [
    "### RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "1fafc446-06ae-4db7-859b-9434dcdd745e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/skopt/optimizer/optimizer.py:517: UserWarning: The objective has been evaluated at point ['balanced_subsample', 5, 'sqrt', 1, 2, 30] before, using random point ['balanced_subsample', 74, 'sqrt', 11, 92, 108]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/skopt/optimizer/optimizer.py:517: UserWarning: The objective has been evaluated at point ['balanced_subsample', 5, 'sqrt', 1, 2, 30] before, using random point ['balanced', 57, 'log2', 6, 25, 45]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/skopt/optimizer/optimizer.py:517: UserWarning: The objective has been evaluated at point ['balanced_subsample', 5, 'sqrt', 1, 2, 30] before, using random point ['balanced', 66, 'log2', 30, 14, 107]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/skopt/optimizer/optimizer.py:517: UserWarning: The objective has been evaluated at point ['balanced_subsample', 5, 'sqrt', 1, 2, 30] before, using random point ['balanced', 73, 'sqrt', 39, 87, 72]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/skopt/optimizer/optimizer.py:517: UserWarning: The objective has been evaluated at point ['balanced_subsample', 5, 'sqrt', 1, 2, 30] before, using random point ['balanced_subsample', 75, 'log2', 15, 91, 49]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Best hyperparameters found:\n",
      "OrderedDict([('Classifier__class_weight', 'balanced'), ('Classifier__max_depth', 5), ('Classifier__max_features', 'sqrt'), ('Classifier__min_samples_leaf', 1), ('Classifier__min_samples_split', 2), ('Classifier__n_estimators', 30)])\n",
      "Best RandomForestClassifier cross-validation F1-macro score: 0.9088854087089381\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from skopt import BayesSearchCV\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "# Best selected features for Distressed is used\n",
    "selected_features = ['ucla_t6_sum', 'eq5d5l_index_t6', 'depression_base_score', 'icdc_t6_sum', 'MCS_t6', 'Return_to_function_t6', 'age', 'anxiety_base_score']\n",
    "X_dis = X_dis[selected_features]\n",
    "X_selected = X_dis[selected_features]\n",
    "\n",
    "# Defining search spaces for each model\n",
    "rf_search_space = {\n",
    "    \"Classifier__n_estimators\": (30, 130),\n",
    "    \"Classifier__max_depth\": (5, 100),\n",
    "    \"Classifier__min_samples_split\": (2, 100),\n",
    "    \"Classifier__min_samples_leaf\": (1, 50),\n",
    "    'Classifier__max_features': ['log2', 'sqrt'],\n",
    "    'Classifier__class_weight': ['balanced', 'balanced_subsample', None]\n",
    "    }\n",
    "\n",
    "# Defining classifier\n",
    "classifier = RandomForestClassifier(random_state=42)\n",
    "stratified_kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "\n",
    "# Defining preprocessing steps\n",
    "scaler = MinMaxScaler()\n",
    "imputer = SimpleImputer()\n",
    "\n",
    "# Defining pipeline\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('Robust', scaler),\n",
    "    ('Simple', imputer),\n",
    "    ('Classifier', classifier)\n",
    "])\n",
    "\n",
    "# Performing Bayesian optimization with F1-macro score\n",
    "opt = BayesSearchCV(\n",
    "    pipeline,\n",
    "    rf_search_space,\n",
    "    scoring='f1_macro',\n",
    "    n_iter=30, \n",
    "    cv=stratified_kfold,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "opt.fit(X_selected, y_dis)\n",
    "\n",
    "print(\"Best hyperparameters found:\")\n",
    "print(opt.best_params_)\n",
    "print(f\"Best {classifier.__class__.__name__} cross-validation F1-macro score: {opt.best_score_}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ddab345-7fda-4816-bfd5-41ae45913c27",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "2d908079-46df-4f8b-9e66-47eb029822e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Best hyperparameters found:\n",
      "OrderedDict([('classifier__C', 90), ('classifier__class_weight', None), ('classifier__coef0', 1), ('classifier__degree', 100), ('classifier__gamma', 'auto'), ('classifier__kernel', 'sigmoid'), ('classifier__max_iter', 4989), ('classifier__probability', True), ('classifier__shrinking', True)])\n",
      "Best SVC cross-validation F1-macro score: 0.8889376147546265\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from skopt import BayesSearchCV\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "selected_features = ['ucla_t6_sum', 'eq5d5l_index_t6', 'depression_base_score', 'icdc_t6_sum', 'MCS_t6', 'Return_to_function_t6', 'age', 'anxiety_base_score']\n",
    "X_selected = X_dis[selected_features]\n",
    "\n",
    "\n",
    "# Defining search spaces for each model\n",
    "params_svc = {\n",
    "    'classifier__C': (90, 100),\n",
    "    'classifier__kernel': ['linear', 'rbf', 'sigmoid'],\n",
    "    'classifier__degree': (2, 100),\n",
    "    'classifier__gamma': ['auto'],\n",
    "    'classifier__coef0': (0, 1),\n",
    "    'classifier__shrinking': [True, False],\n",
    "    'classifier__probability': [True, False],\n",
    "    'classifier__class_weight': ['balanced', None],\n",
    "    'classifier__max_iter': (4000, 5000)\n",
    "}\n",
    "\n",
    "# Defining classifier\n",
    "classifier = SVC(random_state=42)\n",
    "stratified_kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "\n",
    "# Defining preprocessing steps\n",
    "scaler = MinMaxScaler()\n",
    "imputer = SimpleImputer()\n",
    "\n",
    "# Defining pipeline\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('Robust', scaler),\n",
    "    ('Simple', imputer),\n",
    "    ('classifier', classifier)\n",
    "])\n",
    "\n",
    "# Performing Bayesian optimization with F1-macro score\n",
    "opt = BayesSearchCV(\n",
    "    pipeline,\n",
    "    params_svc,\n",
    "    scoring='f1_macro',\n",
    "    n_iter=30,\n",
    "    cv=stratified_kfold,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "opt.fit(X_selected, y_dis)\n",
    "\n",
    "print(\"Best hyperparameters found:\")\n",
    "print(opt.best_params_)\n",
    "print(f\"Best {classifier.__class__.__name__} cross-validation F1-macro score: {opt.best_score_}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c1742d0-c2fa-4b71-b6d1-e391ac9be8d7",
   "metadata": {},
   "source": [
    "### KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "2c1786b2-b70b-4be0-8735-147303c2fbef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/skopt/space/space.py:110: UserWarning: Dimension [1, 2] was inferred to Integer(low=1, high=2, prior='uniform', transform='identity'). In upcoming versions of scikit-optimize, it will be inferred to Categorical(categories=(1, 2), prior=None). See the documentation of the check_dimension function for the upcoming API.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/skopt/space/space.py:110: UserWarning: Dimension [1, 2] was inferred to Integer(low=1, high=2, prior='uniform', transform='identity'). In upcoming versions of scikit-optimize, it will be inferred to Categorical(categories=(1, 2), prior=None). See the documentation of the check_dimension function for the upcoming API.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/skopt/space/space.py:110: UserWarning: Dimension [1, 2] was inferred to Integer(low=1, high=2, prior='uniform', transform='identity'). In upcoming versions of scikit-optimize, it will be inferred to Categorical(categories=(1, 2), prior=None). See the documentation of the check_dimension function for the upcoming API.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/skopt/optimizer/optimizer.py:517: UserWarning: The objective has been evaluated at point ['ball_tree', 3, 6, 2, 'distance'] before, using random point ['kd_tree', 7, 3, 1, 'distance']\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Best hyperparameters found:\n",
      "OrderedDict([('classifier__algorithm', 'brute'), ('classifier__leaf_size', 4), ('classifier__n_neighbors', 6), ('classifier__p', 2), ('classifier__weights', 'distance')])\n",
      "Best KNeighborsClassifier cross-validation F1-macro score: 0.8929530181562623\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from skopt import BayesSearchCV\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "selected_features = ['ucla_t6_sum', 'eq5d5l_index_t6', 'depression_base_score', 'icdc_t6_sum', 'MCS_t6', 'Return_to_function_t6', 'age', 'anxiety_base_score']\n",
    "X_selected = X_dis[selected_features]\n",
    "\n",
    "\n",
    "# Defining search spaces for each model\n",
    "params_knn = {\n",
    "    'classifier__n_neighbors': (1, 10),\n",
    "    'classifier__weights': ['uniform', 'distance'],\n",
    "    'classifier__algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'],\n",
    "    'classifier__leaf_size': (1, 20),\n",
    "    'classifier__p': [1, 2],\n",
    "}\n",
    "\n",
    "# Defining classifier\n",
    "classifier = KNeighborsClassifier(n_jobs=-1)\n",
    "stratified_kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "\n",
    "# Defining preprocessing steps\n",
    "scaler = MinMaxScaler()\n",
    "imputer = SimpleImputer()\n",
    "\n",
    "# Defining pipeline\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('Robust', scaler),\n",
    "    ('Simple', imputer),\n",
    "    ('classifier', classifier)\n",
    "])\n",
    "\n",
    "# Performing Bayesian optimization with F1-macro score\n",
    "opt = BayesSearchCV(\n",
    "    pipeline,\n",
    "    params_knn,\n",
    "    scoring='f1_macro',\n",
    "    n_iter=30, \n",
    "    cv=stratified_kfold,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "opt.fit(X_selected, y_dis)\n",
    "\n",
    "print(\"Best hyperparameters found:\")\n",
    "print(opt.best_params_)\n",
    "print(f\"Best {classifier.__class__.__name__} cross-validation F1-macro score: {opt.best_score_}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "784b3817-5cfe-4e79-9dd2-48cabbf88ac0",
   "metadata": {},
   "source": [
    "### XGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "193f9e33-2398-4fe3-aaae-4739dcfeab45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Best hyperparameters found:\n",
      "OrderedDict([('classifier__colsample_bytree', 0.85), ('classifier__learning_rate', 0.6526424653680359), ('classifier__max_depth', 8), ('classifier__min_child_weight', 3), ('classifier__n_estimators', 92), ('classifier__n_jobs', -1), ('classifier__subsample', 1)])\n",
      "Best XGBClassifier cross-validation F1-macro score: 0.889160862464108\n",
      "\n"
     ]
    }
   ],
   "source": [
    "selected_features = ['ucla_t6_sum', 'eq5d5l_index_t6', 'depression_base_score', 'icdc_t6_sum', 'MCS_t6', 'Return_to_function_t6', 'age', 'anxiety_base_score']\n",
    "X_selected = X_dis[selected_features]\n",
    "\n",
    "\n",
    "# Defining search spaces for each model\n",
    "params_xgb = {\n",
    "    'classifier__n_estimators': (5, 100),\n",
    "    'classifier__max_depth': (1, 50),\n",
    "    'classifier__learning_rate': (0.0001, 1),\n",
    "    'classifier__min_child_weight': (1, 15),\n",
    "    'classifier__subsample': [0.3, 0.5, 0.6, 0.7, 0.8, 0.9, 1],\n",
    "    'classifier__colsample_bytree': [0.3, 0.5, 0.7, 0.75, 0.8, 0.85, 0.9, 1],\n",
    "    'classifier__n_jobs': [-1]\n",
    "}\n",
    "\n",
    "# Defining classifier\n",
    "classifier = xgb.XGBClassifier(random_state=42)\n",
    "stratified_kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "\n",
    "# Defining preprocessing steps\n",
    "scaler = MinMaxScaler()\n",
    "imputer = SimpleImputer()\n",
    "\n",
    "# Defining pipeline\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('Robust', scaler),\n",
    "    ('Simple', imputer),\n",
    "    ('classifier', classifier)\n",
    "])\n",
    "\n",
    "# Performing Bayesian optimization with F1-macro score\n",
    "opt = BayesSearchCV(\n",
    "    pipeline,\n",
    "    params_xgb,\n",
    "    scoring='f1_macro',\n",
    "    n_iter=30,\n",
    "    cv=stratified_kfold,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "opt.fit(X_selected, y_dis)\n",
    "\n",
    "print(\"Best hyperparameters found:\")\n",
    "print(opt.best_params_)\n",
    "print(f\"Best {classifier.__class__.__name__} cross-validation F1-macro score: {opt.best_score_}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35e2245e-3810-4195-ad21-9346d99047ed",
   "metadata": {},
   "source": [
    "### LGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "21de6d14-d713-4833-a8fb-1de7f05b5657",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "[LightGBM] [Info] Number of positive: 39, number of negative: 439\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000125 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 336\n",
      "[LightGBM] [Info] Number of data points in the train set: 478, number of used features: 8\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.081590 -> initscore=-2.420938\n",
      "[LightGBM] [Info] Start training from score -2.420938\n",
      "Best hyperparameters found:\n",
      "OrderedDict([('classifier__boosting_type', 'gbdt'), ('classifier__class_weight', None), ('classifier__colsample_bytree', 0.75), ('classifier__learning_rate', 0.1), ('classifier__max_depth', 25), ('classifier__n_estimators', 150), ('classifier__num_leaves', 2), ('classifier__objective', None)])\n",
      "Best LGBMClassifier cross-validation F1-macro score: 0.9052603356810602\n",
      "\n"
     ]
    }
   ],
   "source": [
    "selected_features = ['ucla_t6_sum', 'eq5d5l_index_t6', 'depression_base_score', 'icdc_t6_sum', 'MCS_t6', 'Return_to_function_t6', 'age', 'anxiety_base_score']\n",
    "X_selected = X_dis[selected_features]\n",
    "\n",
    "\n",
    "# Defining search spaces for each model\n",
    "params_lgbm = {\n",
    "    'classifier__n_estimators': (5, 150),\n",
    "    'classifier__boosting_type': ['gbdt', 'dart'],\n",
    "    'classifier__num_leaves': (2, 25),\n",
    "    'classifier__max_depth': (-1, 25),  #Default = -1\n",
    "    'classifier__learning_rate': [0.001, 0.05, 0.1, 0.2, 0.3, 0.5, 1],  #Default = 0.1\n",
    "    'classifier__objective': ['binary', None], #Default = None. \n",
    "    'classifier__class_weight': [None, 'balanced'], #Default = None\n",
    "    'classifier__colsample_bytree': [0.3, 0.5, 0.7, 0.75, 0.8, 0.85, 0.9, 1] #Default = 1\n",
    "    }\n",
    "\n",
    "# Defining classifier\n",
    "classifier = lgb.LGBMClassifier(random_state=42, n_jobs =-1)\n",
    "stratified_kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "\n",
    "# Defining preprocessing steps\n",
    "scaler = MinMaxScaler()\n",
    "imputer = SimpleImputer()\n",
    "\n",
    "# Defining pipeline\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('Robust', scaler),\n",
    "    ('Simple', imputer),\n",
    "    ('classifier', classifier)\n",
    "])\n",
    "\n",
    "# Performing Bayesian optimization with F1-macro score\n",
    "opt = BayesSearchCV(\n",
    "    pipeline,\n",
    "    params_lgbm,\n",
    "    scoring='f1_macro',\n",
    "    n_iter=30, \n",
    "    cv=stratified_kfold,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "opt.fit(X_selected, y_dis)\n",
    "\n",
    "print(\"Best hyperparameters found:\")\n",
    "print(opt.best_params_)\n",
    "print(f\"Best {classifier.__class__.__name__} cross-validation F1-macro score: {opt.best_score_}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "249446f0-57a3-4aac-aa47-7ba745b9d6c9",
   "metadata": {},
   "source": [
    "# Random search results (f1-macro + std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c968d214-3ed4-455b-a259-8d3b1577e099",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'StratifiedKFold' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Defining variables for validation segments\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m stratified_kfold \u001b[38;5;241m=\u001b[39m \u001b[43mStratifiedKFold\u001b[49m(n_splits\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[1;32m      3\u001b[0m selected_features \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mucla_t6_sum\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msummary_score_kccq_base\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124meq5d5l_index_t6\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMCS_t6\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mImplWeight\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpersonality_type_D\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      4\u001b[0m X_anx \u001b[38;5;241m=\u001b[39m X_anx[selected_features]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'StratifiedKFold' is not defined"
     ]
    }
   ],
   "source": [
    "# Defining variables for validation segments\n",
    "\n",
    "# Anxiety present detection\n",
    "stratified_kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "selected_features = ['ucla_t6_sum', 'summary_score_kccq_base', 'eq5d5l_index_t6', 'MCS_t6', 'ImplWeight', 'personality_type_D']\n",
    "X_anx = X_anx[selected_features]\n",
    "\n",
    "# Distress present detection\n",
    "stratified_kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "selected_features = ['ucla_t6_sum', 'eq5d5l_index_t6', 'depression_base_score', 'icdc_t6_sum', 'MCS_t6', 'Return_to_function_t6', 'age', 'anxiety_base_score']\n",
    "X_dis = X_dis[selected_features]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c712fd9c-d288-48c9-bec3-4acc3dbcd21c",
   "metadata": {},
   "source": [
    "## Anxiety"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a52545-353a-4b8e-9709-c86d800eaff7",
   "metadata": {},
   "source": [
    "### RF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e15d11-459e-46ba-ade3-030b64a5b34c",
   "metadata": {},
   "source": [
    "#### Performing RandomSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "60983c38-a613-4482-b11e-361f9be013aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10000 candidates, totalling 50000 fits\n",
      "0.9209040685715605\n",
      "{'classifier__n_estimators': 100, 'classifier__min_samples_split': 7, 'classifier__min_samples_leaf': 3, 'classifier__max_features': 'sqrt', 'classifier__max_depth': 7, 'classifier__class_weight': 'balanced'}\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "from imblearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.metrics import make_scorer, f1_score\n",
    "from sklearn.model_selection import RandomizedSearchCV, StratifiedKFold\n",
    "import numpy as np\n",
    "\n",
    "# Defining type of split for CV - this is used for imbalanced datasets to keep equal proportion of class balance between folds\n",
    "stratified_kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Defining model and resampling method\n",
    "model = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Defining parameter grid\n",
    "params_rf = {}\n",
    "params_rf['classifier__n_estimators'] = [5, 20, 35, 50, 75, 100, 150, 200, 300, 400, 500, 600, 700, 800, 900, 1000]\n",
    "params_rf['classifier__max_depth'] = [None, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "params_rf['classifier__min_samples_split'] = [2, 3, 5, 6, 7, 8, 9, 10]\n",
    "params_rf['classifier__min_samples_leaf'] = [1, 2, 3, 5, 6, 7, 8, 9, 10]\n",
    "params_rf['classifier__max_features'] = ['log2', 'sqrt']\n",
    "params_rf['classifier__class_weight'] = ['balanced', 'balanced_subsample', None]\n",
    "\n",
    "# Defining preprocessing methods for pipeline\n",
    "scaler = RobustScaler()\n",
    "imputer = SimpleImputer()\n",
    "\n",
    "\n",
    "# Defining pipeline to avoid data leakage\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('Robust', scaler),\n",
    "    ('Simple', imputer),    \n",
    "    ('classifier', model),  \n",
    "    ])\n",
    "\n",
    "\n",
    "############################################################################################\n",
    "\n",
    "# Initializing RandomizedSearchCV\n",
    "opt = RandomizedSearchCV(\n",
    "    pipeline,\n",
    "    param_distributions=params_rf,\n",
    "    n_iter=10000,  # Number of parameter settings that are sampled\n",
    "    cv=stratified_kfold,  # Number of cross-validation folds\n",
    "    random_state=42,\n",
    "    scoring='f1_macro',\n",
    "    verbose=1,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "opt.fit(X_anx_selected, y_anx)\n",
    "\n",
    "#printing(opt.best_score_)\n",
    "print(opt.best_score_)\n",
    "print(opt.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "623ce06c-d1ca-4b1f-8262-9289156ef2fe",
   "metadata": {},
   "source": [
    "#### Validating results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "68c3a02b-c22f-410c-8b18-1c0a8838223d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RandomForestClassifier\n",
      "Mean F1-macro Score: 0.9209040685715605 std: 0.05209723108198606\n"
     ]
    }
   ],
   "source": [
    "# Defining classifier with best params\n",
    "classifier = RandomForestClassifier(n_estimators = 100,\n",
    "min_samples_split = 7,\n",
    "min_samples_leaf = 3,\n",
    "max_features = 'sqrt',\n",
    "max_depth = 7,\n",
    "class_weight = 'balanced',\n",
    "random_state=42)\n",
    "\n",
    "# Defining preprocessing steps\n",
    "scaler = RobustScaler()\n",
    "imputer = SimpleImputer()\n",
    "\n",
    "# Defining pipeline\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('Robust', scaler),\n",
    "    ('Simple', imputer),\n",
    "    ('Classifier', classifier)\n",
    "])\n",
    "\n",
    "# Performing cross-validation\n",
    "scores = cross_val_score(pipeline, X_anx, y_anx, cv=stratified_kfold, scoring='f1_macro', n_jobs=-1)\n",
    "mean_score = np.mean(scores)\n",
    "std_score = np.std(scores)\n",
    "\n",
    "# Printing results\n",
    "print()\n",
    "print(classifier.__class__.__name__)\n",
    "print(\"Mean F1-macro Score:\", mean_score, \"std:\", std_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "457ec92f-7b6c-40a0-baa6-9b114aa0180e",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64069cb2-f072-4a10-8ef7-dbc36ecc1e78",
   "metadata": {},
   "source": [
    "#### Performing RandomSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c2320d61-faed-43be-aec4-afb9d224ddc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 40000 candidates, totalling 200000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:547: FitFailedWarning: \n",
      "19190 fits failed out of a total of 200000.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "19190 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/sklearn/model_selection/_validation.py\", line 895, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/sklearn/base.py\", line 1474, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/imblearn/pipeline.py\", line 326, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **last_step_params[\"fit\"])\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/sklearn/base.py\", line 1474, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/sklearn/svm/_base.py\", line 268, in fit\n",
      "    raise ValueError(\n",
      "ValueError: The dual coefficients or intercepts are not finite. The input data may contain large values and need to be preprocessed.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9126825719632198\n",
      "{'classifier__shrinking': True, 'classifier__probability': False, 'classifier__max_iter': 8000, 'classifier__kernel': 'rbf', 'classifier__gamma': 'auto', 'classifier__degree': 15, 'classifier__coef0': 0.1, 'classifier__class_weight': None, 'classifier__C': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/sklearn/model_selection/_search.py:1051: UserWarning: One or more of the test scores are non-finite: [0.47995118 0.48099838 0.65547234 ... 0.90639697 0.31609963 0.69100935]\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "from imblearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.metrics import make_scorer, f1_score\n",
    "from sklearn.model_selection import RandomizedSearchCV, StratifiedKFold\n",
    "import numpy as np\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Defining type of split for CV - this is used for imbalanced datasets to keep equal proportion of class balance between folds\n",
    "stratified_kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Defining model and resampling method\n",
    "model = SVC(random_state=42)\n",
    "\n",
    "# Defining parameter grid\n",
    "params_svc = {\n",
    "    'classifier__C': [0.001, 0.01, 0.1, 1.0, 10.0, 100.0, 1000.0, 10000.0],\n",
    "    'classifier__kernel': ['linear', 'poly', 'rbf', 'sigmoid'],\n",
    "    'classifier__degree': [2, 3, 4, 6, 10, 15, 20, 40, 100, 150, 200, 400, 700, 1000],\n",
    "    'classifier__gamma': ['scale', 'auto', 0.001, 0.01, 0.1, 1, 2, 5, 10],\n",
    "    'classifier__coef0': [0.0, 0.1, 0.5, 1.0],\n",
    "    'classifier__shrinking': [True, False],\n",
    "    'classifier__probability': [True, False],\n",
    "    'classifier__class_weight': ['balanced', None],\n",
    "    'classifier__max_iter': [500, 1000, 2000, 3000, 4000, 5000, 6000, 7000, 8000, 9000, 10000]\n",
    "}\n",
    "\n",
    "# Defining preprocessing methods for pipeline\n",
    "scaler = RobustScaler()\n",
    "imputer = SimpleImputer()\n",
    "\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('Robust', scaler),\n",
    "    ('Simple', imputer),    \n",
    "    ('classifier', model),  \n",
    "    ])\n",
    "\n",
    "############################################################################################\n",
    "\n",
    "# Initializing RandomizedSearchCV\n",
    "opt = RandomizedSearchCV(\n",
    "    pipeline,\n",
    "    param_distributions=params_svc,\n",
    "    n_iter=40000,  # Number of parameter settings that are sampled\n",
    "    cv=stratified_kfold,  # Number of cross-validation folds\n",
    "    random_state=42,\n",
    "    scoring='f1_macro',\n",
    "    verbose=1,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "opt.fit(X_anx_selected, y_anx)\n",
    "\n",
    "#printing(opt.best_score_)\n",
    "print(opt.best_score_)\n",
    "print(opt.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "079280c5-edc4-45d0-83fa-4bffa53f36a4",
   "metadata": {},
   "source": [
    "#### Validating results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f7723f2b-3222-4b84-932d-b04a7330fbc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SVC\n",
      "Mean F1-macro Score: 0.9126825719632198 std: 0.08203534839628797\n"
     ]
    }
   ],
   "source": [
    "# Defining classifier with best params\n",
    "classifier = SVC(shrinking = True,\n",
    "probability = False,\n",
    "max_iter = 8000,\n",
    "kernel = 'rbf',\n",
    "gamma = 'auto',\n",
    "degree = 15,\n",
    "coef0 = 0.1,\n",
    "class_weight = None,\n",
    "C = 1.0,\n",
    "random_state=42)\n",
    "\n",
    "# Defining preprocessing steps\n",
    "scaler = RobustScaler()\n",
    "imputer = SimpleImputer()\n",
    "\n",
    "# Defining pipeline\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('Robust', scaler),\n",
    "    ('Simple', imputer),\n",
    "    ('Classifier', classifier)\n",
    "])\n",
    "\n",
    "# Performing cross-validation\n",
    "scores = cross_val_score(pipeline, X_anx, y_anx, cv=stratified_kfold, scoring='f1_macro', n_jobs=-1)\n",
    "mean_score = np.mean(scores)\n",
    "std_score = np.std(scores)\n",
    "\n",
    "# Printing results\n",
    "print()\n",
    "print(classifier.__class__.__name__)\n",
    "print(\"Mean F1-macro Score:\", mean_score, \"std:\", std_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "430aed6d-fff2-4695-b825-f4da85aabc11",
   "metadata": {},
   "source": [
    "### KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d68dd1b5-4286-4f25-b220-abf1d806196c",
   "metadata": {},
   "source": [
    "#### Performing RandomSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b58838f3-5f3e-4508-afcc-a1f269a74d00",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/sklearn/model_selection/_search.py:318: UserWarning: The total space of parameters 1600 is smaller than n_iter=10000. Running 1600 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1600 candidates, totalling 8000 fits\n",
      "0.912602819606272\n",
      "{'classifier__weights': 'uniform', 'classifier__p': 1, 'classifier__n_neighbors': 5, 'classifier__leaf_size': 10, 'classifier__algorithm': 'auto'}\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "from imblearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.metrics import make_scorer, f1_score\n",
    "import numpy as np\n",
    "\n",
    "# Defining type of split for CV - this is used for imbalanced datasets to keep equal proportion of class balance between folds\n",
    "stratified_kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Defining model and resampling method\n",
    "model = KNeighborsClassifier()\n",
    "\n",
    "params_knn = {} \n",
    "params_knn['classifier__n_neighbors'] = list(range(1, 41, 2)) #Default = 5\n",
    "params_knn['classifier__weights'] = ['uniform', 'distance'] #Default = uniform\n",
    "params_knn['classifier__algorithm'] = ['auto', 'ball_tree', 'kd_tree', 'brute'] #Default = Auto\n",
    "params_knn['classifier__leaf_size'] = [10, 20, 30, 40, 50] # Default = 30. Only for Balltree and KDTree\n",
    "params_knn['classifier__p'] = [1, 2] #Default = 2\n",
    "\n",
    "# Defining preprocessing methods for pipeline\n",
    "scaler = RobustScaler()\n",
    "imputer = SimpleImputer()\n",
    "\n",
    "# Defining pipeline to avoid data leakage\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('Robust', scaler),\n",
    "    ('Simple', imputer),  \n",
    "    ('classifier', model),  \n",
    "    ])\n",
    "\n",
    "############################################################################################\n",
    "\n",
    "# Initialize RandomizedSearchCV\n",
    "opt = RandomizedSearchCV(\n",
    "    pipeline,\n",
    "    param_distributions=params_knn,\n",
    "    n_iter=10000,  # Number of parameter settings that are sampled\n",
    "    cv=stratified_kfold,  # Number of cross-validation folds\n",
    "    random_state=42,\n",
    "    scoring='f1_macro',\n",
    "    verbose=1,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "opt.fit(X_anx_selected, y_anx)\n",
    "\n",
    "#printing(opt.best_score_)\n",
    "print(opt.best_score_)\n",
    "print(opt.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7230f773-f90f-4b67-bf40-81ee20fc31ce",
   "metadata": {},
   "source": [
    "#### Validating results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1d4cc2c0-d927-4181-bd75-9bad3deb8d52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "KNeighborsClassifier\n",
      "Mean F1-macro Score: 0.912602819606272 std: 0.07830052829708159\n"
     ]
    }
   ],
   "source": [
    "# Defining classifier with best params\n",
    "classifier = KNeighborsClassifier(weights = 'uniform',\n",
    "p = 1,\n",
    "n_neighbors = 5,\n",
    "leaf_size = 10,\n",
    "algorithm = 'auto',\n",
    "n_jobs=-1)\n",
    "\n",
    "# Defining preprocessing steps\n",
    "scaler = RobustScaler()\n",
    "imputer = SimpleImputer()\n",
    "\n",
    "# Defining pipeline\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('Robust', scaler),\n",
    "    ('Simple', imputer),\n",
    "    ('Classifier', classifier)\n",
    "])\n",
    "\n",
    "# Performing cross-validation\n",
    "scores = cross_val_score(pipeline, X_anx, y_anx, cv=stratified_kfold, scoring='f1_macro', n_jobs=-1)\n",
    "mean_score = np.mean(scores)\n",
    "std_score = np.std(scores)\n",
    "\n",
    "# Printing results\n",
    "print()\n",
    "print(classifier.__class__.__name__)\n",
    "print(\"Mean F1-macro Score:\", mean_score, \"std:\", std_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed5b623-4126-46b9-bcfe-8e0f40d60edd",
   "metadata": {},
   "source": [
    "### XGB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d8e713-ec21-40c3-850e-6e79b0131059",
   "metadata": {},
   "source": [
    "#### Performing GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "044a775f-babd-467f-9f1b-27dc0c773d41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10000 candidates, totalling 50000 fits\n",
      "0.9137019004553254\n",
      "{'classifier__subsample': 1, 'classifier__n_jobs': -1, 'classifier__n_estimators': 150, 'classifier__min_child_weight': 1, 'classifier__max_depth': 3, 'classifier__learning_rate': 0.5, 'classifier__colsample_bytree': 1}\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "from imblearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.metrics import make_scorer, f1_score\n",
    "import numpy as np\n",
    "\n",
    "# Defining type of split for CV - this is used for imbalanced datasets to keep equal proportion of class balance between folds\n",
    "stratified_kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Defining model and resampling method\n",
    "model = xgb.XGBClassifier(random_state=42)\n",
    "\n",
    "params_xgb = {}\n",
    "params_xgb['classifier__n_estimators'] =  [5, 20, 35, 50, 75, 100, 150, 200, 300, 400, 500, 600, 700, 800, 900, 1000]  #Default = 100. 75 was best during early testing\n",
    "params_xgb['classifier__max_depth'] = [None, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10] #Default = 6\n",
    "params_xgb['classifier__learning_rate'] = [0.01, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1] #Default = 0.3\n",
    "params_xgb['classifier__min_child_weight'] = [1, 2, 3, 4, 5, 6, 7] #Default = 1\n",
    "params_xgb['classifier__subsample'] = [0.3, 0.5, 0.6, 0.7, 0.8, 0.9, 1] #Default = 1\n",
    "params_xgb['classifier__colsample_bytree'] = [0.3, 0.5, 0.7, 0.75, 0.8, 0.85, 0.9, 1] #Default = 1\n",
    "params_xgb['classifier__n_jobs'] = [-1]\n",
    "\n",
    "# Defining preprocessing methods for pipeline\n",
    "scaler = RobustScaler()\n",
    "imputer = SimpleImputer()\n",
    "\n",
    "# Defininging pipeline to avoid data leakage\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('Robust', scaler),\n",
    "    ('Simple', imputer),    \n",
    "    ('classifier', model),  \n",
    "    ])\n",
    "\n",
    "############################################################################################\n",
    "\n",
    "# Initializing RandomizedSearchCV\n",
    "opt = RandomizedSearchCV(\n",
    "    pipeline,\n",
    "    param_distributions=params_xgb,\n",
    "    n_iter=10000,  # Number of parameter settings that are sampled\n",
    "    cv=stratified_kfold,  # Number of cross-validation folds\n",
    "    random_state=42,\n",
    "    scoring='f1_macro',\n",
    "    verbose=1,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "opt.fit(X_anx_selected, y_anx)\n",
    "\n",
    "#printing(opt.best_score_)\n",
    "print(opt.best_score_)\n",
    "print(opt.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e9c48f6-ed8a-4fb8-ab12-e1ad822d7611",
   "metadata": {},
   "source": [
    "#### Validating results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0b4c0a7b-7e2c-4709-8e76-90cc25c70e54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "XGBClassifier\n",
      "Mean F1-macro Score: 0.9137019004553254 std: 0.08263086794122673\n"
     ]
    }
   ],
   "source": [
    "# Defining classifier with best params\n",
    "classifier = xgb.XGBClassifier(subsample = 1,\n",
    "n_estimators = 150,\n",
    "min_child_weight = 1,\n",
    "max_depth = 3,\n",
    "learning_rate = 0.5,\n",
    "colsample_bytree = 1,\n",
    "random_state=42, n_jobs=-1)\n",
    "\n",
    "# Defining preprocessing steps\n",
    "scaler = RobustScaler()\n",
    "imputer = SimpleImputer()\n",
    "\n",
    "# Defining pipeline\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('Robust', scaler),\n",
    "    ('Simple', imputer),\n",
    "    ('Classifier', classifier)\n",
    "])\n",
    "\n",
    "# Performing cross-validation\n",
    "scores = cross_val_score(pipeline, X_anx, y_anx, cv=stratified_kfold, scoring='f1_macro', n_jobs=-1)\n",
    "mean_score = np.mean(scores)\n",
    "std_score = np.std(scores)\n",
    "\n",
    "# Printing results\n",
    "print()\n",
    "print(classifier.__class__.__name__)\n",
    "print(\"Mean F1-macro Score:\", mean_score, \"std:\", std_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03ece3e7-deef-4b53-90ea-2163a7a398e3",
   "metadata": {},
   "source": [
    "### LGBM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc4b6a50-c1b2-4e61-8a5a-0ff882b58748",
   "metadata": {},
   "source": [
    "#### Performing RandomSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "37a6da7a-391a-48a3-a9d0-d90908bda57e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 5000 candidates, totalling 25000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:547: FitFailedWarning: \n",
      "3615 fits failed out of a total of 25000.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "3615 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/sklearn/model_selection/_validation.py\", line 895, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/sklearn/base.py\", line 1474, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/imblearn/pipeline.py\", line 326, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **last_step_params[\"fit\"])\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/lightgbm/sklearn.py\", line 1187, in fit\n",
      "    super().fit(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/lightgbm/sklearn.py\", line 885, in fit\n",
      "    self._Booster = train(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/lightgbm/engine.py\", line 255, in train\n",
      "    booster = Booster(params=params, train_set=train_set)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/lightgbm/basic.py\", line 3433, in __init__\n",
      "    train_set.construct()\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/lightgbm/basic.py\", line 2462, in construct\n",
      "    self._lazy_init(data=self.data, label=self.label, reference=None,\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/lightgbm/basic.py\", line 2079, in _lazy_init\n",
      "    self.__init_from_np2d(data, params_str, ref_dataset)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/lightgbm/basic.py\", line 2218, in __init_from_np2d\n",
      "    _safe_call(_LIB.LGBM_DatasetCreateFromMat(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/lightgbm/basic.py\", line 263, in _safe_call\n",
      "    raise LightGBMError(_LIB.LGBM_GetLastError().decode('utf-8'))\n",
      "lightgbm.basic.LightGBMError: Check failed: (num_leaves) > (1) at /__w/1/s/lightgbm-python/src/io/config_auto.cpp, line 342 .\n",
      "\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 35, number of negative: 443\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000097 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 309\n",
      "[LightGBM] [Info] Number of data points in the train set: 478, number of used features: 6\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.073222 -> initscore=-2.538222\n",
      "[LightGBM] [Info] Start training from score -2.538222\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "0.910054117835578\n",
      "{'classifier__objective': None, 'classifier__num_leaves': 15, 'classifier__n_estimators': 20, 'classifier__max_depth': 6, 'classifier__learning_rate': 0.1, 'classifier__colsample_bytree': 0.75, 'classifier__class_weight': None, 'classifier__boosting_type': 'gbdt'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/sklearn/model_selection/_search.py:1051: UserWarning: One or more of the test scores are non-finite: [0.7591562  0.48099838 0.87385151 ... 0.86977434 0.87378481 0.85965462]\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "from imblearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.metrics import make_scorer, f1_score\n",
    "import numpy as np\n",
    "\n",
    "# Defining type of split for CV - this is used for imbalanced datasets to keep equal proportion of class balance between folds\n",
    "stratified_kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Defining model and resampling method\n",
    "model = lgb.LGBMClassifier(random_state=42)\n",
    "\n",
    "params_lgbm = {}\n",
    "params_lgbm['classifier__boosting_type'] = ['gbdt', 'dart']  #Default = gbdt\n",
    "params_lgbm['classifier__n_estimators'] = [5, 10, 15, 20, 25, 35, 50, 75, 100, 150, 200, 300, 400] #Default = 100. Good performance around 20\n",
    "params_lgbm['classifier__num_leaves'] = [1, 3, 5, 7, 9, 11, 15] # 20, 25, 31, 40, 50] #Default = 31\n",
    "params_lgbm['classifier__max_depth'] = [-1, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]  #Default = -1\n",
    "params_lgbm['classifier__learning_rate'] = [0.001, 0.05, 0.1, 0.2, 0.3, 0.5, 1]  #Default = 0.1\n",
    "params_lgbm['classifier__objective'] = ['binary', None] #Default = None. \n",
    "params_lgbm['classifier__class_weight'] = [None, 'balanced'] #Default = None\n",
    "params_lgbm['classifier__colsample_bytree'] = [0.3, 0.5, 0.7, 0.75, 0.8, 0.85, 0.9, 1] #Default = 1\n",
    "\n",
    "# Defining preprocessing methods for pipeline\n",
    "scaler = RobustScaler()\n",
    "imputer = SimpleImputer()\n",
    "\n",
    "# Defining pipeline to avoid data leakage\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('Robust', scaler),\n",
    "    ('Simple', imputer),   \n",
    "    ('classifier', model),  \n",
    "    ])\n",
    "############################################################################################\n",
    "\n",
    "# Initializing RandomizedSearchCV\n",
    "opt = RandomizedSearchCV(\n",
    "    pipeline,\n",
    "    param_distributions=params_lgbm,\n",
    "    n_iter=5000,  # Number of parameter settings that are sampled\n",
    "    cv=stratified_kfold,  # Number of cross-validation folds\n",
    "    random_state=42,\n",
    "    scoring='f1_macro',\n",
    "    verbose=1,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "opt.fit(X_anx_selected, y_anx)\n",
    "\n",
    "#printing(opt.best_score_)\n",
    "print(opt.best_score_)\n",
    "print(opt.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c64ce84e-a3c4-4d65-8983-d775e4de8897",
   "metadata": {},
   "source": [
    "#### Validating results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ad664ebd-b5ba-43b2-8398-2532cc2d5b86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "LGBMClassifier\n",
      "Mean F1-macro Score: 0.910054117835578 std: 0.07139540921071352\n"
     ]
    }
   ],
   "source": [
    "# Defining classifier with best params\n",
    "classifier = lgb.LGBMClassifier(objective = None,\n",
    "num_leaves = 15,\n",
    "n_estimators = 20,\n",
    "max_depth = 6,\n",
    "learning_rate = 0.1,\n",
    "colsample_bytree = 0.75,\n",
    "class_weight = None,\n",
    "boosting_type = 'gbdt',\n",
    "random_state=42, n_jobs =-1)\n",
    "\n",
    "# Defining preprocessing steps\n",
    "scaler = RobustScaler()\n",
    "imputer = SimpleImputer()\n",
    "\n",
    "# Defining pipeline\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('Robust', scaler),\n",
    "    ('Simple', imputer),\n",
    "    ('Classifier', classifier)\n",
    "])\n",
    "\n",
    "# Performing cross-validation\n",
    "scores = cross_val_score(pipeline, X_anx, y_anx, cv=stratified_kfold, scoring='f1_macro', n_jobs=-1)\n",
    "mean_score = np.mean(scores)\n",
    "std_score = np.std(scores)\n",
    "\n",
    "# Printing results\n",
    "print()\n",
    "print(classifier.__class__.__name__)\n",
    "print(\"Mean F1-macro Score:\", mean_score, \"std:\", std_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f533bdc9-d6d8-4e6f-b96a-158aae9371df",
   "metadata": {},
   "source": [
    "## Distressed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b9da714-3ad8-4ccf-a5ee-3458f44ed51b",
   "metadata": {},
   "source": [
    "### RF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52f2ce3b-1bfe-4b55-a31e-6776c52fac77",
   "metadata": {},
   "source": [
    "#### Performing RandomSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "95295137-23b0-4493-bdd5-cdcd4fb26bd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10000 candidates, totalling 50000 fits\n",
      "0.9154748466102941\n",
      "{'classifier__n_estimators': 50, 'classifier__min_samples_split': 3, 'classifier__min_samples_leaf': 1, 'classifier__max_features': 'sqrt', 'classifier__max_depth': 7, 'classifier__class_weight': None}\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "from imblearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.metrics import make_scorer, f1_score\n",
    "from sklearn.model_selection import RandomizedSearchCV, StratifiedKFold\n",
    "import numpy as np\n",
    "\n",
    "# Defining type of split for CV - this is used for imbalanced datasets to keep equal proportion of class balance between folds\n",
    "stratified_kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Defining model and resampling method\n",
    "model = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Defining parameter grid\n",
    "params_rf = {}\n",
    "params_rf['classifier__n_estimators'] = [5, 20, 35, 50, 75, 100, 150, 200, 300, 400, 500, 600, 700, 800, 900, 1000]\n",
    "params_rf['classifier__max_depth'] = [None, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "params_rf['classifier__min_samples_split'] = [2, 3, 5, 6, 7, 8, 9, 10]\n",
    "params_rf['classifier__min_samples_leaf'] = [1, 2, 3, 5, 6, 7, 8, 9, 10]\n",
    "params_rf['classifier__max_features'] = ['log2', 'sqrt']\n",
    "params_rf['classifier__class_weight'] = ['balanced', 'balanced_subsample', None]\n",
    "\n",
    "# Defining preprocessing methods for pipeline\n",
    "scaler = MinMaxScaler()\n",
    "imputer = SimpleImputer()\n",
    "\n",
    "\n",
    "# Defining pipeline to avoid data leakage\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('Robust', scaler),\n",
    "    ('Simple', imputer),    \n",
    "    ('classifier', model),  \n",
    "    ])\n",
    "\n",
    "\n",
    "############################################################################################\n",
    "\n",
    "# Initializing RandomizedSearchCV\n",
    "opt = RandomizedSearchCV(\n",
    "    pipeline,\n",
    "    param_distributions=params_rf,\n",
    "    n_iter=10000,  # Number of parameter settings that are sampled\n",
    "    cv=stratified_kfold,  # Number of cross-validation folds\n",
    "    random_state=42,\n",
    "    scoring='f1_macro',\n",
    "    verbose=1,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "opt.fit(X_dis_selected, y_dis)\n",
    "\n",
    "# printing(opt.best_score_)\n",
    "print(opt.best_score_)\n",
    "print(opt.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e8300d-6abd-41c8-a8d0-6f045a7c9f10",
   "metadata": {},
   "source": [
    "#### Validating results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "81b9068a-587b-4ceb-bfd4-de948460a25c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RandomForestClassifier\n",
      "Mean F1-macro Score: 0.9154748466102941 std: 0.08290499798612358\n"
     ]
    }
   ],
   "source": [
    "# Defining classifier with best params\n",
    "classifier = RandomForestClassifier(n_estimators = 50,\n",
    "min_samples_split = 3,\n",
    "min_samples_leaf = 1,\n",
    "max_features = 'sqrt',\n",
    "max_depth = 7,\n",
    "class_weight = None,\n",
    "random_state=42)\n",
    "\n",
    "# Defining preprocessing steps\n",
    "scaler = MinMaxScaler()\n",
    "imputer = SimpleImputer()\n",
    "\n",
    "# Defining pipeline\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('MinMax', scaler),\n",
    "    ('Simple', imputer),\n",
    "    ('Classifier', classifier)\n",
    "])\n",
    "\n",
    "# Performing cross-validation\n",
    "scores = cross_val_score(pipeline, X_dis, y_dis, cv=stratified_kfold, scoring='f1_macro', n_jobs=-1)\n",
    "mean_score = np.mean(scores)\n",
    "std_score = np.std(scores)\n",
    "\n",
    "# Printing results\n",
    "print()\n",
    "print(classifier.__class__.__name__)\n",
    "print(\"Mean F1-macro Score:\", mean_score, \"std:\", std_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1607d825-9bcb-4928-971d-92e4b1091235",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c159ae50-5021-4a95-8b1e-3d05ec214efb",
   "metadata": {},
   "source": [
    "#### Performing RandomSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3b31b4f0-73e3-4c08-8da4-bc1f1402bc28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 40000 candidates, totalling 200000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:547: FitFailedWarning: \n",
      "13414 fits failed out of a total of 200000.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "13414 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/sklearn/model_selection/_validation.py\", line 895, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/sklearn/base.py\", line 1474, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/imblearn/pipeline.py\", line 326, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **last_step_params[\"fit\"])\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/sklearn/base.py\", line 1474, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/sklearn/svm/_base.py\", line 268, in fit\n",
      "    raise ValueError(\n",
      "ValueError: The dual coefficients or intercepts are not finite. The input data may contain large values and need to be preprocessed.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8889376147546265\n",
      "{'classifier__shrinking': False, 'classifier__probability': False, 'classifier__max_iter': 6000, 'classifier__kernel': 'sigmoid', 'classifier__gamma': 'auto', 'classifier__degree': 6, 'classifier__coef0': 0.5, 'classifier__class_weight': None, 'classifier__C': 100.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/sklearn/model_selection/_search.py:1051: UserWarning: One or more of the test scores are non-finite: [0.50148069 0.47873578 0.76638245 ... 0.86357138 0.39866537 0.76118185]\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "from imblearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.metrics import make_scorer, f1_score\n",
    "from sklearn.model_selection import RandomizedSearchCV, StratifiedKFold\n",
    "import numpy as np\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Defining type of split for CV - this is used for imbalanced datasets to keep equal proportion of class balance between folds\n",
    "stratified_kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Defining model and resampling method\n",
    "model = SVC(random_state=42)\n",
    "\n",
    "# Defining parameter grid\n",
    "params_svc = {}\n",
    "params_svc['classifier__C'] = [0.001, 0.01, 0.1, 1.0, 10.0, 100.0, 1000.0, 10000.0],\n",
    "params_svc['classifier__kernel'] = ['linear', 'poly', 'rbf', 'sigmoid'],\n",
    "params_svc['classifier__degree'] = [2, 3, 4, 6, 10, 15, 20, 40, 100, 150, 200, 400, 700, 1000],\n",
    "params_svc['classifier__gamma'] = ['scale', 'auto', 0.001, 0.01, 0.1, 1, 2, 5, 10],\n",
    "params_svc['classifier__coef0'] = [0.0, 0.1, 0.5, 1.0],\n",
    "params_svc['classifier__shrinking'] = [True, False],\n",
    "params_svc['classifier__probability'] = [True, False],\n",
    "params_svc['classifier__class_weight'] = ['balanced', None],\n",
    "params_svc['classifier__max_iter'] = [500, 1000, 2000, 3000, 4000, 5000, 6000, 7000, 8000, 9000, 10000]\n",
    "\n",
    "# Defining preprocessing methods for pipeline\n",
    "scaler = MinMaxScaler()\n",
    "imputer = SimpleImputer()\n",
    "\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('Robust', scaler),\n",
    "    ('Simple', imputer),    \n",
    "    ('classifier', model),  \n",
    "    ])\n",
    "\n",
    "############################################################################################\n",
    "\n",
    "# Initializing RandomizedSearchCV\n",
    "opt = RandomizedSearchCV(\n",
    "    pipeline,\n",
    "    param_distributions=params_svc,\n",
    "    n_iter=40000,  # Number of parameter settings that are sampled\n",
    "    cv=stratified_kfold,  # Number of cross-validation folds\n",
    "    random_state=42,\n",
    "    scoring='f1_macro',\n",
    "    verbose=1,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "opt.fit(X_dis_selected, y_dis)\n",
    "\n",
    "#printing(opt.best_score_)\n",
    "print(opt.best_score_)\n",
    "print(opt.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf86cf40-6a2f-4c47-bdb1-16c6c81993d3",
   "metadata": {},
   "source": [
    "#### Validating results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "12f15227-ad1a-4bb4-b71c-077a583c1497",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SVC\n",
      "Mean F1-macro Score: 0.8889376147546265 std: 0.11453618279448234\n"
     ]
    }
   ],
   "source": [
    "# Defining classifier with best params\n",
    "classifier = SVC(shrinking = False,\n",
    "probability = False,\n",
    "max_iter = 6000,\n",
    "kernel = 'sigmoid',\n",
    "gamma = 'auto',\n",
    "degree = 6,\n",
    "coef0 = 0.5,\n",
    "class_weight = None,\n",
    "C = 100.0,\n",
    "random_state=42)\n",
    "\n",
    "# Defining preprocessing steps\n",
    "scaler = MinMaxScaler()\n",
    "imputer = SimpleImputer()\n",
    "\n",
    "# Defining pipeline\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('MinMax', scaler),\n",
    "    ('Simple', imputer),\n",
    "    ('Classifier', classifier)\n",
    "])\n",
    "\n",
    "# Performing cross-validation\n",
    "scores = cross_val_score(pipeline, X_dis, y_dis, cv=stratified_kfold, scoring='f1_macro', n_jobs=-1)\n",
    "mean_score = np.mean(scores)\n",
    "std_score = np.std(scores)\n",
    "\n",
    "# Printing results\n",
    "print()\n",
    "print(classifier.__class__.__name__)\n",
    "print(\"Mean F1-macro Score:\", mean_score, \"std:\", std_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e745e86c-7873-4bfa-83e8-607c4acaf10e",
   "metadata": {},
   "source": [
    "### KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f06e45d3-7da0-433c-9364-69017896b910",
   "metadata": {},
   "source": [
    "#### Performing RandomSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bb3b2b1c-a74a-4a0f-a3f1-86b29279c014",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/sklearn/model_selection/_search.py:318: UserWarning: The total space of parameters 1600 is smaller than n_iter=10000. Running 1600 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1600 candidates, totalling 8000 fits\n",
      "0.8719658963464401\n",
      "{'classifier__weights': 'uniform', 'classifier__p': 2, 'classifier__n_neighbors': 5, 'classifier__leaf_size': 10, 'classifier__algorithm': 'auto'}\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "from imblearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.metrics import make_scorer, f1_score\n",
    "import numpy as np\n",
    "\n",
    "# Defining type of split for CV - this is used for imbalanced datasets to keep equal proportion of class balance between folds\n",
    "stratified_kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Defining model and resampling method\n",
    "model = KNeighborsClassifier()\n",
    "\n",
    "params_knn = {} \n",
    "params_knn['classifier__n_neighbors'] = list(range(1, 41, 2)) #Default = 5\n",
    "params_knn['classifier__weights'] = ['uniform', 'distance'] #Default = uniform\n",
    "params_knn['classifier__algorithm'] = ['auto', 'ball_tree', 'kd_tree', 'brute'] #Default = Auto\n",
    "params_knn['classifier__leaf_size'] = [10, 20, 30, 40, 50] # Default = 30. Only for Balltree and KDTree\n",
    "params_knn['classifier__p'] = [1, 2] #Default = 2\n",
    "\n",
    "# Defining preprocessing methods for pipeline\n",
    "scaler = MinMaxScaler()\n",
    "imputer = SimpleImputer()\n",
    "\n",
    "# Defining pipeline to avoid data leakage\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('Robust', scaler),\n",
    "    ('Simple', imputer),    \n",
    "    ('classifier', model),  \n",
    "    ])\n",
    "\n",
    "############################################################################################\n",
    "\n",
    "# Initializing RandomizedSearchCV\n",
    "opt = RandomizedSearchCV(\n",
    "    pipeline,\n",
    "    param_distributions=params_knn,\n",
    "    n_iter=10000,  # Number of parameter settings that are sampled\n",
    "    cv=stratified_kfold,  # Number of cross-validation folds\n",
    "    random_state=42,\n",
    "    scoring='f1_macro',\n",
    "    verbose=1,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "opt.fit(X_dis_selected, y_dis)\n",
    "\n",
    "#printing(opt.best_score_)\n",
    "print(opt.best_score_)\n",
    "print(opt.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98fc304e-ef76-48fd-af3b-b2bc3e33116e",
   "metadata": {},
   "source": [
    "#### Validating results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "dca22860-d9d9-46e3-84b7-f0beb3f5ca85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "KNeighborsClassifier\n",
      "Mean F1-macro Score: 0.8719658963464401 std: 0.0778962832148059\n"
     ]
    }
   ],
   "source": [
    "# Defining classifier with best params\n",
    "classifier = KNeighborsClassifier(weights = 'uniform',\n",
    "p = 2,\n",
    "n_neighbors = 5,\n",
    "leaf_size = 10,\n",
    "algorithm = 'auto',\n",
    "n_jobs=-1)\n",
    "\n",
    "# Defining preprocessing steps\n",
    "scaler = MinMaxScaler()\n",
    "imputer = SimpleImputer()\n",
    "\n",
    "# Defining pipeline\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('MinMax', scaler),\n",
    "    ('Simple', imputer),\n",
    "    ('Classifier', classifier)\n",
    "])\n",
    "\n",
    "# Performing cross-validation\n",
    "scores = cross_val_score(pipeline, X_dis, y_dis, cv=stratified_kfold, scoring='f1_macro', n_jobs=-1)\n",
    "mean_score = np.mean(scores)\n",
    "std_score = np.std(scores)\n",
    "\n",
    "# Printing results\n",
    "print()\n",
    "print(classifier.__class__.__name__)\n",
    "print(\"Mean F1-macro Score:\", mean_score, \"std:\", std_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ff8aeb0-8ff0-490e-a8ff-d1b643b4c359",
   "metadata": {},
   "source": [
    "### XGB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e673092a-77ec-4792-bc09-7d822d0ceff2",
   "metadata": {},
   "source": [
    "#### Performing RandomSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "53d05d70-5332-4344-b1aa-f2f7b22e332a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10000 candidates, totalling 50000 fits\n",
      "0.8952635753908563\n",
      "{'classifier__subsample': 1, 'classifier__n_jobs': -1, 'classifier__n_estimators': 75, 'classifier__min_child_weight': 1, 'classifier__max_depth': 9, 'classifier__learning_rate': 0.3, 'classifier__colsample_bytree': 1}\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "from imblearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.metrics import make_scorer, f1_score\n",
    "import numpy as np\n",
    "\n",
    "# Defining type of split for CV - this is used for imbalanced datasets to keep equal proportion of class balance between folds\n",
    "stratified_kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Defining model and resampling method\n",
    "model = xgb.XGBClassifier(random_state=42)\n",
    "\n",
    "params_xgb = {}\n",
    "params_xgb['classifier__n_estimators'] =  [5, 20, 35, 50, 75, 100, 150, 200, 300, 400, 500, 600, 700, 800, 900, 1000]  #Default = 100. 75 was best during early testing\n",
    "params_xgb['classifier__max_depth'] = [None, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10] #Default = 6\n",
    "params_xgb['classifier__learning_rate'] = [0.01, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1] #Default = 0.3\n",
    "params_xgb['classifier__min_child_weight'] = [1, 2, 3, 4, 5, 6, 7] #Default = 1\n",
    "params_xgb['classifier__subsample'] = [0.3, 0.5, 0.6, 0.7, 0.8, 0.9, 1] #Default = 1\n",
    "params_xgb['classifier__colsample_bytree'] = [0.3, 0.5, 0.7, 0.75, 0.8, 0.85, 0.9, 1] #Default = 1\n",
    "params_xgb['classifier__n_jobs'] = [-1]\n",
    "\n",
    "# Defining preprocessing methods for pipeline\n",
    "scaler = MinMaxScaler()\n",
    "imputer = SimpleImputer()\n",
    "\n",
    "# Defining pipeline to avoid data leakage\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('Robust', scaler),\n",
    "    ('Simple', imputer),    \n",
    "    ('classifier', model),  \n",
    "    ])\n",
    "\n",
    "############################################################################################\n",
    "\n",
    "# Initializing RandomizedSearchCV\n",
    "opt = RandomizedSearchCV(\n",
    "    pipeline,\n",
    "    param_distributions=params_xgb,\n",
    "    n_iter=10000,  # Number of parameter settings that are sampled\n",
    "    cv=stratified_kfold,  # Number of cross-validation folds\n",
    "    random_state=42,\n",
    "    scoring='f1_macro',\n",
    "    verbose=1,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "opt.fit(X_dis_selected, y_dis)\n",
    "\n",
    "#printing(opt.best_score_)\n",
    "print(opt.best_score_)\n",
    "print(opt.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35ede8cf-fd9a-4716-a7c8-01621d2f68c2",
   "metadata": {},
   "source": [
    "#### Validating results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "362e891c-0d0a-4714-ac57-b98c7d98ad71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "XGBClassifier\n",
      "Mean F1-macro Score: 0.8952635753908563 std: 0.06856591410585108\n"
     ]
    }
   ],
   "source": [
    "# Defining classifier with best params\n",
    "classifier = xgb.XGBClassifier(subsample = 1,\n",
    "n_estimators = 75,\n",
    "min_child_weight = 1,\n",
    "max_depth = 9,\n",
    "learning_rate = 0.3,\n",
    "colsample_bytree = 1,\n",
    "random_state=42, n_jobs=-1)\n",
    "\n",
    "# Defining preprocessing steps\n",
    "scaler = MinMaxScaler()\n",
    "imputer = SimpleImputer()\n",
    "\n",
    "# Defining pipeline\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('MinMax', scaler),\n",
    "    ('Simple', imputer),\n",
    "    ('Classifier', classifier)\n",
    "])\n",
    "\n",
    "# Performing cross-validation\n",
    "scores = cross_val_score(pipeline, X_dis, y_dis, cv=stratified_kfold, scoring='f1_macro', n_jobs=-1)\n",
    "mean_score = np.mean(scores)\n",
    "std_score = np.std(scores)\n",
    "\n",
    "# Printing results\n",
    "print()\n",
    "print(classifier.__class__.__name__)\n",
    "print(\"Mean F1-macro Score:\", mean_score, \"std:\", std_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a0c100-5115-49f8-9088-1d7177ff3bcc",
   "metadata": {},
   "source": [
    "### LGBM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99d1210d-8bfa-4649-978f-9d20afe3123c",
   "metadata": {},
   "source": [
    "#### Performing RandomSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "785cf183-0265-4e05-bdc5-ae1532c82c67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 5000 candidates, totalling 25000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:547: FitFailedWarning: \n",
      "3615 fits failed out of a total of 25000.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "3615 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/sklearn/model_selection/_validation.py\", line 895, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/sklearn/base.py\", line 1474, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/imblearn/pipeline.py\", line 326, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **last_step_params[\"fit\"])\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/lightgbm/sklearn.py\", line 1187, in fit\n",
      "    super().fit(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/lightgbm/sklearn.py\", line 885, in fit\n",
      "    self._Booster = train(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/lightgbm/engine.py\", line 255, in train\n",
      "    booster = Booster(params=params, train_set=train_set)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/lightgbm/basic.py\", line 3433, in __init__\n",
      "    train_set.construct()\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/lightgbm/basic.py\", line 2462, in construct\n",
      "    self._lazy_init(data=self.data, label=self.label, reference=None,\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/lightgbm/basic.py\", line 2079, in _lazy_init\n",
      "    self.__init_from_np2d(data, params_str, ref_dataset)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/lightgbm/basic.py\", line 2218, in __init_from_np2d\n",
      "    _safe_call(_LIB.LGBM_DatasetCreateFromMat(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/lightgbm/basic.py\", line 263, in _safe_call\n",
      "    raise LightGBMError(_LIB.LGBM_GetLastError().decode('utf-8'))\n",
      "lightgbm.basic.LightGBMError: Check failed: (num_leaves) > (1) at /__w/1/s/lightgbm-python/src/io/config_auto.cpp, line 342 .\n",
      "\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 39, number of negative: 439\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000113 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 336\n",
      "[LightGBM] [Info] Number of data points in the train set: 478, number of used features: 8\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.081590 -> initscore=-2.420938\n",
      "[LightGBM] [Info] Start training from score -2.420938\n",
      "0.9189097609684165\n",
      "{'classifier__objective': 'binary', 'classifier__num_leaves': 3, 'classifier__n_estimators': 75, 'classifier__max_depth': 3, 'classifier__learning_rate': 0.3, 'classifier__colsample_bytree': 0.7, 'classifier__class_weight': None, 'classifier__boosting_type': 'dart'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/sklearn/model_selection/_search.py:1051: UserWarning: One or more of the test scores are non-finite: [0.7331352  0.47873578 0.8768325  ... 0.88179247 0.85250686 0.87104133]\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "from imblearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.metrics import make_scorer, f1_score\n",
    "import numpy as np\n",
    "\n",
    "# Defining type of split for CV - this is used for imbalanced datasets to keep equal proportion of class balance between folds\n",
    "stratified_kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Defining model and resampling method\n",
    "model = lgb.LGBMClassifier(random_state=42)\n",
    "\n",
    "params_lgbm = {}\n",
    "params_lgbm['classifier__boosting_type'] = ['gbdt', 'dart']  #Default = gbdt\n",
    "params_lgbm['classifier__n_estimators'] = [5, 10, 15, 20, 25, 35, 50, 75, 100, 150, 200, 300, 400] #Default = 100. Good performance around 20\n",
    "params_lgbm['classifier__num_leaves'] = [1, 3, 5, 7, 9, 11, 15] # 20, 25, 31, 40, 50] #Default = 31\n",
    "params_lgbm['classifier__max_depth'] = [-1, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]  #Default = -1\n",
    "params_lgbm['classifier__learning_rate'] = [0.001, 0.05, 0.1, 0.2, 0.3, 0.5, 1]  #Default = 0.1\n",
    "params_lgbm['classifier__objective'] = ['binary', None] #Default = None. \n",
    "params_lgbm['classifier__class_weight'] = [None, 'balanced'] #Default = None\n",
    "params_lgbm['classifier__colsample_bytree'] = [0.3, 0.5, 0.7, 0.75, 0.8, 0.85, 0.9, 1] #Default = 1\n",
    "\n",
    "# Defining preprocessing methods for pipeline\n",
    "scaler = MinMaxScaler()\n",
    "imputer = SimpleImputer()\n",
    "\n",
    "# Defining pipeline to avoid data leakage\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('Robust', scaler),\n",
    "    ('Simple', imputer),    \n",
    "    ('classifier', model),  \n",
    "    ])\n",
    "############################################################################################\n",
    "\n",
    "# Initializing RandomizedSearchCV\n",
    "opt = RandomizedSearchCV(\n",
    "    pipeline,\n",
    "    param_distributions=params_lgbm,\n",
    "    n_iter=5000,  # Number of parameter settings that are sampled\n",
    "    cv=stratified_kfold,  # Number of cross-validation folds\n",
    "    random_state=42,\n",
    "    scoring='f1_macro',\n",
    "    verbose=1,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "opt.fit(X_dis_selected, y_dis)\n",
    "\n",
    "#printing(opt.best_score_)\n",
    "print(opt.best_score_)\n",
    "print(opt.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39e3559a-fb47-4458-9533-639ce5947a6a",
   "metadata": {},
   "source": [
    "#### Validating Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fa90cc30-7e4e-4c77-a18e-74121d34cb07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "LGBMClassifier\n",
      "Mean F1-macro Score: 0.9189097609684165 std: 0.04825543987086979\n"
     ]
    }
   ],
   "source": [
    "# Defining classifier with best params\n",
    "classifier = lgb.LGBMClassifier(objective = 'binary',\n",
    "num_leaves = 3,\n",
    "n_estimators = 75,\n",
    "max_depth = 3,\n",
    "learning_rate = 0.3,\n",
    "colsample_bytree = 0.7,\n",
    "class_weight = None,\n",
    "boosting_type = 'dart',\n",
    "random_state=42, n_jobs =-1)\n",
    "\n",
    "# Defining preprocessing steps\n",
    "scaler = MinMaxScaler()\n",
    "imputer = SimpleImputer()\n",
    "\n",
    "# Defining pipeline\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('MinMax', scaler),\n",
    "    ('Simple', imputer),\n",
    "    ('Classifier', classifier)\n",
    "])\n",
    "\n",
    "# Performing cross-validation\n",
    "scores = cross_val_score(pipeline, X_dis, y_dis, cv=stratified_kfold, scoring='f1_macro', n_jobs=-1)\n",
    "mean_score = np.mean(scores)\n",
    "std_score = np.std(scores)\n",
    "\n",
    "# Printing results\n",
    "print()\n",
    "print(classifier.__class__.__name__)\n",
    "print(\"Mean F1-macro Score:\", mean_score, \"std:\", std_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e9a618e-5521-4d90-a6de-5cb28f64bb6c",
   "metadata": {},
   "source": [
    "# Gridsearch results (f1-macro + std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c50e06a9-08f7-4202-b596-0f8a7b980210",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining variables for validation segments\n",
    "\n",
    "# Anxiety present detection\n",
    "stratified_kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "selected_features = ['ucla_t6_sum', 'summary_score_kccq_base', 'eq5d5l_index_t6', 'MCS_t6', 'ImplWeight', 'personality_type_D']\n",
    "X_anx = X_anx[selected_features]\n",
    "\n",
    "# Distress present detection\n",
    "stratified_kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "selected_features = ['ucla_t6_sum', 'eq5d5l_index_t6', 'depression_base_score', 'icdc_t6_sum', 'MCS_t6', 'Return_to_function_t6', 'age', 'anxiety_base_score']\n",
    "X_dis = X_dis[selected_features]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a620981f-621a-49dc-a9ce-2ae004a9cb9e",
   "metadata": {},
   "source": [
    "## Anxiety"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "581e675a-fdab-4313-91d1-bfeac33b76da",
   "metadata": {},
   "source": [
    "### RF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c74ec287-fe6f-47da-b85b-f96ef3db5213",
   "metadata": {},
   "source": [
    "#### Performing GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2edca44c-17f1-4800-a49a-3b8d1cf70227",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1944 candidates, totalling 9720 fits\n",
      "0.9209040685715605\n",
      "{'classifier__class_weight': 'balanced', 'classifier__max_depth': None, 'classifier__max_features': 'log2', 'classifier__min_samples_leaf': 3, 'classifier__min_samples_split': 7, 'classifier__n_estimators': 90}\n"
     ]
    }
   ],
   "source": [
    "# Defining type of split for CV - this is used for imbalanced datasets to keep equal proportion of class balance between folds\n",
    "stratified_kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Defining model and resampling method\n",
    "model = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Defining parameter grid\n",
    "# Number in comments = value chosen in randomsearch\n",
    "params_rf = {}\n",
    "params_rf['classifier__n_estimators'] = [60, 70, 80, 90, 100, 110, 120, 130, 140] #100\n",
    "params_rf['classifier__max_depth'] = [None, 6, 7, 8] #7\n",
    "params_rf['classifier__min_samples_split'] = [6, 7, 8] #7\n",
    "params_rf['classifier__min_samples_leaf'] = [2, 3, 4] #3\n",
    "params_rf['classifier__max_features'] = ['log2', 'sqrt'] #sqrt\n",
    "params_rf['classifier__class_weight'] = ['balanced', 'balanced_subsample', None] #balanced\n",
    "\n",
    "# Defining preprocessing methods for pipeline\n",
    "scaler = RobustScaler()\n",
    "imputer = SimpleImputer()\n",
    "\n",
    "# Defining pipeline to avoid data leakage\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('Robust', scaler),\n",
    "    ('Simple', imputer),    \n",
    "    ('classifier', model),  \n",
    "    ])\n",
    "\n",
    "############################################################################################\n",
    "\n",
    "# Initializing RandomizedSearchCV\n",
    "opt = GridSearchCV(\n",
    "    pipeline,\n",
    "    param_grid=params_rf,  # Number of parameter settings that are sampled\n",
    "    cv=stratified_kfold,  # Number of cross-validation folds\n",
    "    scoring='f1_macro',\n",
    "    verbose=1,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "opt.fit(X_anx_selected, y_anx)\n",
    "\n",
    "#printing(opt.best_score_)\n",
    "print(opt.best_score_)\n",
    "print(opt.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e34fb6e7-8705-4022-be22-2bd42cba77f8",
   "metadata": {},
   "source": [
    "#### Validating results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0f7758dc-218e-4bec-9e1f-4e0a819aef51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RandomForestClassifier\n",
      "Mean F1-macro Score: 0.9209040685715605 std: 0.05209723108198606\n"
     ]
    }
   ],
   "source": [
    "# Defining classifier with best params\n",
    "classifier = RandomForestClassifier(class_weight = 'balanced',\n",
    "max_depth = None,\n",
    "max_features = 'log2',\n",
    "min_samples_leaf = 3,\n",
    "min_samples_split = 7,\n",
    "n_estimators = 90,\n",
    "random_state=42)\n",
    "\n",
    "# Defining preprocessing steps\n",
    "scaler = RobustScaler()\n",
    "imputer = SimpleImputer()\n",
    "\n",
    "# Defining pipeline\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('Robust', scaler),\n",
    "    ('Simple', imputer),\n",
    "    ('Classifier', classifier)\n",
    "])\n",
    "\n",
    "# Performing cross-validation\n",
    "scores = cross_val_score(pipeline, X_anx, y_anx, cv=stratified_kfold, scoring='f1_macro', n_jobs=-1)\n",
    "mean_score = np.mean(scores)\n",
    "std_score = np.std(scores)\n",
    "\n",
    "# Printing results\n",
    "print()\n",
    "print(classifier.__class__.__name__)\n",
    "print(\"Mean F1-macro Score:\", mean_score, \"std:\", std_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e040a10-8166-4f79-8b56-1e735245b015",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7263de1-6234-443e-baff-342cbcd0619c",
   "metadata": {},
   "source": [
    "#### Performing GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "b6560271-e904-4e23-acb2-d2a0f32aabd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 12960 candidates, totalling 64800 fits\n",
      "0.8889376147546265\n",
      "{'classifier__C': 50, 'classifier__class_weight': None, 'classifier__coef0': 0.3, 'classifier__degree': 5, 'classifier__gamma': 'auto', 'classifier__kernel': 'sigmoid', 'classifier__max_iter': 5000, 'classifier__probability': True, 'classifier__shrinking': True}\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "from imblearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.metrics import make_scorer, f1_score\n",
    "from sklearn.model_selection import RandomizedSearchCV, StratifiedKFold\n",
    "import numpy as np\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Defining type of split for CV - this is used for imbalanced datasets to keep equal proportion of class balance between folds\n",
    "stratified_kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Defining model and resampling method\n",
    "model = SVC(random_state=42)\n",
    "\n",
    "# Defining parameter grid\n",
    "params_svc = {\n",
    "    'classifier__C': [0.25, 0.5, 0.75, 0.875, 1.0, 1.125, 1.25, 1.5, 1.75],                      #1.0\n",
    "    'classifier__kernel': ['rbf'],                                  #rbf\n",
    "    'classifier__degree': [12, 13, 14, 15, 16, 17, 18],             #15\n",
    "    'classifier__gamma': ['scale', 'auto'],                         #auto\n",
    "    'classifier__coef0': [0.05, 0.1, 0.2, 0.3, 0.4],                                                   #0.1\n",
    "    'classifier__shrinking': [True],                                                      #True\n",
    "    'classifier__probability': [False],                                                    #False\n",
    "    'classifier__class_weight': ['balanced', None],                                              #None\n",
    "    'classifier__max_iter': [7000, 8000, 9000]   #8000\n",
    "}\n",
    "\n",
    "# Defining preprocessing methods for pipeline\n",
    "scaler = RobustScaler()\n",
    "imputer = SimpleImputer()\n",
    "\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('Robust', scaler),\n",
    "    ('Simple', imputer),    \n",
    "    ('classifier', model),  \n",
    "    ])\n",
    "\n",
    "############################################################################################\n",
    "\n",
    "# Initializing RandomizedSearchCV\n",
    "opt = GridSearchCV(\n",
    "    pipeline,\n",
    "    param_grid=params_svc,  # Number of parameter settings that are sampled\n",
    "    cv=stratified_kfold,  # Number of cross-validation folds\n",
    "    scoring='f1_macro',\n",
    "    verbose=1,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "opt.fit(X_anx_selected, y_anx)\n",
    "\n",
    "#printing(opt.best_score_)\n",
    "print(opt.best_score_)\n",
    "print(opt.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5732878e-b0f8-4f2b-9d8d-df4d9d785a37",
   "metadata": {},
   "source": [
    "#### Validating results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "37da7138-b603-457d-8207-2f31588f9ff1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SVC\n",
      "Mean F1-macro Score: 0.9126825719632198 std: 0.08203534839628797\n"
     ]
    }
   ],
   "source": [
    "# Defining classifier with best params\n",
    "classifier = SVC(C = 1.0,\n",
    "class_weight = None,\n",
    "coef0 = 0.05,\n",
    "degree = 12,\n",
    "gamma = 'auto',\n",
    "kernel = 'rbf',\n",
    "max_iter = 7000,\n",
    "probability = False,\n",
    "shrinking = True,\n",
    "random_state=42)\n",
    "\n",
    "# Defining preprocessing steps\n",
    "scaler = RobustScaler()\n",
    "imputer = SimpleImputer()\n",
    "\n",
    "# Defining pipeline\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('Robust', scaler),\n",
    "    ('Simple', imputer),\n",
    "    ('Classifier', classifier)\n",
    "])\n",
    "\n",
    "# Performing cross-validation\n",
    "scores = cross_val_score(pipeline, X_anx, y_anx, cv=stratified_kfold, scoring='f1_macro', n_jobs=-1)\n",
    "mean_score = np.mean(scores)\n",
    "std_score = np.std(scores)\n",
    "\n",
    "# Printing results\n",
    "print()\n",
    "print(classifier.__class__.__name__)\n",
    "print(\"Mean F1-macro Score:\", mean_score, \"std:\", std_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef02102f-c870-4c1b-9f55-8a758e65a63e",
   "metadata": {},
   "source": [
    "### KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b7385d-e68d-4f46-b60d-f5eb7b78c298",
   "metadata": {},
   "source": [
    "#### Performing GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "99229494-c045-4ef3-8f1f-65cc1a12ff51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 3200 candidates, totalling 16000 fits\n",
      "0.912602819606272\n",
      "{'classifier__algorithm': 'auto', 'classifier__leaf_size': 10, 'classifier__n_neighbors': 5, 'classifier__p': 1, 'classifier__weights': 'uniform'}\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "from imblearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.metrics import make_scorer, f1_score\n",
    "import numpy as np\n",
    "\n",
    "# Defining type of split for CV - this is used for imbalanced datasets to keep equal proportion of class balance between folds\n",
    "stratified_kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Defining model and resampling method\n",
    "model = KNeighborsClassifier()\n",
    "\n",
    "params_knn = {} \n",
    "params_knn['classifier__n_neighbors'] = list(range(1, 41, 1)) #Default = 5\n",
    "params_knn['classifier__weights'] = ['uniform', 'distance'] #Default = uniform\n",
    "params_knn['classifier__algorithm'] = ['auto', 'ball_tree', 'kd_tree', 'brute'] #Default = Auto\n",
    "params_knn['classifier__leaf_size'] = [10, 20, 30, 40, 50] # Default = 30. Only for Balltree and KDTree\n",
    "params_knn['classifier__p'] = [1, 2] #Default = 2\n",
    "\n",
    "# Defining preprocessing methods for pipeline\n",
    "scaler = RobustScaler()\n",
    "imputer = SimpleImputer()\n",
    "\n",
    "\n",
    "# Defining pipeline to avoid data leakage\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('Robust', scaler),\n",
    "    ('Simple', imputer),  \n",
    "    ('classifier', model),  \n",
    "    ])\n",
    "\n",
    "############################################################################################\n",
    "\n",
    "# Initializing RandomizedSearchCV\n",
    "opt = GridSearchCV(\n",
    "    pipeline,\n",
    "    param_grid=params_knn,  # Number of parameter settings that are sampled\n",
    "    cv=stratified_kfold,  # Number of cross-validation folds\n",
    "    scoring='f1_macro',\n",
    "    verbose=1,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "opt.fit(X_anx_selected, y_anx)\n",
    "\n",
    "#printing(opt.best_score_)\n",
    "print(opt.best_score_)\n",
    "print(opt.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bbb01d2-6112-4827-8dd1-b2cf0599c365",
   "metadata": {},
   "source": [
    "#### Validating results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bade9fb9-447c-402c-82ab-5d24c324b0e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "KNeighborsClassifier\n",
      "Mean F1-macro Score: 0.912602819606272 std: 0.07830052829708159\n"
     ]
    }
   ],
   "source": [
    "# Defining classifier with best params\n",
    "classifier = KNeighborsClassifier(algorithm = 'auto',\n",
    "leaf_size = 10,\n",
    "n_neighbors = 5,\n",
    "p = 1,\n",
    "weights = 'uniform',\n",
    "n_jobs=-1)\n",
    "\n",
    "# Defining preprocessing steps\n",
    "scaler = RobustScaler()\n",
    "imputer = SimpleImputer()\n",
    "\n",
    "# Defining pipeline\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('Robust', scaler),\n",
    "    ('Simple', imputer),\n",
    "    ('Classifier', classifier)\n",
    "])\n",
    "\n",
    "# Performing cross-validation\n",
    "scores = cross_val_score(pipeline, X_anx, y_anx, cv=stratified_kfold, scoring='f1_macro', n_jobs=-1)\n",
    "mean_score = np.mean(scores)\n",
    "std_score = np.std(scores)\n",
    "\n",
    "# Printing results\n",
    "print()\n",
    "print(classifier.__class__.__name__)\n",
    "print(\"Mean F1-macro Score:\", mean_score, \"std:\", std_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b38ee630-24da-47eb-b683-6f0863af047a",
   "metadata": {},
   "source": [
    "### XGB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "842eccab-9b75-4044-9cee-e84c0157d3f3",
   "metadata": {},
   "source": [
    "#### Performing GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "597fc1d7-4ea5-4174-b32c-c7f58ad6e641",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1620 candidates, totalling 8100 fits\n",
      "0.9209335388734046\n",
      "{'classifier__colsample_bytree': 0.9, 'classifier__learning_rate': 0.4, 'classifier__max_depth': 3, 'classifier__min_child_weight': 1, 'classifier__n_estimators': 162, 'classifier__subsample': 0.9}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:547: FitFailedWarning: \n",
      "2025 fits failed out of a total of 8100.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "2025 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/sklearn/model_selection/_validation.py\", line 895, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/sklearn/base.py\", line 1474, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/imblearn/pipeline.py\", line 326, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **last_step_params[\"fit\"])\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/xgboost/core.py\", line 730, in inner_f\n",
      "    return func(**kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/xgboost/sklearn.py\", line 1519, in fit\n",
      "    self._Booster = train(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/xgboost/core.py\", line 730, in inner_f\n",
      "    return func(**kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/xgboost/training.py\", line 181, in train\n",
      "    bst.update(dtrain, i, obj)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/xgboost/core.py\", line 2050, in update\n",
      "    _check_call(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/xgboost/core.py\", line 282, in _check_call\n",
      "    raise XGBoostError(py_str(_LIB.XGBGetLastError()))\n",
      "xgboost.core.XGBoostError: value 95 for Parameter colsample_bytree exceed bound [0,1]\n",
      "colsample_bytree: Subsample ratio of columns, resample on each tree construction.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/model_selection/_search.py:1051: UserWarning: One or more of the test scores are non-finite: [0.87538541 0.91464793 0.91972752 ... 0.89098766 0.89295302 0.89618064]\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "from imblearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.metrics import make_scorer, f1_score\n",
    "import numpy as np\n",
    "\n",
    "# Defining type of split for CV - this is used for imbalanced datasets to keep equal proportion of class balance between folds\n",
    "stratified_kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Defining model and resampling method\n",
    "model = xgb.XGBClassifier(random_state=42)\n",
    "\n",
    "params_xgb = {}\n",
    "params_xgb['classifier__n_estimators'] =  [100, 112, 125, 137, 150, 162, 175, 187, 200]  # 150\n",
    "params_xgb['classifier__max_depth'] = [3, 4, 5] # 3\n",
    "params_xgb['classifier__learning_rate'] = [0.4, 0.45, 0.5, 0.55, 0.6] # 0.5\n",
    "params_xgb['classifier__min_child_weight'] = [1] #  1\n",
    "params_xgb['classifier__subsample'] = [0.8, 0.9, 1] #  1\n",
    "params_xgb['classifier__colsample_bytree'] = [0.9, 0,95, 1] #Default = 1\n",
    "\n",
    "# Defining preprocessing methods for pipeline\n",
    "scaler = RobustScaler()\n",
    "imputer = SimpleImputer()\n",
    "\n",
    "# Defining pipeline to avoid data leakage\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('Robust', scaler),\n",
    "    ('Simple', imputer),    \n",
    "    ('classifier', model),  \n",
    "    ])\n",
    "\n",
    "############################################################################################\n",
    "\n",
    "# Initializing RandomizedSearchCV\n",
    "opt = GridSearchCV(\n",
    "    pipeline,\n",
    "    param_grid=params_xgb,  # Number of parameter settings that are sampled\n",
    "    cv=stratified_kfold,  # Number of cross-validation folds\n",
    "    scoring='f1_macro',\n",
    "    verbose=1,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "opt.fit(X_anx_selected, y_anx)\n",
    "\n",
    "#printing(opt.best_score_)\n",
    "print(opt.best_score_)\n",
    "print(opt.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75fb62a5-5a2a-4733-a768-ffe791b93f6e",
   "metadata": {},
   "source": [
    "#### Validating results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "acfd7050-2995-4059-9354-57e12cc7a68c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "XGBClassifier\n",
      "Mean F1-macro Score: 0.9209335388734046 std: 0.08810354178811651\n"
     ]
    }
   ],
   "source": [
    "# Defining classifier with best params\n",
    "classifier = xgb.XGBClassifier(colsample_bytree = 0.9,\n",
    "learning_rate = 0.4,\n",
    "max_depth = 3,\n",
    "min_child_weight = 1,\n",
    "n_estimators = 162,\n",
    "subsample = 0.9,\n",
    "random_state=42, n_jobs=-1)\n",
    "\n",
    "# Defining preprocessing steps\n",
    "scaler = RobustScaler()\n",
    "imputer = SimpleImputer()\n",
    "\n",
    "# Defining pipeline\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('Robust', scaler),\n",
    "    ('Simple', imputer),\n",
    "    ('Classifier', classifier)\n",
    "])\n",
    "\n",
    "# Performing cross-validation\n",
    "scores = cross_val_score(pipeline, X_anx, y_anx, cv=stratified_kfold, scoring='f1_macro', n_jobs=-1)\n",
    "mean_score = np.mean(scores)\n",
    "std_score = np.std(scores)\n",
    "\n",
    "# Printing results\n",
    "print()\n",
    "print(classifier.__class__.__name__)\n",
    "print(\"Mean F1-macro Score:\", mean_score, \"std:\", std_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aebd469c-0747-4e7e-b5b7-88b09b6625a2",
   "metadata": {},
   "source": [
    "### LGBM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d42d3b1e-da62-45ce-8910-49369fa7abd2",
   "metadata": {},
   "source": [
    "#### Performing GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "766427fe-81d7-42b7-b710-3eb4fbe21f3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 2700 candidates, totalling 13500 fits\n",
      "[LightGBM] [Info] Number of positive: 35, number of negative: 443\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000088 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 309\n",
      "[LightGBM] [Info] Number of data points in the train set: 478, number of used features: 6\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.073222 -> initscore=-2.538222\n",
      "[LightGBM] [Info] Start training from score -2.538222\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "0.910054117835578\n",
      "{'classifier__boosting_type': 'gbdt', 'classifier__class_weight': None, 'classifier__colsample_bytree': 0.75, 'classifier__learning_rate': 0.1, 'classifier__max_depth': 5, 'classifier__n_estimators': 20, 'classifier__num_leaves': 10, 'classifier__objective': 'binary'}\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "from imblearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.metrics import make_scorer, f1_score\n",
    "import numpy as np\n",
    "\n",
    "# Defining type of split for CV - this is used for imbalanced datasets to keep equal proportion of class balance between folds\n",
    "stratified_kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Defining model and resampling method\n",
    "model = lgb.LGBMClassifier(random_state=42, n_jobs =-1)\n",
    "\n",
    "params_lgbm = {}\n",
    "params_lgbm['classifier__boosting_type'] = ['gbdt', 'dart']  # gbdt\n",
    "params_lgbm['classifier__n_estimators'] = [10, 15, 20, 25, 30] # 20\n",
    "params_lgbm['classifier__num_leaves'] = [10, 13, 15, 17, 20] #np.arange(0.10, 0.21, 0.01).tolist() # 15 \n",
    "params_lgbm['classifier__max_depth'] = [5, 6, 7]  # 6\n",
    "params_lgbm['classifier__learning_rate'] = [0.05, 0.1, 0.15]  # 0.1\n",
    "params_lgbm['classifier__objective'] = ['binary', None] # None\n",
    "params_lgbm['classifier__class_weight'] = [None] # None\n",
    "params_lgbm['classifier__colsample_bytree'] = [0.7, 0.75, 0.8] #np.arange(0.7, 0.81, 0.01).tolist() #0.75\n",
    "\n",
    "# Defining preprocessing methods for pipeline\n",
    "scaler = RobustScaler()\n",
    "imputer = SimpleImputer()\n",
    "\n",
    "# Defining pipeline to avoid data leakage\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('Robust', scaler),\n",
    "    ('Simple', imputer),    \n",
    "    ('classifier', model),  \n",
    "    ])\n",
    "\n",
    "############################################################################################\n",
    "\n",
    "# Initializing RandomizedSearchCV\n",
    "opt = GridSearchCV(\n",
    "    pipeline,\n",
    "    param_grid=params_lgbm,  # Number of parameter settings that are sampled\n",
    "    cv=stratified_kfold,  # Number of cross-validation folds\n",
    "    scoring='f1_macro',\n",
    "    verbose=1,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "opt.fit(X_anx_selected, y_anx)\n",
    "\n",
    "#printing(opt.best_score_)\n",
    "print(opt.best_score_)\n",
    "print(opt.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7332b093-85d5-4f3a-8a9a-8415230c03ca",
   "metadata": {},
   "source": [
    "#### Validating results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b8047487-f94a-4efe-a0cf-8a7b7e4adca0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "LGBMClassifier\n",
      "Mean F1-macro Score: 0.910054117835578 std: 0.07139540921071352\n"
     ]
    }
   ],
   "source": [
    "# Defining classifier with best params\n",
    "classifier = lgb.LGBMClassifier(boosting_type = 'gbdt',\n",
    "class_weight = None,\n",
    "colsample_bytree = 0.75,\n",
    "learning_rate = 0.1,\n",
    "max_depth = 5,\n",
    "n_estimators = 20,\n",
    "num_leaves = 10,\n",
    "objective = 'binary',\n",
    "random_state=42, n_jobs =-1)\n",
    "\n",
    "# Defining preprocessing steps\n",
    "scaler = RobustScaler()\n",
    "imputer = SimpleImputer()\n",
    "\n",
    "# Defining pipeline\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('Robust', scaler),\n",
    "    ('Simple', imputer),\n",
    "    ('Classifier', classifier)\n",
    "])\n",
    "\n",
    "# Performing cross-validation\n",
    "scores = cross_val_score(pipeline, X_anx, y_anx, cv=stratified_kfold, scoring='f1_macro', n_jobs=-1)\n",
    "mean_score = np.mean(scores)\n",
    "std_score = np.std(scores)\n",
    "\n",
    "# Printing results\n",
    "print()\n",
    "print(classifier.__class__.__name__)\n",
    "print(\"Mean F1-macro Score:\", mean_score, \"std:\", std_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a44e0dd-b5c0-4706-b2cc-35e97c71d46c",
   "metadata": {},
   "source": [
    "## Distressed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caef4444-37a8-476a-b4d6-d6ac4e894582",
   "metadata": {},
   "source": [
    "### RF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab601241-66a4-4887-8e08-9439b1fa0637",
   "metadata": {},
   "source": [
    "#### Performing GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "12abe5e3-c8ad-4727-8772-5c28504a845e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1512 candidates, totalling 7560 fits\n",
      "0.9154748466102941\n",
      "{'classifier__class_weight': None, 'classifier__max_depth': 6, 'classifier__max_features': 'sqrt', 'classifier__min_samples_leaf': 1, 'classifier__min_samples_split': 2, 'classifier__n_estimators': 66}\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "from imblearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.metrics import make_scorer, f1_score\n",
    "from sklearn.model_selection import RandomizedSearchCV, StratifiedKFold\n",
    "import numpy as np\n",
    "\n",
    "# Defining type of split for CV - this is used for imbalanced datasets to keep equal proportion of class balance between folds\n",
    "stratified_kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Defining model and resampling method\n",
    "model = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Defining parameter grid\n",
    "params_rf = {}\n",
    "params_rf['classifier__n_estimators'] = np.arange(40, 67, 2).tolist() #50\n",
    "params_rf['classifier__max_depth'] = [6, 7, 8] #7\n",
    "params_rf['classifier__min_samples_split'] = [2, 3, 4] #3\n",
    "params_rf['classifier__min_samples_leaf'] = [1, 2] #1\n",
    "params_rf['classifier__max_features'] = ['log2', 'sqrt'] #sqrt\n",
    "params_rf['classifier__class_weight'] = ['balanced', 'balanced_subsample', None] #None\n",
    "\n",
    "# Defining preprocessing methods for pipeline\n",
    "scaler = MinMaxScaler()\n",
    "imputer = SimpleImputer()\n",
    "\n",
    "# Defining pipeline to avoid data leakage\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('Robust', scaler),\n",
    "    ('Simple', imputer),    \n",
    "    ('classifier', model),  \n",
    "    ])\n",
    "\n",
    "############################################################################################\n",
    "\n",
    "# Initializing RandomizedSearchCV\n",
    "opt = GridSearchCV(\n",
    "    pipeline,\n",
    "    param_grid=params_rf,  # Number of parameter settings that are sampled\n",
    "    cv=stratified_kfold,  # Number of cross-validation fold\n",
    "    scoring='f1_macro',\n",
    "    verbose=1,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "opt.fit(X_dis_selected, y_dis)\n",
    "\n",
    "#printing(opt.best_score_)\n",
    "print(opt.best_score_)\n",
    "print(opt.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc512f5-c632-42c6-ade4-41c5404ac707",
   "metadata": {},
   "source": [
    "#### Validating results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4861d871-2058-470a-8b20-83c3acb6bd2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RandomForestClassifier\n",
      "Mean F1-macro Score: 0.9154748466102941 std: 0.08290499798612358\n"
     ]
    }
   ],
   "source": [
    "# Defining classifier with best params\n",
    "classifier = RandomForestClassifier(class_weight = None,\n",
    "max_depth = 6,\n",
    "max_features = 'sqrt',\n",
    "min_samples_leaf = 1,\n",
    "min_samples_split = 2,\n",
    "n_estimators = 66,\n",
    "random_state=42)\n",
    "\n",
    "# Defining preprocessing steps\n",
    "scaler = MinMaxScaler()\n",
    "imputer = SimpleImputer()\n",
    "\n",
    "# Defining pipeline\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('MinMax', scaler),\n",
    "    ('Simple', imputer),\n",
    "    ('Classifier', classifier)\n",
    "])\n",
    "\n",
    "# Performing cross-validation\n",
    "scores = cross_val_score(pipeline, X_dis, y_dis, cv=stratified_kfold, scoring='f1_macro', n_jobs=-1)\n",
    "mean_score = np.mean(scores)\n",
    "std_score = np.std(scores)\n",
    "\n",
    "# Printing results\n",
    "print()\n",
    "print(classifier.__class__.__name__)\n",
    "print(\"Mean F1-macro Score:\", mean_score, \"std:\", std_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "590fe980-aba9-4650-8db0-b1826ac9af3b",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca3b96f-e0e1-487c-a921-b1ce1027d5f3",
   "metadata": {},
   "source": [
    "#### Performing GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "0661f8f8-d9aa-4045-825d-48f543860431",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 12960 candidates, totalling 64800 fits\n",
      "0.8889376147546265\n",
      "{'classifier__C': 50, 'classifier__class_weight': None, 'classifier__coef0': 0.3, 'classifier__degree': 5, 'classifier__gamma': 'auto', 'classifier__kernel': 'sigmoid', 'classifier__max_iter': 5000, 'classifier__probability': True, 'classifier__shrinking': True}\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "from imblearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.metrics import make_scorer, f1_score\n",
    "from sklearn.model_selection import RandomizedSearchCV, StratifiedKFold\n",
    "import numpy as np\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Defining type of split for CV - this is used for imbalanced datasets to keep equal proportion of class balance between folds\n",
    "stratified_kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Defining model and resampling method\n",
    "model = SVC(random_state=42)\n",
    "\n",
    "# Defining parameter grid\n",
    "params_svc = {}\n",
    "params_svc['classifier__C'] = [25, 50, 75, 100, 125, 150, 250, 500, 750]  #100\n",
    "params_svc['classifier__kernel'] = ['rbf', 'sigmoid'] #sigmoid\n",
    "params_svc['classifier__degree'] = [5, 6, 7] #6\n",
    "params_svc['classifier__gamma'] = ['scale', 'auto'] #auto\n",
    "params_svc['classifier__coef0'] = [0.3, 0.4, 0.5, 0.6, 0.7]  #0.5 \n",
    "params_svc['classifier__shrinking'] = [True, False] #False\n",
    "params_svc['classifier__probability'] = [True, False] #False\n",
    "params_svc['classifier__class_weight'] = ['balanced', None] #None\n",
    "params_svc['classifier__max_iter'] = [5000, 6000, 7000] #6000\n",
    "\n",
    "# Defining preprocessing methods for pipeline\n",
    "scaler = MinMaxScaler()\n",
    "imputer = SimpleImputer()\n",
    "\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('Robust', scaler),\n",
    "    ('Simple', imputer),    \n",
    "    ('classifier', model),  \n",
    "    ])\n",
    "\n",
    "\n",
    "############################################################################################\n",
    "\n",
    "# Initializing RandomizedSearchCV\n",
    "opt = GridSearchCV(\n",
    "    pipeline,\n",
    "    param_grid=params_svc,  # Number of parameter settings that are sampled\n",
    "    cv=stratified_kfold,  # Number of cross-validation folds\n",
    "    scoring='f1_macro',\n",
    "    verbose=1,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "opt.fit(X_dis_selected, y_dis)\n",
    "\n",
    "#printing(opt.best_score_)\n",
    "print(opt.best_score_)\n",
    "print(opt.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf8c2068-5567-475a-b53c-60bd6644eff3",
   "metadata": {},
   "source": [
    "#### Validating results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "af9b129d-fbf1-4fce-9781-94f9dad07284",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SVC\n",
      "Mean F1-macro Score: 0.8889376147546265 std: 0.11453618279448234\n"
     ]
    }
   ],
   "source": [
    "# Defining classifier with best params\n",
    "classifier = SVC(C = 50,\n",
    "class_weight = None,\n",
    "coef0 = 0.3,\n",
    "degree = 5,\n",
    "gamma = 'auto',\n",
    "kernel = 'sigmoid',\n",
    "max_iter = 5000,\n",
    "probability = True,\n",
    "shrinking = True,\n",
    "random_state=42)\n",
    "\n",
    "# Defining preprocessing steps\n",
    "scaler = MinMaxScaler()\n",
    "imputer = SimpleImputer()\n",
    "\n",
    "# Defining pipeline\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('MinMax', scaler),\n",
    "    ('Simple', imputer),\n",
    "    ('Classifier', classifier)\n",
    "])\n",
    "\n",
    "# Performing cross-validation\n",
    "scores = cross_val_score(pipeline, X_dis, y_dis, cv=stratified_kfold, scoring='f1_macro', n_jobs=-1)\n",
    "mean_score = np.mean(scores)\n",
    "std_score = np.std(scores)\n",
    "\n",
    "# Printing results\n",
    "print()\n",
    "print(classifier.__class__.__name__)\n",
    "print(\"Mean F1-macro Score:\", mean_score, \"std:\", std_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76926c35-49fe-4720-aef4-912d93d9612e",
   "metadata": {},
   "source": [
    "### KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aeb2aca-d1a3-4a8d-b178-bccb61250bbb",
   "metadata": {},
   "source": [
    "#### Performing GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "75a75ec2-4665-4aae-8338-3e4ab32215de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1120 candidates, totalling 5600 fits\n",
      "0.8929530181562623\n",
      "{'classifier__algorithm': 'auto', 'classifier__leaf_size': 5, 'classifier__n_neighbors': 6, 'classifier__p': 2, 'classifier__weights': 'distance'}\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "from imblearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.metrics import make_scorer, f1_score\n",
    "import numpy as np\n",
    "\n",
    "# Defining type of split for CV - this is used for imbalanced datasets to keep equal proportion of class balance between folds\n",
    "stratified_kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Defining model and resampling method\n",
    "model = KNeighborsClassifier()\n",
    "\n",
    "params_knn = {} \n",
    "params_knn['classifier__n_neighbors'] = np.arange(1, 11, 1).tolist() # 5\n",
    "params_knn['classifier__weights'] = ['uniform', 'distance'] # uniform\n",
    "params_knn['classifier__algorithm'] = ['auto', 'ball_tree', 'kd_tree', 'brute'] # Auto\n",
    "params_knn['classifier__leaf_size'] = [5, 8, 10, 12, 15, 20, 30] # 10\n",
    "params_knn['classifier__p'] = [1, 2] #Default = 2\n",
    "\n",
    "# Defining preprocessing methods for pipeline\n",
    "scaler = MinMaxScaler()\n",
    "imputer = SimpleImputer()\n",
    "\n",
    "# Defining pipeline to avoid data leakage\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('Robust', scaler),\n",
    "    ('Simple', imputer),    \n",
    "    ('classifier', model),  \n",
    "    ])\n",
    "\n",
    "############################################################################################\n",
    "\n",
    "# Initializing RandomizedSearchCV\n",
    "opt = GridSearchCV(\n",
    "    pipeline,\n",
    "    param_grid=params_knn,  # Number of parameter settings that are sampled\n",
    "    cv=stratified_kfold,  # Number of cross-validation folds\n",
    "    scoring='f1_macro',\n",
    "    verbose=1,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "opt.fit(X_dis_selected, y_dis)\n",
    "\n",
    "#printing(opt.best_score_)\n",
    "print(opt.best_score_)\n",
    "print(opt.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a3fbf4a-69ea-4e8e-897d-18bf14c1215d",
   "metadata": {},
   "source": [
    "#### Validating results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c312410a-2d9c-4216-b571-7cf9d4d7d0b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "KNeighborsClassifier\n",
      "Mean F1-macro Score: 0.8929530181562623 std: 0.07793678442493912\n"
     ]
    }
   ],
   "source": [
    "# Defining classifier with best params\n",
    "classifier = KNeighborsClassifier(algorithm = 'auto',\n",
    "leaf_size = 5,\n",
    "n_neighbors = 6,\n",
    "p = 2,\n",
    "weights = 'distance',\n",
    "n_jobs=-1)\n",
    "\n",
    "# Defining preprocessing steps\n",
    "scaler = MinMaxScaler()\n",
    "imputer = SimpleImputer()\n",
    "\n",
    "# Defining pipeline\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('MinMax', scaler),\n",
    "    ('Simple', imputer),\n",
    "    ('Classifier', classifier)\n",
    "])\n",
    "\n",
    "# Performing cross-validation\n",
    "scores = cross_val_score(pipeline, X_dis, y_dis, cv=stratified_kfold, scoring='f1_macro', n_jobs=-1)\n",
    "mean_score = np.mean(scores)\n",
    "std_score = np.std(scores)\n",
    "\n",
    "# Printing results\n",
    "print()\n",
    "print(classifier.__class__.__name__)\n",
    "print(\"Mean F1-macro Score:\", mean_score, \"std:\", std_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c86d05a-54a5-4944-a87b-3b0da23515e1",
   "metadata": {},
   "source": [
    "### XGB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09bb71ce-d118-44e1-be5d-62fc0149e4fc",
   "metadata": {},
   "source": [
    "#### Performing GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "5b92f841-080e-4e0b-a8c9-2d45c3d57092",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 2520 candidates, totalling 12600 fits\n",
      "0.8952635753908563\n",
      "{'classifier__colsample_bytree': 1, 'classifier__learning_rate': 0.3000000000000001, 'classifier__max_depth': 8, 'classifier__min_child_weight': 1, 'classifier__n_estimators': 63, 'classifier__subsample': 1}\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "from imblearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.metrics import make_scorer, f1_score\n",
    "import numpy as np\n",
    "\n",
    "# Defining type of split for CV - this is used for imbalanced datasets to keep equal proportion of class balance between folds\n",
    "stratified_kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Defining model and resampling method\n",
    "model = xgb.XGBClassifier(random_state=42)\n",
    "\n",
    "params_xgb = {}\n",
    "params_xgb['classifier__n_estimators'] =  [50, 63, 75, 87, 100]  #75\n",
    "params_xgb['classifier__max_depth'] = [8, 9, 10] # 9\n",
    "params_xgb['classifier__learning_rate'] = np.arange(0.2, 0.41, 0.01).tolist() # 0.3\n",
    "params_xgb['classifier__min_child_weight'] = [1, 2] # 1\n",
    "params_xgb['classifier__subsample'] = [0.9, 1] # = 1\n",
    "params_xgb['classifier__colsample_bytree'] = [0.9, 1] #Default = 1\n",
    "\n",
    "# Defining preprocessing methods for pipeline\n",
    "scaler = MinMaxScaler()\n",
    "imputer = SimpleImputer()\n",
    "\n",
    "# Defining pipeline to avoid data leakage\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('Robust', scaler),\n",
    "    ('Simple', imputer),    \n",
    "    ('classifier', model),  \n",
    "    ])\n",
    "\n",
    "############################################################################################\n",
    "\n",
    "# Initializing RandomizedSearchCV\n",
    "opt = GridSearchCV(\n",
    "    pipeline,\n",
    "    param_grid=params_xgb,  # Number of parameter settings that are sampled\n",
    "    cv=stratified_kfold,  # Number of cross-validation folds\n",
    "    scoring='f1_macro',\n",
    "    verbose=1,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "opt.fit(X_dis_selected, y_dis)\n",
    "\n",
    "#printing(opt.best_score_)\n",
    "print(opt.best_score_)\n",
    "print(opt.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1548eba4-2648-4fbb-9f9e-a1b710c3c112",
   "metadata": {},
   "source": [
    "#### Validating results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bb5c3d84-3874-4466-ba1c-ed527f48d0d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "XGBClassifier\n",
      "Mean F1-macro Score: 0.8952635753908563 std: 0.06856591410585108\n"
     ]
    }
   ],
   "source": [
    "# Defining classifier with best params\n",
    "classifier = xgb.XGBClassifier(colsample_bytree = 1,\n",
    "learning_rate = 0.3,\n",
    "max_depth = 8,\n",
    "min_child_weight = 1,\n",
    "n_estimators = 63,\n",
    "subsample = 1,\n",
    "random_state=42, n_jobs=-1)\n",
    "\n",
    "# Defining preprocessing steps\n",
    "scaler = MinMaxScaler()\n",
    "imputer = SimpleImputer()\n",
    "\n",
    "# Defining pipeline\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('MinMax', scaler),\n",
    "    ('Simple', imputer),\n",
    "    ('Classifier', classifier)\n",
    "])\n",
    "\n",
    "# Performing cross-validation\n",
    "scores = cross_val_score(pipeline, X_dis, y_dis, cv=stratified_kfold, scoring='f1_macro', n_jobs=-1)\n",
    "mean_score = np.mean(scores)\n",
    "std_score = np.std(scores)\n",
    "\n",
    "# Printing results\n",
    "print()\n",
    "print(classifier.__class__.__name__)\n",
    "print(\"Mean F1-macro Score:\", mean_score, \"std:\", std_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8128e75-5e64-4a43-ad44-5664adba527a",
   "metadata": {},
   "source": [
    "### LGBM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "602a9472-84c8-439f-b746-095dc2dba61f",
   "metadata": {},
   "source": [
    "#### Performing GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "c1bc9728-8908-4a34-9491-650727eac6cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 2250 candidates, totalling 11250 fits\n",
      "[LightGBM] [Info] Number of positive: 39, number of negative: 439\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000114 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 336\n",
      "[LightGBM] [Info] Number of data points in the train set: 478, number of used features: 8\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.081590 -> initscore=-2.420938\n",
      "[LightGBM] [Info] Start training from score -2.420938\n",
      "0.9270874323416697\n",
      "{'classifier__boosting_type': 'dart', 'classifier__class_weight': None, 'classifier__colsample_bytree': 0.7, 'classifier__learning_rate': 0.2, 'classifier__max_depth': 2, 'classifier__n_estimators': 87, 'classifier__num_leaves': 3, 'classifier__objective': 'binary'}\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "from imblearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.metrics import make_scorer, f1_score\n",
    "import numpy as np\n",
    "\n",
    "# Defining type of split for CV - this is used for imbalanced datasets to keep equal proportion of class balance between folds\n",
    "stratified_kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Defining model and resampling method\n",
    "model = lgb.LGBMClassifier(random_state=42)\n",
    "\n",
    "params_lgbm = {}\n",
    "params_lgbm['classifier__boosting_type'] = ['gbdt', 'dart']  # dart\n",
    "params_lgbm['classifier__n_estimators'] = [50, 63, 75, 87, 100] # 75\n",
    "params_lgbm['classifier__num_leaves'] = [2, 3, 4] # 3\n",
    "params_lgbm['classifier__max_depth'] = [2, 3, 4,]  # 3\n",
    "params_lgbm['classifier__learning_rate'] = [0.2, 0.25, 0.3, 0.35, 0.4]  # 0.3 \n",
    "params_lgbm['classifier__objective'] = ['binary'] # binary. \n",
    "params_lgbm['classifier__class_weight'] = [None] # None\n",
    "params_lgbm['classifier__colsample_bytree'] = [0.6, 0.65, 0.7, 0.75, 0.8] # 0.7\n",
    "\n",
    "# Defining preprocessing methods for pipeline\n",
    "scaler = MinMaxScaler()\n",
    "imputer = SimpleImputer()\n",
    "\n",
    "# Defining pipeline to avoid data leakage\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('Robust', scaler),\n",
    "    ('Simple', imputer),    \n",
    "    ('classifier', model),  \n",
    "    ])\n",
    "############################################################################################\n",
    "\n",
    "# Initializing RandomizedSearchCV\n",
    "opt = GridSearchCV(\n",
    "    pipeline,\n",
    "    param_grid=params_lgbm,  # Number of parameter settings that are sampled\n",
    "    cv=stratified_kfold,  # Number of cross-validation folds\n",
    "    scoring='f1_macro',\n",
    "    verbose=1,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "opt.fit(X_dis_selected, y_dis)\n",
    "\n",
    "#printing(opt.best_score_)\n",
    "print(opt.best_score_)\n",
    "print(opt.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e66ef12c-0f2e-439f-9cdd-29ce187ab025",
   "metadata": {},
   "source": [
    "#### Validating results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8cb952f7-da9d-4eef-8712-af53999e272e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "LGBMClassifier\n",
      "Mean F1-macro Score: 0.9270874323416697 std: 0.05159671162606029\n"
     ]
    }
   ],
   "source": [
    "# Defining classifier with best params\n",
    "classifier = lgb.LGBMClassifier(boosting_type = 'dart',\n",
    "class_weight = None,\n",
    "colsample_bytree = 0.7,\n",
    "learning_rate = 0.2,\n",
    "max_depth = 2,\n",
    "n_estimators = 87,\n",
    "num_leaves = 3,\n",
    "objective = 'binary',\n",
    "random_state=42, n_jobs =-1)\n",
    "\n",
    "# Defining preprocessing steps\n",
    "scaler = MinMaxScaler()\n",
    "imputer = SimpleImputer()\n",
    "\n",
    "# Defining pipeline\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('MinMax', scaler),\n",
    "    ('Simple', imputer),\n",
    "    ('Classifier', classifier)\n",
    "])\n",
    "\n",
    "# Performing cross-validation\n",
    "scores = cross_val_score(pipeline, X_dis, y_dis, cv=stratified_kfold, scoring='f1_macro', n_jobs=-1)\n",
    "mean_score = np.mean(scores)\n",
    "std_score = np.std(scores)\n",
    "\n",
    "# Printing results\n",
    "print()\n",
    "print(classifier.__class__.__name__)\n",
    "print(\"Mean F1-macro Score:\", mean_score, \"std:\", std_score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 ",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
